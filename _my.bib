@ARTICLE{Drew2025-dz,
  title         = "Can deep neural networks learn biological vision?",
  author        = "Drew, Linsley and Pinyuan, Feng and Thomas, Serre",
  journal       = "arXiv [q-bio.NC]",
  abstract      = "Deep neural networks (DNNs) once showed increasing alignment
                   with primate neural responses as they improved on computer
                   vision benchmarks. This trend raised the exciting possibility
                   that better models of biological vision would come as a
                   byproduct of the deep learning revolution in artificial
                   intelligence. However, the trend has reversed over recent
                   years as DNNs have scaled to human or superhuman recognition
                   accuracy, a divergence that may stem from modern DNNs
                   learning to rely on different visual features than primates
                   to solve tasks. Where will better computational models of
                   biological vision come from? We propose that vision science
                   must break from artificial intelligence to develop algorithms
                   that are designed with biological visual systems in mind
                   instead of internet data benchmarks. We predict that the next
                   generation of deep learning models of biological vision will
                   be trained with data diets, training routines, and objectives
                   that are closer to those that shape human vision than those
                   that are in use today.",
  month         =  "8~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "2504.16940",
  keywords      = "Project/Density\_IQA"
}

@ARTICLE{Berk2025-az,
  title         = "Emergence and evolution of interpretable concepts in
                   diffusion models",
  author        = "Berk, Tinaz and Zalan, Fabian and Mahdi, Soltanolkotabi",
  journal       = "arXiv [cs.CV]",
  abstract      = "Diffusion models have become the go-to method for
                   text-to-image generation, producing high-quality images from
                   noise through a process called reverse diffusion.
                   Understanding the dynamics of the reverse diffusion process
                   is crucial in steering the generation and achieving high
                   sample quality. However, the inner workings of diffusion
                   models is still largely a mystery due to their black-box
                   nature and complex, multi-step generation process.
                   Mechanistic Interpretability (MI) techniques, such as Sparse
                   Autoencoders (SAEs), aim at uncovering the operating
                   principles of models through granular analysis of their
                   internal representations. These MI techniques have been
                   successful in understanding and steering the behavior of
                   large language models at scale. However, the great potential
                   of SAEs has not yet been applied toward gaining insight into
                   the intricate generative process of diffusion models. In this
                   work, we leverage the SAE framework to probe the inner
                   workings of a popular text-to-image diffusion model, and
                   uncover a variety of human-interpretable concepts in its
                   activations. Interestingly, we find that even before the
                   first reverse diffusion step is completed, the final
                   composition of the scene can be predicted surprisingly well
                   by looking at the spatial distribution of activated concepts.
                   Moreover, going beyond correlational analysis, we show that
                   the discovered concepts have a causal effect on the model
                   output and can be leveraged to steer the generative process.
                   We design intervention techniques aimed at manipulating image
                   composition and style, and demonstrate that (1) in early
                   stages of diffusion image composition can be effectively
                   controlled, (2) in the middle stages of diffusion image
                   composition is finalized, however stylistic interventions are
                   effective, and (3) in the final stages of diffusion only
                   minor textural details are subject to change.",
  month         =  "21~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.15473"
}

@ARTICLE{Ramesh2024-ok,
  title         = "Many Perception Tasks are Highly Redundant Functions of their
                   Input Data",
  author        = "Ramesh, Rahul and Bisulco, Anthony and DiTullio, Ronald W and
                   Wei, Linran and Balasubramanian, Vijay and Daniilidis, Kostas
                   and Chaudhari, Pratik",
  journal       = "arXiv [cs.CV]",
  abstract      = "We show that many perception tasks, from visual recognition,
                   semantic segmentation, optical flow, depth estimation to
                   vocalization discrimination, are highly redundant functions
                   of their input data. Images or spectrograms, projected into
                   different subspaces, formed by orthogonal bases in pixel,
                   Fourier or wavelet domains, can be used to solve these tasks
                   remarkably well regardless of whether it is the top subspace
                   where data varies the most, some intermediate subspace with
                   moderate variability--or the bottom subspace where data
                   varies the least. This phenomenon occurs because different
                   subspaces have a large degree of redundant information
                   relevant to the task.",
  month         =  "18~" # jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2407.13841"
}

@ARTICLE{Xiangchen2025-nu,
  title         = "Structure-guided Diffusion Transformer for low-light image
                   enhancement",
  author        = "Xiangchen, Yin and Zhenda, Yu and Longtao, Jiang and Xin, Gao
                   and Xiao, Sun and Zhi, Liu and Xun, Yang",
  journal       = "arXiv [cs.CV]",
  abstract      = "While the diffusion transformer (DiT) has become a focal
                   point of interest in recent years, its application in
                   low-light image enhancement remains a blank area for
                   exploration. Current methods recover the details from
                   low-light images while inevitably amplifying the noise in
                   images, resulting in poor visual quality. In this paper, we
                   firstly introduce DiT into the low-light enhancement task and
                   design a novel Structure-guided Diffusion Transformer based
                   Low-light image enhancement (SDTL) framework. We compress the
                   feature through wavelet transform to improve the inference
                   efficiency of the model and capture the multi-directional
                   frequency band. Then we propose a Structure Enhancement
                   Module (SEM) that uses structural prior to enhance the
                   texture and leverages an adaptive fusion strategy to achieve
                   more accurate enhancement effect. In Addition, we propose a
                   Structure-guided Attention Block (SAB) to pay more attention
                   to texture-riched tokens and avoid interference from noisy
                   areas in noise prediction. Extensive qualitative and
                   quantitative experiments demonstrate that our method achieves
                   SOTA performance on several popular datasets, validating the
                   effectiveness of SDTL in improving image quality and the
                   potential of DiT in low-light enhancement tasks.",
  month         =  "21~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.15054"
}

@ARTICLE{Haiwen2025-cg,
  title         = "{LoftUp}: Learning a coordinate-based feature upsampler for
                   vision foundation models",
  author        = "Haiwen, Huang and Anpei, Chen and Volodymyr, Havrylov and
                   Andreas, Geiger and Dan, Zhang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Vision foundation models (VFMs) such as DINOv2 and CLIP have
                   achieved impressive results on various downstream tasks, but
                   their limited feature resolution hampers performance in
                   applications requiring pixel-level understanding. Feature
                   upsampling offers a promising direction to address this
                   challenge. In this work, we identify two critical factors for
                   enhancing feature upsampling: the upsampler architecture and
                   the training objective. For the upsampler architecture, we
                   introduce a coordinate-based cross-attention transformer that
                   integrates the high-resolution images with coordinates and
                   low-resolution VFM features to generate sharp, high-quality
                   features. For the training objective, we propose constructing
                   high-resolution pseudo-groundtruth features by leveraging
                   class-agnostic masks and self-distillation. Our approach
                   effectively captures fine-grained details and adapts flexibly
                   to various input and feature resolutions. Through
                   experiments, we demonstrate that our approach significantly
                   outperforms existing feature upsampling techniques across
                   various downstream tasks. Our code is released at
                   https://github.com/andrehuang/loftup.",
  month         =  "18~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.14032"
}

@ARTICLE{Thomas2025-ee,
  title         = "Prompt-hacking: The new p-hacking?",
  author        = "Thomas, Kosch and Sebastian, Feger",
  journal       = "arXiv [cs.HC]",
  abstract      = "As Large Language Models (LLMs) become increasingly embedded
                   in empirical research workflows, their use as analytical
                   tools raises pressing concerns for scientific integrity. This
                   opinion paper draws a parallel between ``prompt-hacking'',
                   the strategic tweaking of prompts to elicit desirable outputs
                   from LLMs, and the well-documented practice of ``p-hacking''
                   in statistical analysis. We argue that the inherent biases,
                   non-determinism, and opacity of LLMs make them unsuitable for
                   data analysis tasks demanding rigor, impartiality, and
                   reproducibility. We emphasize how researchers may
                   inadvertently, or even deliberately, adjust prompts to
                   confirm hypotheses while undermining research validity. We
                   advocate for a critical view of using LLMs in research,
                   transparent prompt documentation, and clear standards for
                   when LLM use is appropriate. We discuss how LLMs can replace
                   traditional analytical methods, whereas we recommend that
                   LLMs should only be used with caution, oversight, and
                   justification.",
  month         =  "20~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2504.14571"
}

@ARTICLE{Yang2025-ps,
  title         = "Learning human perspective in line drawings from single
                   sketches",
  author        = "Yang, Jinfan and Foord-Kelcey, Leo and Takikawa, Suzuran and
                   Vining, Nicholas and Mitra, Niloy and Sheffer, Alla",
  journal       = "arXiv [cs.GR]",
  abstract      = "Artist-drawn sketches only loosely conform to analytical
                   models of perspective projection. This deviation of
                   human-drawn perspective from analytical perspective models is
                   persistent and well known, but has yet to be algorithmically
                   replicated or even well understood. Capturing human
                   perspective can benefit many computer graphics applications,
                   including sketch-based modeling and non-photorealistic
                   rendering. We propose the first dedicated method for learning
                   and replicating human perspective. A core challenge in
                   learning this perspective is the lack of suitable large-scale
                   data, as well as the heterogeneity of human drawing choices.
                   We overcome the data paucity by learning, in a one-shot
                   setup, from a single artist sketch of a given 3D shape and a
                   best matching analytical camera view of the same shape. We
                   match the contours of the depicted shape in this view to
                   corresponding artist strokes. We then learn a spatially
                   continuous local perspective deviation function that modifies
                   the camera perspective projecting the contours to their
                   corresponding strokes while retaining key geometric
                   properties that artists strive to preserve when depicting 3D
                   content. We leverage the observation that artists employ
                   similar perspectives when depicting shapes from slightly
                   different view angles to algorithmically augment our training
                   data. First, we use the perspective function learned from the
                   single example to generate more human-like contour renders
                   from nearby views; then, we pair these renders with the
                   analytical camera contours from these views and use these
                   pairs as additional training data. The resulting learned
                   perspective functions are well aligned with the training
                   sketch perspectives and are consistent across views. We
                   compare our results to potential alternatives, demonstrating
                   the superiority of the proposed approach, and showcasing
                   applications that benefit from learned human perspective.",
  month         =  "3~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.GR",
  eprint        = "2504.03099"
}

@ARTICLE{Stoinski2024-qi,
  title     = "{THINGSplus}: New norms and metadata for the {THINGS} database of
               1854 object concepts and 26,107 natural object images",
  author    = "Stoinski, Laura M and Perkuhn, Jonas and Hebart, Martin N",
  journal   = "Behavior research methods",
  publisher = "Springer",
  volume    =  56,
  number    =  3,
  pages     = "1583--1603",
  abstract  = "To study visual and semantic object representations, the need for
               well-curated object concepts and images has grown significantly
               over the past years. To address this, we have previously
               developed THINGS, a large-scale database of 1854 systematically
               sampled object concepts with 26,107 high-quality naturalistic
               images of these concepts. With THINGSplus, we significantly
               extend THINGS by adding concept- and image-specific norms and
               metadata for all 1854 concepts and one copyright-free image
               example per concept. Concept-specific norms were collected for
               the properties of real-world size, manmadeness, preciousness,
               liveliness, heaviness, naturalness, ability to move or be moved,
               graspability, holdability, pleasantness, and arousal. Further, we
               provide 53 superordinate categories as well as typicality ratings
               for all their members. Image-specific metadata includes a
               nameability measure, based on human-generated labels of the
               objects depicted in the 26,107 images. Finally, we identified one
               new public domain image per concept. Property (M = 0.97, SD =
               0.03) and typicality ratings (M = 0.97, SD = 0.01) demonstrate
               excellent consistency, with the subsequently collected arousal
               ratings as the only exception (r = 0.69). Our property (M = 0.85,
               SD = 0.11) and typicality (r = 0.72, 0.74, 0.88) data correlated
               strongly with external norms, again with the lowest validity for
               arousal (M = 0.41, SD = 0.08). To summarize, THINGSplus provides
               a large-scale, externally validated extension to existing object
               norms and an important extension to THINGS, allowing detailed
               selection of stimuli and control variables for a wide range of
               research interested in visual object processing, language, and
               semantic memory.",
  month     =  mar,
  year      =  2024,
  keywords  = "Concrete concepts; Database; Object concepts; Object features;
               Object images; Semantic norms; Visual norms;Project/Metamerism",
  doi       = "10.3758/s13428-023-02110-8",
  pmc       = "PMC10991023",
  pmid      =  37095326,
  issn      = "1554-351X,1554-3528",
  language  = "en"
}

@ARTICLE{Hebart2019-pt,
  title     = "{THINGS}: A database of 1,854 object concepts and more than
               26,000 naturalistic object images",
  author    = "Hebart, Martin N and Dickter, Adam H and Kidder, Alexis and Kwok,
               Wan Y and Corriveau, Anna and Van Wicklin, Caitlin and Baker,
               Chris I",
  journal   = "PloS one",
  publisher = "Public Library of Science (PLoS)",
  volume    =  14,
  number    =  10,
  pages     = "e0223792",
  abstract  = "In recent years, the use of a large number of object concepts and
               naturalistic object images has been growing strongly in cognitive
               neuroscience research. Classical databases of object concepts are
               based mostly on a manually curated set of concepts. Further,
               databases of naturalistic object images typically consist of
               single images of objects cropped from their background, or a
               large number of naturalistic images of varying quality, requiring
               elaborate manual image curation. Here we provide a set of 1,854
               diverse object concepts sampled systematically from concrete
               picturable and nameable nouns in the American English language.
               Using these object concepts, we conducted a large-scale web image
               search to compile a database of 26,107 high-quality naturalistic
               images of those objects, with 12 or more object images per
               concept and all images cropped to square size. Using
               crowdsourcing, we provide higher-level category membership for
               the 27 most common categories and validate them by relating them
               to representations in a semantic embedding derived from large
               text corpora. Finally, by feeding images through a deep
               convolutional neural network, we demonstrate that they exhibit
               high selectivity for different object concepts, while at the same
               time preserving variability of different object images within
               each concept. Together, the THINGS database provides a rich
               resource of object concepts and object images and offers a tool
               for both systematic and large-scale naturalistic research in the
               fields of psychology, neuroscience, and computer science.",
  month     =  "15~" # oct,
  year      =  2019,
  keywords  = "Project/Metamerism",
  doi       = "10.1371/journal.pone.0223792",
  pmc       = "PMC6793944",
  pmid      =  31613926,
  issn      = "1932-6203",
  language  = "en"
}

@ARTICLE{Zeng2024-qq,
  title         = "{PriorDiffusion}: Leverage language prior in diffusion models
                   for monocular depth estimation",
  author        = "Zeng, Ziyao and Ni, Jingcheng and Wang, Daniel and Rim,
                   Patrick and Chung, Younjoon and Yang, Fengyu and Hong,
                   Byung-Woo and Wong, Alex",
  journal       = "arXiv [cs.CV]",
  abstract      = "This paper explores the potential of leveraging language
                   priors learned by text-to-image diffusion models to address
                   ambiguity and visual nuisance in monocular depth estimation.
                   Particularly, traditional monocular depth estimation suffers
                   from inherent ambiguity due to the absence of stereo or
                   multi-view depth cues, and nuisance due to lack of robustness
                   of vision. We argue that language prior in diffusion models
                   can enhance monocular depth estimation by leveraging the
                   geometric prior aligned with the language description, which
                   is learned during text-to-image pre-training. To generate
                   images that reflect the text properly, the model must
                   comprehend the size and shape of specified objects, their
                   spatial relationship, and the scale of the scene. Thus, we
                   propose PriorDiffusion, using a pre-trained text-to-image
                   diffusion model that takes both image and text description
                   that aligned with the scene to infer affine-invariant depth
                   through a denoising process. We also show that language
                   priors can guide the model's attention to specific regions
                   and help it perceive the 3D scene in alignment with user
                   intent. Simultaneously, it acts as a constraint to accelerate
                   the convergence of the diffusion trajectory, since learning
                   3D properties from a condensed, low-dimensional language
                   feature is more efficient compared with learning from a
                   redundant, high-dimensional image feature. By training on
                   HyperSim and Virtual KITTI, we achieve state-of-the-art
                   zero-shot performance and a faster convergence speed,
                   compared with other diffusion-based depth estimators, across
                   NYUv2, KITTI, ETH3D, and ScanNet.",
  month         =  "24~" # nov,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2411.16750"
}

@ARTICLE{Szpiro2025-jz,
  title    = "Perceptual learning improves discrimination but does not reduce
              distortions in appearance",
  author   = "Szpiro, Sarit F A and Burlingham, Charlie S and Simoncelli, Eero P
              and Carrasco, Marisa",
  journal  = "PLoS computational biology",
  volume   =  21,
  number   =  4,
  pages    = "e1012980",
  abstract = "Human perceptual sensitivity often improves with training, a
              phenomenon known as ``perceptual learning.'' Another important
              perceptual dimension is appearance, the subjective sense of
              stimulus magnitude. Are training-induced improvements in
              sensitivity accompanied by more accurate appearance? Here, we
              examined this question by measuring both discrimination
              (sensitivity) and estimation (appearance) responses to
              near-horizontal motion directions, which are known to be repulsed
              away from horizontal. Participants performed discrimination and
              estimation tasks before and after training in either the
              discrimination or the estimation task or none (control group).
              Human observers who trained in either discrimination or estimation
              exhibited improvements in discrimination accuracy, but estimation
              repulsion did not decrease; instead, it either persisted or
              increased. Hence, distortions in perception can be exacerbated
              after perceptual learning. We developed a computational observer
              model in which perceptual learning arises from increases in the
              precision of underlying neural representations, which explains
              this counterintuitive finding. For each observer, the fitted model
              accounted for discrimination performance, the distribution of
              estimates, and their changes with training. Our empirical findings
              and modeling suggest that learning enhances distinctions between
              categories, a potentially important aspect of real-world
              perception and perceptual learning.",
  month    =  "15~" # apr,
  year     =  2025,
  doi      = "10.1371/journal.pcbi.1012980",
  pmid     =  40233123,
  issn     = "1553-734X,1553-7358",
  language = "en"
}

@ARTICLE{Samuel2025-fm,
  title         = "Human aligned compression for robust models",
  author        = "Samuel, Räber and Andreas, Plesner and Till, Aczel and Roger,
                   Wattenhofer",
  journal       = "arXiv [cs.CV]",
  abstract      = "Adversarial attacks on image models threaten system
                   robustness by introducing imperceptible perturbations that
                   cause incorrect predictions. We investigate human-aligned
                   learned lossy compression as a defense mechanism, comparing
                   two learned models (HiFiC and ELIC) against traditional JPEG
                   across various quality levels. Our experiments on ImageNet
                   subsets demonstrate that learned compression methods
                   outperform JPEG, particularly for Vision Transformer
                   architectures, by preserving semantically meaningful content
                   while removing adversarial noise. Even in white-box settings
                   where attackers can access the defense, these methods
                   maintain substantial effectiveness. We also show that
                   sequential compression--applying rounds of
                   compression/decompression--significantly enhances defense
                   efficacy while maintaining classification performance. Our
                   findings reveal that human-aligned compression provides an
                   effective, computationally efficient defense that protects
                   the image features most relevant to human and machine
                   understanding. It offers a practical approach to improving
                   model robustness against adversarial threats.",
  month         =  "16~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.12255"
}

@ARTICLE{Tao2025-zn,
  title         = "Metric-Solver: Sliding anchored metric depth estimation from
                   a single image",
  author        = "Tao, Wen and Jiepeng, Wang and Yabo, Chen and Shugong, Xu and
                   Chi, Zhang and Xuelong, Li",
  journal       = "arXiv [cs.CV]",
  abstract      = "Accurate and generalizable metric depth estimation is crucial
                   for various computer vision applications but remains
                   challenging due to the diverse depth scales encountered in
                   indoor and outdoor environments. In this paper, we introduce
                   Metric-Solver, a novel sliding anchor-based metric depth
                   estimation method that dynamically adapts to varying scene
                   scales. Our approach leverages an anchor-based
                   representation, where a reference depth serves as an anchor
                   to separate and normalize the scene depth into two
                   components: scaled near-field depth and tapered far-field
                   depth. The anchor acts as a normalization factor, enabling
                   the near-field depth to be normalized within a consistent
                   range while mapping far-field depth smoothly toward zero.
                   Through this approach, any depth from zero to infinity in the
                   scene can be represented within a unified representation,
                   effectively eliminating the need to manually account for
                   scene scale variations. More importantly, for the same scene,
                   the anchor can slide along the depth axis, dynamically
                   adjusting to different depth scales. A smaller anchor
                   provides higher resolution in the near-field, improving depth
                   precision for closer objects while a larger anchor improves
                   depth estimation in far regions. This adaptability enables
                   the model to handle depth predictions at varying distances
                   and ensure strong generalization across datasets. Our design
                   enables a unified and adaptive depth representation across
                   diverse environments. Extensive experiments demonstrate that
                   Metric-Solver outperforms existing methods in both accuracy
                   and cross-dataset generalization.",
  month         =  "16~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.12103"
}

@ARTICLE{Kaifeng2025-fc,
  title         = "Generalized visual relation detection with diffusion models",
  author        = "Kaifeng, Gao and Siqi, Chen and Hanwang, Zhang and Jun, Xiao
                   and Yueting, Zhuang and Qianru, Sun",
  journal       = "arXiv [cs.CV]",
  abstract      = "Visual relation detection (VRD) aims to identify
                   relationships (or interactions) between object pairs in an
                   image. Although recent VRD models have achieved impressive
                   performance, they are all restricted to pre-defined relation
                   categories, while failing to consider the semantic ambiguity
                   characteristic of visual relations. Unlike objects, the
                   appearance of visual relations is always subtle and can be
                   described by multiple predicate words from different
                   perspectives, e.g., ``ride'' can be depicted as ``race'' and
                   ``sit on'', from the sports and spatial position views,
                   respectively. To this end, we propose to model visual
                   relations as continuous embeddings, and design diffusion
                   models to achieve generalized VRD in a conditional generative
                   manner, termed Diff-VRD. We model the diffusion process in a
                   latent space and generate all possible relations in the image
                   as an embedding sequence. During the generation, the visual
                   and text embeddings of subject-object pairs serve as
                   conditional signals and are injected via cross-attention.
                   After the generation, we design a subsequent matching stage
                   to assign the relation words to subject-object pairs by
                   considering their semantic similarities. Benefiting from the
                   diffusion-based generative process, our Diff-VRD is able to
                   generate visual relations beyond the pre-defined category
                   labels of datasets. To properly evaluate this generalized VRD
                   task, we introduce two evaluation metrics, i.e.,
                   text-to-image retrieval and SPICE PR Curve inspired by image
                   captioning. Extensive experiments in both human-object
                   interaction (HOI) detection and scene graph generation (SGG)
                   benchmarks attest to the superiority and effectiveness of
                   Diff-VRD.",
  month         =  "16~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.12100"
}

@ARTICLE{Liao2025-tq,
  title         = "Improved visual-spatial reasoning via {R1}-Zero-like training",
  author        = "Liao, Zhenyi and Xie, Qingsong and Zhang, Yanhao and Kong,
                   Zijian and Lu, Haonan and Yang, Zhenyu and Deng, Zhijie",
  journal       = "arXiv [cs.CV]",
  abstract      = "Increasing attention has been placed on improving the
                   reasoning capacities of multi-modal large language models
                   (MLLMs). As the cornerstone for AI agents that function in
                   the physical realm, video-based visual-spatial intelligence
                   (VSI) emerges as one of the most pivotal reasoning
                   capabilities of MLLMs. This work conducts a first, in-depth
                   study on improving the visual-spatial reasoning of MLLMs via
                   R1-Zero-like training. Technically, we first identify that
                   the visual-spatial reasoning capacities of small- to
                   medium-sized Qwen2-VL models cannot be activated via Chain of
                   Thought (CoT) prompts. We then incorporate GRPO training for
                   improved visual-spatial reasoning, using the carefully
                   curated VSI-100k dataset, following DeepSeek-R1-Zero. During
                   the investigation, we identify the necessity to keep the KL
                   penalty (even with a small value) in GRPO. With just 120 GPU
                   hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can
                   outperform the base model by 12.1\% and surpass GPT-4o.
                   Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B,
                   achieves performance comparable to that of the best
                   open-source model LLaVA-NeXT-Video-72B. Additionally, we
                   compare vsGRPO to supervised fine-tuning and direct
                   preference optimization baselines and observe strong
                   performance superiority. The code and dataset will be
                   available soon.",
  month         =  "1~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.00883"
}

@ARTICLE{Ziqi2025-qu,
  title         = "Aligning generative denoising with discriminative objectives
                   unleashes diffusion for visual perception",
  author        = "Ziqi, Pang and Xin, Xu and Yu-Xiong, Wang",
  journal       = "arXiv [cs.CV]",
  abstract      = "With the success of image generation, generative diffusion
                   models are increasingly adopted for discriminative tasks, as
                   pixel generation provides a unified perception interface.
                   However, directly repurposing the generative denoising
                   process for discriminative objectives reveals critical gaps
                   rarely addressed previously. Generative models tolerate
                   intermediate sampling errors if the final distribution
                   remains plausible, but discriminative tasks require rigorous
                   accuracy throughout, as evidenced in challenging multi-modal
                   tasks like referring image segmentation. Motivated by this
                   gap, we analyze and enhance alignment between generative
                   diffusion processes and perception tasks, focusing on how
                   perception quality evolves during denoising. We find: (1)
                   earlier denoising steps contribute disproportionately to
                   perception quality, prompting us to propose tailored learning
                   objectives reflecting varying timestep contributions; (2)
                   later denoising steps show unexpected perception degradation,
                   highlighting sensitivity to training-denoising distribution
                   shifts, addressed by our diffusion-tailored data
                   augmentation; and (3) generative processes uniquely enable
                   interactivity, serving as controllable user interfaces
                   adaptable to correctional prompts in multi-round
                   interactions. Our insights significantly improve
                   diffusion-based perception models without architectural
                   changes, achieving state-of-the-art performance on depth
                   estimation, referring image segmentation, and generalist
                   perception tasks. Code available at
                   https://github.com/ziqipang/ADDP.",
  month         =  "15~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.11457"
}

@ARTICLE{Wu2025-qj,
  title         = "{H3AE}: High compression, high speed, and high quality
                   {AutoEncoder} for video diffusion models",
  author        = "Wu, Yushu and Li, Yanyu and Skorokhodov, Ivan and Kag, Anil
                   and Menapace, Willi and Girish, Sharath and Siarohin,
                   Aliaksandr and Wang, Yanzhi and Tulyakov, Sergey",
  journal       = "arXiv [cs.CV]",
  abstract      = "Autoencoder (AE) is the key to the success of latent
                   diffusion models for image and video generation, reducing the
                   denoising resolution and improving efficiency. However, the
                   power of AE has long been underexplored in terms of network
                   design, compression ratio, and training strategy. In this
                   work, we systematically examine the architecture design
                   choices and optimize the computation distribution to obtain a
                   series of efficient and high-compression video AEs that can
                   decode in real time on mobile devices. We also unify the
                   design of plain Autoencoder and image-conditioned I2V VAE,
                   achieving multifunctionality in a single network. In
                   addition, we find that the widely adopted discriminative
                   losses, i.e., GAN, LPIPS, and DWT losses, provide no
                   significant improvements when training AEs at scale. We
                   propose a novel latent consistency loss that does not require
                   complicated discriminator design or hyperparameter tuning,
                   but provides stable improvements in reconstruction quality.
                   Our AE achieves an ultra-high compression ratio and real-time
                   decoding speed on mobile while outperforming prior art in
                   terms of reconstruction metrics by a large margin. We finally
                   validate our AE by training a DiT on its latent space and
                   demonstrate fast, high-quality text-to-video generation
                   capability.",
  month         =  "14~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.10567"
}

@ARTICLE{Ohayon2024-fj,
  title         = "Posterior-Mean Rectified Flow: Towards minimum {MSE}
                   photo-realistic image restoration",
  author        = "Ohayon, Guy and Michaeli, Tomer and Elad, Michael",
  journal       = "arXiv [eess.IV]",
  abstract      = "Photo-realistic image restoration algorithms are typically
                   evaluated by distortion measures (e.g., PSNR, SSIM) and by
                   perceptual quality measures (e.g., FID, NIQE), where the
                   desire is to attain the lowest possible distortion without
                   compromising on perceptual quality. To achieve this goal,
                   current methods commonly attempt to sample from the posterior
                   distribution, or to optimize a weighted sum of a distortion
                   loss (e.g., MSE) and a perceptual quality loss (e.g., GAN).
                   Unlike previous works, this paper is concerned specifically
                   with the optimal estimator that minimizes the MSE under a
                   constraint of perfect perceptual index, namely where the
                   distribution of the reconstructed images is equal to that of
                   the ground-truth ones. A recent theoretical result shows that
                   such an estimator can be constructed by optimally
                   transporting the posterior mean prediction (MMSE estimate) to
                   the distribution of the ground-truth images. Inspired by this
                   result, we introduce Posterior-Mean Rectified Flow (PMRF), a
                   simple yet highly effective algorithm that approximates this
                   optimal estimator. In particular, PMRF first predicts the
                   posterior mean, and then transports the result to a
                   high-quality image using a rectified flow model that
                   approximates the desired optimal transport map. We
                   investigate the theoretical utility of PMRF and demonstrate
                   that it consistently outperforms previous methods on a
                   variety of image restoration tasks.",
  month         =  "1~" # oct,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "eess.IV",
  eprint        = "2410.00418",
  keywords      = "Project/Density\_IQA"
}

@ARTICLE{Zheng2025-ch,
  title         = "Rethinking diffusion model in high dimension",
  author        = "Zheng, Zhenxin and Zheng, Zhenjie",
  journal       = "arXiv [stat.ML]",
  abstract      = "Curse of Dimensionality is an unavoidable challenge in
                   statistical probability models, yet diffusion models seem to
                   overcome this limitation, achieving impressive results in
                   high-dimensional data generation. Diffusion models assume
                   that they can learn the statistical properties of the
                   underlying probability distribution, enabling sampling from
                   this distribution to generate realistic samples. But is this
                   really how they work? To address this question, this paper
                   conducts a detailed analysis of the objective function and
                   inference methods of diffusion models, leading to several
                   important conclusions that help answer the above question: 1)
                   In high-dimensional sparse scenarios, the target of the
                   objective function fitting degrades from a weighted sum of
                   multiple samples to a single sample. 2) The mainstream
                   inference methods can all be represented within a simple
                   unified framework, without requiring statistical concepts
                   such as Markov chains and SDEs. 3) Guided by this simple
                   framework, more efficient inference methods can be
                   discovered.",
  month         =  "11~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "2503.08643"
}

@ARTICLE{Degenhard2025-em,
  title    = "When Do We Feel Present in a Virtual Reality? Towards Sensitivity
              and User Acceptance of Presence Questionnaires",
  author   = "Degenhard, Annalisa and Askari, Ali and Rietzler, Michael and
              Rukzio, Enrico",
  abstract = "Presence is an important and widely used metric to measure the
              quality of virtual reality (VR) applications. Given the
              multifaceted and subjective nature of presence, the most common
              measures for presence are questionnaires. But there is little
              research on their validity regarding specific presence dimensions
              and their responsiveness to differences in perception among users.
              We investigated four presence questionnaires (SUS, PQ, IPQ,
              Bouchard) on their responsiveness to intensity variations of known
              presence dimensions and asked users about their consistency with
              their experience. Therefore, we created five VR scenarios that
              were designed to emphasize a specific presence dimension. Our
              findings showed heterogeneous sensitivity of the questionnaires
              dependent on the different dimensions of presence. This highlights
              a context-specific suitability of presence questionnaires. The
              questionnaires' sensitivity was further stated as lower than
              actually perceived. Based on our findings, we offer guidance on
              selecting these questionnaires based on their suitability for
              particular use cases.",
  month    =  "14~" # apr,
  year     =  2025,
  eprint   = "2504.10162",
  doi      = "10.1145/3706598.3714204"
}

@ARTICLE{Tung2025-wn,
  title         = "From visual explanations to counterfactual explanations with
                   latent diffusion",
  author        = "Tung, Luu and Nam, Le and Duc, Le and Bac, Le",
  journal       = "arXiv [cs.CV]",
  abstract      = "Visual counterfactual explanations are ideal hypothetical
                   images that change the decision-making of the classifier with
                   high confidence toward the desired class while remaining
                   visually plausible and close to the initial image. In this
                   paper, we propose a new approach to tackle two key challenges
                   in recent prominent works: i) determining which specific
                   counterfactual features are crucial for distinguishing the
                   ``concept'' of the target class from the original class, and
                   ii) supplying valuable explanations for the non-robust
                   classifier without relying on the support of an adversarially
                   robust model. Our method identifies the essential region for
                   modification through algorithms that provide visual
                   explanations, and then our framework generates realistic
                   counterfactual explanations by combining adversarial attacks
                   based on pruning the adversarial gradient of the target
                   classifier and the latent diffusion model. The proposed
                   method outperforms previous state-of-the-art results on
                   various evaluation criteria on ImageNet and CelebA-HQ
                   datasets. In general, our method can be applied to arbitrary
                   classifiers, highlight the strong association between visual
                   and counterfactual explanations, make semantically meaningful
                   changes from the target classifier, and provide observers
                   with subtle counterfactual images.",
  month         =  "12~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.09202"
}

@ARTICLE{Yilin2025-jq,
  title         = "{MotionDreamer}: One-to-many motion synthesis with localized
                   generative masked transformer",
  author        = "Yilin, Wang and Chuan, Guo and Yuxuan, Mu and Javed, Muhammad
                   Gohar and Xinxin, Zuo and Juwei, Lu and Hai, Jiang and Li,
                   Cheng",
  journal       = "arXiv [cs.CV]",
  abstract      = "Generative masked transformers have demonstrated remarkable
                   success across various content generation tasks, primarily
                   due to their ability to effectively model large-scale dataset
                   distributions with high consistency. However, in the
                   animation domain, large datasets are not always available.
                   Applying generative masked modeling to generate diverse
                   instances from a single MoCap reference may lead to
                   overfitting, a challenge that remains unexplored. In this
                   work, we present MotionDreamer, a localized masked modeling
                   paradigm designed to learn internal motion patterns from a
                   given motion with arbitrary topology and duration. By
                   embedding the given motion into quantized tokens with a novel
                   distribution regularization method, MotionDreamer constructs
                   a robust and informative codebook for local motion patterns.
                   Moreover, a sliding window local attention is introduced in
                   our masked transformer, enabling the generation of natural
                   yet diverse animations that closely resemble the reference
                   motion patterns. As demonstrated through comprehensive
                   experiments, MotionDreamer outperforms the state-of-the-art
                   methods that are typically GAN or Diffusion-based in both
                   faithfulness and diversity. Thanks to the consistency and
                   robustness of the quantization-based approach, MotionDreamer
                   can also effectively perform downstream tasks such as
                   temporal motion editing, \textcolor{update}{crowd animation},
                   and beat-aligned dance generation, all using a single
                   reference motion. Visit our project page:
                   https://motiondreamer.github.io/",
  month         =  "11~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.08959"
}

@ARTICLE{Pascal2025-xe,
  title         = "{LookingGlass}: Generative Anamorphoses via Laplacian Pyramid
                   Warping",
  author        = "Pascal, Chang and Sergio, Sancho and Jingwei, Tang and
                   Markus, Gross and Vinicius, C Azevedo",
  journal       = "arXiv [cs.CV]",
  abstract      = "Anamorphosis refers to a category of images that are
                   intentionally distorted, making them unrecognizable when
                   viewed directly. Their true form only reveals itself when
                   seen from a specific viewpoint, which can be through some
                   catadioptric device like a mirror or a lens. While the
                   construction of these mathematical devices can be traced back
                   to as early as the 17th century, they are only interpretable
                   when viewed from a specific vantage point and tend to lose
                   meaning when seen normally. In this paper, we revisit these
                   famous optical illusions with a generative twist. With the
                   help of latent rectified flow models, we propose a method to
                   create anamorphic images that still retain a valid
                   interpretation when viewed directly. To this end, we
                   introduce Laplacian Pyramid Warping, a frequency-aware image
                   warping technique key to generating high-quality visuals. Our
                   work extends Visual Anagrams (arXiv:2311.17919) to latent
                   space models and to a wider range of spatial transforms,
                   enabling the creation of novel generative perceptual
                   illusions.",
  month         =  "11~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.08902"
}

@ARTICLE{Bahador2025-od,
  title    = "Vision transformers exhibit human-like biases: Evidence of
              orientation and color selectivity, categorical perception, and
              phase transitions",
  author   = "Bahador, Nooshin",
  journal  = "arXiv [cs.CV]",
  abstract = "This study explored whether Vision Transformers (ViTs) developed
              orientation and color biases similar to those observed in the
              human brain. Using synthetic datasets with controlled variations
              in noise levels, angles, lengths, widths, and colors, we analyzed
              the behavior of ViTs fine-tuned with LoRA. Our findings revealed
              four key insights: First, ViTs exhibited an oblique effect showing
              the lowest angle prediction errors at 180 deg (horizontal) across
              all conditions. Second, angle prediction errors varied by color.
              Errors were highest for bluish hues and lowest for yellowish ones.
              Additionally, clustering analysis of angle prediction errors
              showed that ViTs grouped colors in a way that aligned with human
              perceptual categories. In addition to orientation and color
              biases, we observed phase transition phenomena. While two phase
              transitions occurred consistently across all conditions, the
              training loss curves exhibited delayed transitions when color was
              incorporated as an additional data attribute. Finally, we observed
              that attention heads in certain layers inherently develop
              specialized capabilities, functioning as task-agnostic feature
              extractors regardless of the downstream task. These observations
              suggest that biases and properties arise primarily from
              pre-training on the original dataset which shapes the model's
              foundational representations and the inherent architectural
              constraints of the vision transformer, rather than being solely
              determined by downstream data statistics.",
  month    =  "13~" # apr,
  year     =  2025,
  eprint   = "2504.09393"
}

@ARTICLE{Rubinstein2025-ur,
  title         = "Are we done with object-centric learning?",
  author        = "Rubinstein, Alexander and Prabhu, Ameya and Bethge, Matthias
                   and Oh, Seong Joon",
  journal       = "arXiv [cs.CV]",
  abstract      = "Object-centric learning (OCL) seeks to learn representations
                   that only encode an object, isolated from other objects or
                   background cues in a scene. This approach underpins various
                   aims, including out-of-distribution (OOD) generalization,
                   sample-efficient composition, and modeling of structured
                   environments. Most research has focused on developing
                   unsupervised mechanisms that separate objects into discrete
                   slots in the representation space, evaluated using
                   unsupervised object discovery. However, with recent
                   sample-efficient segmentation models, we can separate objects
                   in the pixel space and encode them independently. This
                   achieves remarkable zero-shot performance on OOD object
                   discovery benchmarks, is scalable to foundation models, and
                   can handle a variable number of slots out-of-the-box. Hence,
                   the goal of OCL methods to obtain object-centric
                   representations has been largely achieved. Despite this
                   progress, a key question remains: How does the ability to
                   separate objects within a scene contribute to broader OCL
                   objectives, such as OOD generalization? We address this by
                   investigating the OOD generalization challenge caused by
                   spurious background cues through the lens of OCL. We propose
                   a novel, training-free probe called $\textbf{Object-Centric
                   Classification with Applied Masks (OCCAM)}$, demonstrating
                   that segmentation-based encoding of individual objects
                   significantly outperforms slot-based OCL methods. However,
                   challenges in real-world applications remain. We provide the
                   toolbox for the OCL community to use scalable object-centric
                   representations, and focus on practical applications and
                   fundamental questions, such as understanding object
                   perception in human cognition. Our code is available
                   $\href{https://github.com/AlexanderRubinstein/OCCAM}{here}$.",
  month         =  "9~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.07092"
}

@ARTICLE{Mallick2025-jy,
  title         = "{D}-feat occlusions: Diffusion features for robustness to
                   partial visual occlusions in object recognition",
  author        = "Mallick, Rupayan and Dong, Sibo and Ruiz, Nataniel and
                   Bargal, Sarah Adel",
  journal       = "arXiv [cs.CV]",
  abstract      = "Applications of diffusion models for visual tasks have been
                   quite noteworthy. This paper targets making classification
                   models more robust to occlusions for the task of object
                   recognition by proposing a pipeline that utilizes a frozen
                   diffusion model. Diffusion features have demonstrated success
                   in image generation and image completion while understanding
                   image context. Occlusion can be posed as an image completion
                   problem by deeming the pixels of the occluder to be
                   `missing.' We hypothesize that such features can help
                   hallucinate object visual features behind occluding objects,
                   and hence we propose using them to enable models to become
                   more occlusion robust. We design experiments to include
                   input-based augmentations as well as feature-based
                   augmentations. Input-based augmentations involve finetuning
                   on images where the occluder pixels are inpainted, and
                   feature-based augmentations involve augmenting classification
                   features with intermediate diffusion features. We demonstrate
                   that our proposed use of diffusion-based features results in
                   models that are more robust to partial object occlusions for
                   both Transformers and ConvNets on ImageNet with simulated
                   occlusions. We also propose a dataset that encompasses
                   real-world occlusions and demonstrate that our method is more
                   robust to partial object occlusions.",
  month         =  "8~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.06432"
}

@ARTICLE{Ning2025-vn,
  title         = "Have we unified image generation and understanding yet? An
                   empirical study of {GPT}-{4o}'s image generation ability",
  author        = "Ning, Li and Jingran, Zhang and Justin, Cui",
  journal       = "arXiv [cs.CV]",
  abstract      = "OpenAI's multimodal GPT-4o has demonstrated remarkable
                   capabilities in image generation and editing, yet its ability
                   to achieve world knowledge-informed semantic
                   synthesis--seamlessly integrating domain knowledge,
                   contextual reasoning, and instruction adherence--remains
                   unproven. In this study, we systematically evaluate these
                   capabilities across three critical dimensions: (1) Global
                   Instruction Adherence, (2) Fine-Grained Editing Precision,
                   and (3) Post-Generation Reasoning. While existing benchmarks
                   highlight GPT-4o's strong capabilities in image generation
                   and editing, our evaluation reveals GPT-4o's persistent
                   limitations: the model frequently defaults to literal
                   interpretations of instructions, inconsistently applies
                   knowledge constraints, and struggles with conditional
                   reasoning tasks. These findings challenge prevailing
                   assumptions about GPT-4o's unified understanding and
                   generation capabilities, exposing significant gaps in its
                   dynamic knowledge integration. Our study calls for the
                   development of more robust benchmarks and training strategies
                   that go beyond surface-level alignment, emphasizing
                   context-aware and reasoning-grounded multimodal generation.",
  month         =  "9~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.08003"
}

@ARTICLE{Davila2025-jx,
  title     = "Adaptation with naturalistic textures in macaque {V1} and {V2}",
  author    = "Davila, Aida and Kohn, Adam",
  journal   = "The Journal of neuroscience: the official journal of the Society
               for Neuroscience",
  publisher = "Society for Neuroscience",
  volume    =  45,
  number    =  14,
  pages     = "e2257232025",
  abstract  = "Adaptation affects neuronal responsivity and selectivity
               throughout the visual hierarchy. However, because most prior
               studies have tailored stimuli to a single brain area of interest,
               we have a poor understanding of how exposure to a particular
               image alters responsivity and tuning at different stages of
               visual processing. Here we assess how adaptation with
               naturalistic textures alters neuronal responsivity and
               selectivity in primary visual cortex (V1) and area V2 of macaque
               monkeys. Neurons in both areas respond to textures, but V2
               neurons are sensitive to higher-order image statistics which do
               not strongly modulate V1 responsivity. We tested the specificity
               of adaptation in each area with textures and spectrally matched
               ``noise'' stimuli. Adaptation reduced responsivity in both V1 and
               V2, but only in V2 was the reduction dependent on the presence of
               higher-order texture statistics. Despite this specificity, the
               texture information provided by single neurons and populations
               was reduced after adaptation, in both V1 and V2. Our results
               suggest that adaptation effects for a given feature are induced
               at the stage of processing that tuning for that feature first
               arises and that stimulus-specific adaptation effects need not
               result in improved sensory encoding.",
  month     =  "2~" # apr,
  year      =  2025,
  keywords  = "V1; V2; adaptation; textures;
               vision;Project/Metamerism;\_To\_read",
  doi       = "10.1523/JNEUROSCI.2257-23.2025",
  pmc       = "PMC11968565",
  pmid      =  39900500,
  issn      = "0270-6474,1529-2401",
  language  = "en"
}

@ARTICLE{Cheng2025-ar,
  title     = "A neural geometry approach comprehensively explains apparently
               conflicting models of visual perceptual learning",
  author    = "Cheng, Yu-Ang and Sanayei, Mehdi and Chen, Xing and Jia, Ke and
               Li, Sheng and Fang, Fang and Watanabe, Takeo and Thiele,
               Alexander and Zhang, Ru-Yuan",
  journal   = "Nature human behaviour",
  publisher = "Springer Science and Business Media LLC",
  pages     = "1--18",
  abstract  = "Visual perceptual learning (VPL), defined as long-term
               improvement in a visual task, is considered a crucial tool for
               elucidating underlying visual and brain plasticity. Previous
               studies have proposed several neural models of VPL, including
               changes in neural tuning or in noise correlations. Here, to
               adjudicate different models, we propose that all neural changes
               at single units can be conceptualized as geometric
               transformations of population response manifolds in a
               high-dimensional neural space. Following this neural geometry
               approach, we identified neural manifold shrinkage due to reduced
               trial-by-trial population response variability, rather than
               tuning or correlation changes, as the primary mechanism of VPL.
               Furthermore, manifold shrinkage successfully explains VPL effects
               across artificial neural responses in deep neural networks,
               multivariate blood-oxygenation-level-dependent signals in humans
               and multiunit activities in monkeys. These converging results
               suggest that our neural geometry approach comprehensively
               explains a wide range of empirical results and reconciles
               previously conflicting models of VPL.",
  month     =  "31~" # mar,
  year      =  2025,
  keywords  = "Project/Metamerism;\_To\_read",
  doi       = "10.1038/s41562-025-02149-x",
  pmid      =  40164913,
  issn      = "2397-3374,2397-3374",
  language  = "en"
}

@ARTICLE{de-Haas2018-en,
  title     = "Feature-location effects in the Thatcher illusion",
  author    = "de Haas, Benjamin and Schwarzkopf, D Samuel",
  journal   = "Journal of vision",
  publisher = "The Association for Research in Vision and Ophthalmology",
  volume    =  18,
  number    =  4,
  pages     =  16,
  abstract  = "Face perception is impaired for inverted images, and a prominent
               example of this is the Thatcher illusion: ``Thatcherized'' (i.e.,
               rotated) eyes and mouths make a face look grotesque, but only if
               the whole face is seen upright rather than inverted. Inversion
               effects are often interpreted as evidence for configural face
               processing. However, recent findings have led to the alternative
               proposal that the Thatcher illusion rests on orientation
               sensitivity for isolated facial regions. Here, we tested whether
               the Thatcher effect depends not only on the orientation of facial
               regions but also on their visual-field location. Using a
               match-to-sample task with isolated eye and mouth regions we found
               a significant Feature × Location interaction. Observers were
               better at discriminating Thatcherized from normal eyes in the
               upper compared to the lower visual field, and vice versa for
               mouths. These results show that inversion effects can at least
               partly be driven by nonconfigural factors and that one of these
               factors is a match between facial features and their typical
               visual-field location. This echoes recent results showing
               feature-location effects in face individuation. We discuss the
               role of these findings for the hypothesis that spatial and
               feature tuning in the ventral stream are linked.",
  month     =  "1~" # apr,
  year      =  2018,
  keywords  = "face; illusions; mouth; visual fields; eye;
               atypical;Project/Density\_IQA",
  doi       = "10.1167/18.4.16",
  pmid      =  29710306,
  issn      = "1534-7362",
  language  = "en"
}

@ARTICLE{Yujin2025-qb,
  title         = "{S2R}-{HDR}: A Large-Scale Rendered Dataset for {HDR} Fusion",
  author        = "Yujin, Wang and Jiarui, Wu and Yichen, Bian and Fan, Zhang
                   and Tianfan, Xue",
  journal       = "arXiv [cs.CV]",
  abstract      = "The generalization of learning-based high dynamic range (HDR)
                   fusion is often limited by the availability of training data,
                   as collecting large-scale HDR images from dynamic scenes is
                   both costly and technically challenging. To address these
                   challenges, we propose S2R-HDR, the first large-scale
                   high-quality synthetic dataset for HDR fusion, with 24,000
                   HDR samples. Using Unreal Engine 5, we design a diverse set
                   of realistic HDR scenes that encompass various dynamic
                   elements, motion types, high dynamic range scenes, and
                   lighting. Additionally, we develop an efficient rendering
                   pipeline to generate realistic HDR images. To further
                   mitigate the domain gap between synthetic and real-world
                   data, we introduce S2R-Adapter, a domain adaptation designed
                   to bridge this gap and enhance the generalization ability of
                   models. Experimental results on real-world datasets
                   demonstrate that our approach achieves state-of-the-art HDR
                   reconstruction performance. Dataset and code will be
                   available at https://openimaginglab.github.io/S2R-HDR.",
  month         =  "10~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.07667",
  keywords      = "Project/NaturalContrast"
}

@ARTICLE{Dorszewski2025-bq,
  title         = "From colors to classes: Emergence of concepts in vision
                   Transformers",
  author        = "Dorszewski, Teresa and Tětková, Lenka and Jenssen, Robert and
                   Hansen, Lars Kai and Wickstrøm, Kristoffer Knutsen",
  journal       = "arXiv [cs.CV]",
  abstract      = "Vision Transformers (ViTs) are increasingly utilized in
                   various computer vision tasks due to their powerful
                   representation capabilities. However, it remains understudied
                   how ViTs process information layer by layer. Numerous studies
                   have shown that convolutional neural networks (CNNs) extract
                   features of increasing complexity throughout their layers,
                   which is crucial for tasks like domain adaptation and
                   transfer learning. ViTs, lacking the same inductive biases as
                   CNNs, can potentially learn global dependencies from the
                   first layers due to their attention mechanisms. Given the
                   increasing importance of ViTs in computer vision, there is a
                   need to improve the layer-wise understanding of ViTs. In this
                   work, we present a novel, layer-wise analysis of concepts
                   encoded in state-of-the-art ViTs using neuron labeling. Our
                   findings reveal that ViTs encode concepts with increasing
                   complexity throughout the network. Early layers primarily
                   encode basic features such as colors and textures, while
                   later layers represent more specific classes, including
                   objects and animals. As the complexity of encoded concepts
                   increases, the number of concepts represented in each layer
                   also rises, reflecting a more diverse and specific set of
                   features. Additionally, different pretraining strategies
                   influence the quantity and category of encoded concepts, with
                   finetuning to specific downstream tasks generally reducing
                   the number of encoded concepts and shifting the concepts to
                   more relevant categories.",
  month         =  "31~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.24071"
}

@ARTICLE{Greenwood2025-zo,
  title    = "The development of visual acuity and crowding: Finding the balance
              between fine detail and 'gist' processing",
  author   = "Greenwood, John A and Kyprianou, Marilia and Dekker, Tessa M",
  journal  = "bioRxiv",
  pages    = "2025.03.27.645750",
  abstract = "The adult visual system is characterised by high-resolution foveal
              vision and a peripheral field limited by crowding, the disruption
              to object recognition in clutter that gives a summary 'gist' in
              place of fine detail. In children, crowding is elevated in the
              fovea, with estimates of the age where foveal crowding drops to
              adult-like levels varying widely from 5 to 12+ years. As crowding
              restricts key processes like reading, better characterisation of
              this developmental trajectory is critical. We developed methods
              optimised to measure crowding in children, whereby adults and
              typically developing children aged 3-13 years (n=119) judged the
              orientation of a foveal 'VacMan' target either in isolation or
              surrounded by 'ghost' flankers. Stimulus sizes and separation were
              scaled adaptively. In the isolated condition, acuity (measured as
              gap-size thresholds) dropped rapidly to adult-like levels at 5-6
              years. Thresholds rose when flankers were added, demonstrating
              foveal crowding at all ages. These elevations were highest at 3-4
              years and persisted at 5-6 years, dropping to adult-like levels at
              7-8 years. A meta-analysis of our thresholds and those from 11
              prior studies reveals the same overall developmental trajectory,
              despite widely varying stimuli and procedures, with some divergent
              estimates likely driven by measurement confounds such as
              underestimated adult crowding levels. We further demonstrate that
              developmental foveal crowding shows the same selectivity for
              target-flanker similarity as peripheral crowding, consistent with
              common mechanisms. This prolonged development of crowding reveals
              a shifting balance in the visual system between the processing of
              fine detail vs. the 'gist' of the scene.",
  month    =  "28~" # mar,
  year     =  2025,
  keywords = "Project/Metamerism",
  doi      = "10.1101/2025.03.27.645750",
  language = "en"
}

@ARTICLE{Assa2025-xn,
  title    = "Temporal coding enables hyperacuity in event based vision",
  author   = "Assa, Eldad and Rivkind, Alexander and Kreiserman, Michael and
              Khan, Fahad Shahbaz and Khan, Salman and Ahissar, Ehud",
  journal  = "bioRxiv",
  pages    = "2025.03.25.645190",
  abstract = "AbstractThe fact that the eyes are constantly in motion, even
              during ‘fixation’, entails that the spike times of retinal outputs
              carry information about the visual scene even when the scene is
              static. Moreover, this motion implies that fine details of the
              visual scene could not be decoded from pure spatial retinal
              representations due to smearing. Understanding the interplay of
              temporal and spatial information in visual processing is thus
              pivotal for both biological research and bio-inspired
              computer-vision applications. In this study, we consider data from
              a popular event-based camera that was designed to emulate the
              function of a biological retina in hardware. Similarly to
              biological eye, and in contrast to standard frame-based cameras,
              this camera outputs an asynchronous sequence of “spike” events. We
              used this camera to obtain dataset of event streams of tiny
              images, i.e., images whose recognition is impaired by
              photosensor’s pixelization and thus their recognition requires
              hyperacuity. Using these datasets we demonstrate here the
              superiority of event-based spatio-temporal coding over frame-based
              spatial coding in the recognition of tiny images by artificial
              neural networks (ANNs). We further demonstrate the benefits of
              event sequences for unsupervised learning. Interestingly, Vernier
              hyperacuity, which is a standard measure of shape hyperacuity,
              emerged in ANNs following training on tiny images, resembling the
              natural hyperacuity observed in humans. Our findings underscore
              the essential role of precise temporal information in visual
              processing, offering insights for advancing both biological
              understanding and bio-inspired engineering of visual perception.",
  month    =  "26~" # mar,
  year     =  2025,
  doi      = "10.1101/2025.03.25.645190",
  language = "en"
}

@ARTICLE{Lonnqvist2025-yw,
  title         = "Contour integration underlies human-like vision",
  author        = "Lonnqvist, Ben and Scialom, Elsa and Gokce, Abdulkadir and
                   Merchant, Zehra and Herzog, Michael H and Schrimpf, Martin",
  journal       = "arXiv [cs.CV]",
  abstract      = "Despite the tremendous success of deep learning in computer
                   vision, models still fall behind humans in generalizing to
                   new input distributions. Existing benchmarks do not
                   investigate the specific failure points of models by
                   analyzing performance under many controlled conditions. Our
                   study systematically dissects where and why models struggle
                   with contour integration -- a hallmark of human vision -- by
                   designing an experiment that tests object recognition under
                   various levels of object fragmentation. Humans (n=50) perform
                   at high accuracy, even with few object contours present. This
                   is in contrast to models which exhibit substantially lower
                   sensitivity to increasing object contours, with most of the
                   over 1,000 models we tested barely performing above chance.
                   Only at very large scales ($\sim5B$ training dataset size) do
                   models begin to approach human performance. Importantly,
                   humans exhibit an integration bias -- a preference towards
                   recognizing objects made up of directional fragments over
                   directionless fragments. We find that not only do models that
                   share this property perform better at our task, but that this
                   bias also increases with model training dataset size, and
                   training models to exhibit contour integration leads to high
                   shape bias. Taken together, our results suggest that contour
                   integration is a hallmark of object vision that underlies
                   object recognition performance, and may be a mechanism
                   learned from data at scale.",
  month         =  "7~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.05253",
  keywords      = "\_To\_read"
}

@ARTICLE{Duay2025-ik,
  title    = "Theoretical physical color gamuts define luminosity and
              naturalness perceptual limits in natural scenes",
  author   = "Duay, Killian and Nagai, Takehiro",
  journal  = "bioRxiv",
  pages    = "2025.04.04.647230",
  abstract = "ABSTRACTRealism in augmented reality (AR) hinges on the seamless
              blending of virtual elements into real-world environments. One
              possible factor influencing this realism may be the physical
              gamut: an internal representation of all perceivable colors within
              a natural scene. Previous studies on luminosity thresholds suggest
              that this gamut, rooted in optimal colors theory, constrains
              perceptual judgments. While promising, such findings were based on
              abstract and two-dimensional stimuli only. Before extending this
              framework to more realistic AR scenarios, an essential next step
              is to assess whether the physical gamut theory also applies to
              naturalistic stimuli. This study addresses that gap. Our results
              reveal that the physical gamut remains a valid construct for
              natural objects viewed in realistic scenes. Moreover, observers’
              judgments of luminosity thresholds appear guided not only by a
              criterion of self-luminosity, but also by an implicit sense of
              naturalness. These insights pave the way for exploring AR realism
              through the lens of physical gamut theory.",
  month    =  "5~" # apr,
  year     =  2025,
  keywords = "Project/NaturalContrast;\_To\_read",
  doi      = "10.1101/2025.04.04.647230",
  language = "en"
}

@ARTICLE{Qingtao2025-dm,
  title         = "Probability density geodesics in image diffusion latent space",
  author        = "Qingtao, Yu and Jaskirat, Singh and Zhaoyuan, Yang and Tu,
                   Peter Henry and Jing, Zhang and Hongdong, Li and Richard,
                   Hartley and Dylan, Campbell",
  journal       = "arXiv [cs.CV]",
  abstract      = "Diffusion models indirectly estimate the probability density
                   over a data space, which can be used to study its structure.
                   In this work, we show that geodesics can be computed in
                   diffusion latent space, where the norm induced by the
                   spatially-varying inner product is inversely proportional to
                   the probability density. In this formulation, a path that
                   traverses a high density (that is, probable) region of image
                   latent space is shorter than the equivalent path through a
                   low density region. We present algorithms for solving the
                   associated initial and boundary value problems and show how
                   to compute the probability density along the path and the
                   geodesic distance between two points. Using these techniques,
                   we analyze how closely video clips approximate geodesics in a
                   pre-trained image diffusion space. Finally, we demonstrate
                   how these techniques can be applied to training-free image
                   sequence interpolation and extrapolation, given a pre-trained
                   image diffusion model.",
  month         =  "9~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.06675",
  keywords      = "\_To\_read;Project/Density\_IQA"
}

@ARTICLE{Jian2025-wn,
  title         = "Illusion spaces in {VR}: The interplay between size and taper
                   angle perception in grasping",
  author        = "Jian, Zhang and Wafa, Johal and Jarrod, Knibbe",
  journal       = "arXiv [cs.HC]",
  abstract      = "Leveraging the integration of visual and proprioceptive cues,
                   research has uncovered various perception thresholds in VR
                   that can be exploited to support haptic feedback for
                   grasping. While previous studies have explored individual
                   dimensions, such as size, the combined effect of multiple
                   geometric properties on perceptual illusions remains poorly
                   understood. We present a two-alternative forced choice study
                   investigating the perceptual interplay between object size
                   and taper angle. We introduce an illusion space model,
                   providing detailed insights into how physical and virtual
                   object configurations affect human perception. Our insights
                   reveal how, for example, as virtual sizes increase, users
                   perceive that taper angles increase, and as virtual angles
                   decrease, users overestimate sizes. We provide a mathematical
                   model of the illusion space, and an associated tool, which
                   can be used as a guide for the design of future VR haptic
                   devices and for proxy object selections.",
  month         =  "8~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2504.05791"
}

@ARTICLE{Shunsuke2025-jz,
  title         = "Reconstruction-free anomaly detection with diffusion models
                   via direct latent likelihood evaluation",
  author        = "Shunsuke, Sakai and Tatsuhito, Hasegawa",
  journal       = "arXiv [cs.CV]",
  abstract      = "Diffusion models, with their robust distribution
                   approximation capabilities, have demonstrated excellent
                   performance in anomaly detection. However, conventional
                   reconstruction-based approaches rely on computing the
                   reconstruction error between the original and denoised
                   images, which requires careful noise-strength tuning and over
                   ten network evaluations per input-leading to significantly
                   slower detection speeds. To address these limitations, we
                   propose a novel diffusion-based anomaly detection method that
                   circumvents the need for resource-intensive reconstruction.
                   Instead of reconstructing the input image, we directly infer
                   its corresponding latent variables and measure their density
                   under the Gaussian prior distribution. Remarkably, the prior
                   density proves effective as an anomaly score even when using
                   a short partial diffusion process of only 2-5 steps. We
                   evaluate our method on the MVTecAD dataset, achieving an AUC
                   of 0.991 at 15 FPS, thereby setting a new state-of-the-art
                   speed-AUC anomaly detection trade-off.",
  month         =  "8~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.05662"
}

@ARTICLE{Yavuz2025-ur,
  title         = "Variational Self-Supervised Learning",
  author        = "Yavuz, Mehmet Can and Berrin, Yanikoglu",
  journal       = "arXiv [cs.LG]",
  abstract      = "We present Variational Self-Supervised Learning (VSSL), a
                   novel framework that combines variational inference with
                   self-supervised learning to enable efficient, decoder-free
                   representation learning. Unlike traditional VAEs that rely on
                   input reconstruction via a decoder, VSSL symmetrically
                   couples two encoders with Gaussian outputs. A
                   momentum-updated teacher network defines a dynamic,
                   data-dependent prior, while the student encoder produces an
                   approximate posterior from augmented views. The
                   reconstruction term in the ELBO is replaced with a cross-view
                   denoising objective, preserving the analytical tractability
                   of Gaussian KL divergence. We further introduce cosine-based
                   formulations of KL and log-likelihood terms to enhance
                   semantic alignment in high-dimensional latent spaces.
                   Experiments on CIFAR-10, CIFAR-100, and ImageNet-100 show
                   that VSSL achieves competitive or superior performance to
                   leading self-supervised methods, including BYOL and MoCo V3.
                   VSSL offers a scalable, probabilistically grounded approach
                   to learning transferable representations without generative
                   reconstruction, bridging the gap between variational modeling
                   and modern self-supervised techniques.",
  month         =  "6~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2504.04318"
}

@ARTICLE{Litao2025-ba,
  title         = "Attention in Diffusion Model: A Survey",
  author        = "Litao, Hua and Fan, Liu and Jie, Su and Xingyu, Miao and
                   Zizhou, Ouyang and Zeyu, Wang and Runze, Hu and Zhenyu, Wen
                   and Bing, Zhai and Yang, Long and Haoran, Duan and Yuan, Zhou",
  journal       = "arXiv [cs.LG]",
  abstract      = "Attention mechanisms have become a foundational component in
                   diffusion models, significantly influencing their capacity
                   across a wide range of generative and discriminative tasks.
                   This paper presents a comprehensive survey of attention
                   within diffusion models, systematically analysing its roles,
                   design patterns, and operations across different modalities
                   and tasks. We propose a unified taxonomy that categorises
                   attention-related modifications into parts according to the
                   structural components they affect, offering a clear lens
                   through which to understand their functional diversity. In
                   addition to reviewing architectural innovations, we examine
                   how attention mechanisms contribute to performance
                   improvements in diverse applications. We also identify
                   current limitations and underexplored areas, and outline
                   potential directions for future research. Our study provides
                   valuable insights into the evolving landscape of diffusion
                   models, with a particular focus on the integrative and
                   ubiquitous role of attention.",
  month         =  "1~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2504.03738"
}

@ARTICLE{Khajuria2024-yb,
  title         = "How structured are the representations in transformer-based
                   vision encoders? An analysis of multi-object representations
                   in vision-language models",
  author        = "Khajuria, Tarun and Dias, Braian Olmiro and Aru, Jaan",
  journal       = "arXiv [cs.CV]",
  abstract      = "Forming and using symbol-like structured representations for
                   reasoning has been considered essential for generalising over
                   novel inputs. The primary tool that allows generalisation
                   outside training data distribution is the ability to abstract
                   away irrelevant information into a compact form relevant to
                   the task. An extreme form of such abstract representations is
                   symbols. Humans make use of symbols to bind information while
                   abstracting away irrelevant parts to utilise the information
                   consistently and meaningfully. This work estimates the state
                   of such structured representations in vision encoders.
                   Specifically, we evaluate image encoders in large
                   vision-language pre-trained models to address the question of
                   which desirable properties their representations lack by
                   applying the criteria of symbolic structured reasoning
                   described for LLMs to the image models. We test the
                   representation space of image encoders like VIT, BLIP, CLIP,
                   and FLAVA to characterise the distribution of the object
                   representations in these models. In particular, we create
                   decoding tasks using multi-object scenes from the COCO
                   dataset, relating the token space to its input content for
                   various objects in the scene. We use these tasks to
                   characterise the network's token and layer-wise information
                   modelling. Our analysis highlights that the CLS token, used
                   for the downstream task, only focuses on a few objects
                   necessary for the trained downstream task. Still, other
                   individual objects are well-modelled separately by the tokens
                   in the network originating from those objects. We further
                   observed a widespread distribution of scene information. This
                   demonstrates that information is far more entangled in tokens
                   than optimal for representing objects similar to symbols.
                   Given these symbolic properties, we show the network dynamics
                   that cause failure modes of these models on basic downstream
                   tasks in a multi-object scene.",
  month         =  "13~" # jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2406.09067"
}

@ARTICLE{Rahimi2025-sk,
  title         = "Explanations of large language models explain language
                   representations in the brain",
  author        = "Rahimi, Maryam and Yaghoobzadeh, Yadollah and Daliri,
                   Mohammad Reza",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LLMs) not only exhibit human-like
                   performance but also share computational principles with the
                   brain's language processing mechanisms. While prior research
                   has focused on mapping LLMs' internal representations to
                   neural activity, we propose a novel approach using
                   explainable AI (XAI) to strengthen this link. Applying
                   attribution methods, we quantify the influence of preceding
                   words on LLMs' next-word predictions and use these
                   explanations to predict fMRI data from participants listening
                   to narratives. We find that attribution methods robustly
                   predict brain activity across the language network, revealing
                   a hierarchical pattern: explanations from early layers align
                   with the brain's initial language processing stages, while
                   later layers correspond to more advanced stages.
                   Additionally, layers with greater influence on next-word
                   prediction$\unicode{x2014}$reflected in higher attribution
                   scores$\unicode{x2014}$demonstrate stronger brain alignment.
                   These results underscore XAI's potential for exploring the
                   neural basis of language and suggest brain alignment for
                   assessing the biological plausibility of explanation methods.",
  month         =  "20~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2502.14671"
}

@ARTICLE{Boehm2025-iz,
  title         = "Understanding cross-model perceptual invariances through
                   ensemble metamers",
  author        = "Boehm, Lukas and Mueller, Jonas Leo and Loeffler, Christoffer
                   and Schwinn, Leo and Eskofier, Bjoern and Zanca, Dario",
  journal       = "arXiv [cs.CV]",
  abstract      = "Understanding the perceptual invariances of artificial neural
                   networks is essential for improving explainability and
                   aligning models with human vision. Metamers - stimuli that
                   are physically distinct yet produce identical neural
                   activations - serve as a valuable tool for investigating
                   these invariances. We introduce a novel approach to metamer
                   generation by leveraging ensembles of artificial neural
                   networks, capturing shared representational subspaces across
                   diverse architectures, including convolutional neural
                   networks and vision transformers. To characterize the
                   properties of the generated metamers, we employ a suite of
                   image-based metrics that assess factors such as semantic
                   fidelity and naturalness. Our findings show that
                   convolutional neural networks generate more recognizable and
                   human-like metamers, while vision transformers produce
                   realistic but less transferable metamers, highlighting the
                   impact of architectural biases on representational
                   invariances.",
  month         =  "2~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.01739",
  keywords      = "Project/Metamerism;\_To\_read"
}

@ARTICLE{Yang2025-iz,
  title         = "Towards physically plausible video generation via {VLM}
                   planning",
  author        = "Yang, Xindi and Li, Baolu and Zhang, Yiming and Yin, Zhenfei
                   and Bai, Lei and Ma, Liqian and Wang, Zhiyong and Cai,
                   Jianfei and Wong, Tien-Tsin and Lu, Huchuan and Jia, Xu",
  journal       = "arXiv [cs.CV]",
  abstract      = "Video diffusion models (VDMs) have advanced significantly in
                   recent years, enabling the generation of highly realistic
                   videos and drawing the attention of the community in their
                   potential as world simulators. However, despite their
                   capabilities, VDMs often fail to produce physically plausible
                   videos due to an inherent lack of understanding of physics,
                   resulting in incorrect dynamics and event sequences. To
                   address this limitation, we propose a novel two-stage
                   image-to-video generation framework that explicitly
                   incorporates physics. In the first stage, we employ a Vision
                   Language Model (VLM) as a coarse-grained motion planner,
                   integrating chain-of-thought and physics-aware reasoning to
                   predict a rough motion trajectories/changes that approximate
                   real-world physical dynamics while ensuring the inter-frame
                   consistency. In the second stage, we use the predicted motion
                   trajectories/changes to guide the video generation of a VDM.
                   As the predicted motion trajectories/changes are rough, noise
                   is added during inference to provide freedom to the VDM in
                   generating motion with more fine details. Extensive
                   experimental results demonstrate that our framework can
                   produce physically plausible motion, and comparative
                   evaluations highlight the notable superiority of our approach
                   over existing methods. More video results are available on
                   our Project Page:
                   https://madaoer.github.io/projects/physically\_plausible\_video\_generation.",
  month         =  "30~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.23368"
}

@ARTICLE{Wen2025-su,
  title         = "{FoundationStereo}: Zero-shot stereo matching",
  author        = "Wen, Bowen and Trepte, Matthew and Aribido, Joseph and Kautz,
                   Jan and Gallo, Orazio and Birchfield, Stan",
  journal       = "arXiv [cs.CV]",
  abstract      = "Tremendous progress has been made in deep stereo matching to
                   excel on benchmark datasets through per-domain fine-tuning.
                   However, achieving strong zero-shot generalization - a
                   hallmark of foundation models in other computer vision tasks
                   - remains challenging for stereo matching. We introduce
                   FoundationStereo, a foundation model for stereo depth
                   estimation designed to achieve strong zero-shot
                   generalization. To this end, we first construct a large-scale
                   (1M stereo pairs) synthetic training dataset featuring large
                   diversity and high photorealism, followed by an automatic
                   self-curation pipeline to remove ambiguous samples. We then
                   design a number of network architecture components to enhance
                   scalability, including a side-tuning feature backbone that
                   adapts rich monocular priors from vision foundation models to
                   mitigate the sim-to-real gap, and long-range context
                   reasoning for effective cost volume filtering. Together,
                   these components lead to strong robustness and accuracy
                   across domains, establishing a new standard in zero-shot
                   stereo depth estimation. Project page:
                   https://nvlabs.github.io/FoundationStereo/",
  month         =  "16~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.09898",
  keywords      = "Project/DepthModel"
}

@ARTICLE{Stone2024-tq,
  title         = "Learning visual composition through improved semantic
                   guidance",
  author        = "Stone, Austin and Soltau, Hagen and Geirhos, Robert and Yi,
                   Xi and Xia, Ye and Cao, Bingyi and Chen, Kaifeng and Ogale,
                   Abhijit and Shlens, Jonathon",
  journal       = "arXiv [cs.CV]",
  abstract      = "Visual imagery does not consist of solitary objects, but
                   instead reflects the composition of a multitude of fluid
                   concepts. While there have been great advances in visual
                   representation learning, such advances have focused on
                   building better representations for a small number of
                   discrete objects bereft of an understanding of how these
                   objects are interacting. One can observe this limitation in
                   representations learned through captions or contrastive
                   learning -- where the learned model treats an image
                   essentially as a bag of words. Several works have attempted
                   to address this limitation through the development of bespoke
                   learned architectures to directly address the shortcomings in
                   compositional learning. In this work, we focus on simple, and
                   scalable approaches. In particular, we demonstrate that by
                   substantially improving weakly labeled data, i.e. captions,
                   we can vastly improve the performance of standard contrastive
                   learning approaches. Previous CLIP models achieved near
                   chance rate on challenging tasks probing compositional
                   learning. However, our simple approach boosts performance of
                   CLIP substantially and surpasses all bespoke architectures.
                   Furthermore, we showcase our results on a relatively new
                   captioning benchmark derived from DOCCI. We demonstrate
                   through a series of ablations that a standard CLIP model
                   trained with enhanced data may demonstrate impressive
                   performance on image retrieval tasks.",
  month         =  "19~" # dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2412.15396"
}

@ARTICLE{Zhang2025-pk,
  title         = "Morpheus: Benchmarking physical reasoning of video generative
                   models with real physical experiments",
  author        = "Zhang, Chenyu and Cherniavskii, Daniil and Zadaianchuk,
                   Andrii and Tragoudaras, Antonios and Vozikis, Antonios and
                   Nijdam, Thijmen and Derck W E, Prinzhorn and Bodracska, Mark
                   and Sebe, Nicu and Gavves, Efstratios",
  journal       = "arXiv [cs.CV]",
  abstract      = "Recent advances in image and video generation raise hopes
                   that these models possess world modeling capabilities, the
                   ability to generate realistic, physically plausible videos.
                   This could revolutionize applications in robotics, autonomous
                   driving, and scientific simulation. However, before treating
                   these models as world models, we must ask: Do they adhere to
                   physical conservation laws? To answer this, we introduce
                   Morpheus, a benchmark for evaluating video generation models
                   on physical reasoning. It features 80 real-world videos
                   capturing physical phenomena, guided by conservation laws.
                   Since artificial generations lack ground truth, we assess
                   physical plausibility using physics-informed metrics
                   evaluated with respect to infallible conservation laws known
                   per physical setting, leveraging advances in physics-informed
                   neural networks and vision-language foundation models. Our
                   findings reveal that even with advanced prompting and video
                   conditioning, current models struggle to encode physical
                   principles despite generating aesthetically pleasing videos.
                   All data, leaderboard, and code are open-sourced at our
                   project page.",
  month         =  "3~" # apr,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2504.02918",
  keywords      = "Project/Density\_IQA"
}

@ARTICLE{Liu2025-vk,
  title         = "Advances and challenges in foundation agents: From
                   brain-inspired intelligence to evolutionary, collaborative,
                   and safe systems",
  author        = "Liu, Bang and Li, Xinfeng and Zhang, Jiayi and Wang, Jinlin
                   and He, Tanjin and Hong, Sirui and Liu, Hongzhang and Zhang,
                   Shaokun and Song, Kaitao and Zhu, Kunlun and Cheng, Yuheng
                   and Wang, Suyuchen and Wang, Xiaoqiang and Luo, Yuyu and Jin,
                   Haibo and Zhang, Peiyan and Liu, Ollie and Chen, Jiaqi and
                   Zhang, Huan and Yu, Zhaoyang and Shi, Haochen and Li, Boyan
                   and Wu, Dekun and Teng, Fengwei and Jia, Xiaojun and Xu,
                   Jiawei and Xiang, Jinyu and Lin, Yizhang and Liu, Tianming
                   and Liu, Tongliang and Su, Yu and Sun, Huan and Berseth, Glen
                   and Nie, Jianyun and Foster, Ian and Ward, Logan and Wu,
                   Qingyun and Gu, Yu and Zhuge, Mingchen and Tang, Xiangru and
                   Wang, Haohan and You, Jiaxuan and Wang, Chi and Pei, Jian and
                   Yang, Qiang and Qi, Xiaoliang and Wu, Chenglin",
  journal       = "arXiv [cs.AI]",
  abstract      = "The advent of large language models (LLMs) has catalyzed a
                   transformative shift in artificial intelligence, paving the
                   way for advanced intelligent agents capable of sophisticated
                   reasoning, robust perception, and versatile action across
                   diverse domains. As these agents increasingly drive AI
                   research and practical applications, their design,
                   evaluation, and continuous improvement present intricate,
                   multifaceted challenges. This survey provides a comprehensive
                   overview, framing intelligent agents within a modular,
                   brain-inspired architecture that integrates principles from
                   cognitive science, neuroscience, and computational research.
                   We structure our exploration into four interconnected parts.
                   First, we delve into the modular foundation of intelligent
                   agents, systematically mapping their cognitive, perceptual,
                   and operational modules onto analogous human brain
                   functionalities, and elucidating core components such as
                   memory, world modeling, reward processing, and emotion-like
                   systems. Second, we discuss self-enhancement and adaptive
                   evolution mechanisms, exploring how agents autonomously
                   refine their capabilities, adapt to dynamic environments, and
                   achieve continual learning through automated optimization
                   paradigms, including emerging AutoML and LLM-driven
                   optimization strategies. Third, we examine collaborative and
                   evolutionary multi-agent systems, investigating the
                   collective intelligence emerging from agent interactions,
                   cooperation, and societal structures, highlighting parallels
                   to human social dynamics. Finally, we address the critical
                   imperative of building safe, secure, and beneficial AI
                   systems, emphasizing intrinsic and extrinsic security
                   threats, ethical alignment, robustness, and practical
                   mitigation strategies necessary for trustworthy real-world
                   deployment.",
  month         =  "31~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2504.01990"
}

@ARTICLE{Zhang2025-gw,
  title    = "Object categorization based on abstract concepts by monkeys,
              humans, and vision models",
  author   = "Zhang, Han and Zheng, Zhihao and Hu, Jiaqi and Wang, Qiao and Xu,
              Mengya and Zhou, Zhaojiayi and Li, Zixuan and Okazawa, Gouki",
  journal  = "bioRxiv",
  pages    = "2025.04.02.646407",
  abstract = "Viewing an object in the external world evokes a rich variety of
              concepts associated with it in the human mind, such as whether it
              is animate or not, man-made or not, big or small, and countless
              other attributes. This concept recognition seems to depend on the
              human-specific knowledge about these objects. However, studies
              have shown that neurons in visual areas of non-human primates
              encode some object concepts, raising questions about the extent to
              which object concepts are uniquely possessed by humans. Here, we
              show that macaque monkeys can rapidly learn to classify natural
              object images according to a surprisingly rich set of rules
              defined by what humans would call abstract concepts. We tested
              more than 10 binary classification tasks, including animate versus
              inanimate, natural versus man-made objects, and mammalian versus
              non-mammalian animals. The monkeys learned each rule in a few
              days, generalized the learned rules to new images of objects they
              had probably never seen before and made error patterns consistent
              with human judgments. Visual deep neural networks (DNNs) could
              also perform the tasks that monkeys could learn, whereas both DNNs
              and monkeys failed to classify some less common abstract concepts.
              Thus, the monkeys' successful classification likely depended on
              whether concepts were embedded in natural images as visual
              features that could be extracted by high-level visual processing.
              The observed capacity of primate brains may subserve the formation
              of rich abstract concepts that the human mind possesses.",
  month    =  "3~" # apr,
  year     =  2025,
  doi      = "10.1101/2025.04.02.646407"
}

@ARTICLE{Hu2024-na,
  title         = "Interpreting low-level vision models with causal effect maps",
  author        = "Hu, Jinfan and Gu, Jinjin and Yu, Shiyao and Yu, Fanghua and
                   Li, Zheyuan and You, Zhiyuan and Lu, Chaochao and Dong, Chao",
  journal       = "arXiv [cs.CV]",
  abstract      = "Deep neural networks have significantly improved the
                   performance of low-level vision tasks but also increased the
                   difficulty of interpretability. A deep understanding of deep
                   models is beneficial for both network design and practical
                   reliability. To take up this challenge, we introduce
                   causality theory to interpret low-level vision models and
                   propose a model-/task-agnostic method called Causal Effect
                   Map (CEM). With CEM, we can visualize and quantify the
                   input-output relationships on either positive or negative
                   effects. After analyzing various low-level vision tasks with
                   CEM, we have reached several interesting insights, such as:
                   (1) Using more information of input images (e.g., larger
                   receptive field) does NOT always yield positive outcomes. (2)
                   Attempting to incorporate mechanisms with a global receptive
                   field (e.g., channel attention) into image denoising may
                   prove futile. (3) Integrating multiple tasks to train a
                   general model could encourage the network to prioritize local
                   information over global context. Based on the causal effect
                   theory, the proposed diagnostic tool can refresh our common
                   knowledge and bring a deeper understanding of low-level
                   vision models. Codes are available at
                   https://github.com/J-FHu/CEM.",
  month         =  "29~" # jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2407.19789"
}

@ARTICLE{Zhi2025-gz,
  title         = "Visual acuity consistent foveated rendering towards retinal
                   resolution",
  author        = "Zhi, Zhang and Meng, Gai and Sheng, Li",
  journal       = "arXiv [cs.GR]",
  abstract      = "Prior foveated rendering methods often suffer from a
                   limitation where the shading load escalates with increasing
                   display resolution, leading to decreased efficiency,
                   particularly when dealing with retinal-level resolutions. To
                   tackle this challenge, we begin with the essence of the human
                   visual system (HVS) perception and present visual
                   acuity-consistent foveated rendering (VaFR), aiming to
                   achieve exceptional rendering performance at retinal-level
                   resolutions. Specifically, we propose a method with a novel
                   log-polar mapping function derived from the human visual
                   acuity model, which accommodates the natural bandwidth of the
                   visual system. This mapping function and its associated
                   shading rate guarantee a consistent output of rendering
                   information, regardless of variations in the display
                   resolution of the VR HMD. Consequently, our VaFR outperforms
                   alternative methods, improving rendering speed while
                   preserving perceptual visual quality, particularly when
                   operating at retinal resolutions. We validate our approach
                   using both the rasterization and ray-casting rendering
                   pipelines. We also validate our approach using different
                   binocular rendering strategies for HMD devices. In diverse
                   testing scenarios, our approach delivers better perceptual
                   visual quality than prior foveated rendering while achieving
                   an impressive speedup of 6.5$\times$-9.29$\times$ for
                   deferred rendering of 3D scenarios and an even more powerful
                   speedup of 10.4$\times$-16.4$\times$ for ray-casting at
                   retinal resolution. Additionally, our approach significantly
                   enhances the rendering performance of binocular 8K path
                   tracing, achieving smooth frame rates.",
  month         =  "30~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.GR",
  eprint        = "2503.23410"
}

@ARTICLE{Yuxiang2025-fd,
  title         = "{FreeInv}: Free Lunch for Improving {DDIM} Inversion",
  author        = "Yuxiang, Bao and Huijie, Liu and Xun, Gao and Huan, Fu and
                   Guoliang, Kang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Naive DDIM inversion process usually suffers from a
                   trajectory deviation issue, i.e., the latent trajectory
                   during reconstruction deviates from the one during inversion.
                   To alleviate this issue, previous methods either learn to
                   mitigate the deviation or design cumbersome compensation
                   strategy to reduce the mismatch error, exhibiting substantial
                   time and computation cost. In this work, we present a nearly
                   free-lunch method (named FreeInv) to address the issue more
                   effectively and efficiently. In FreeInv, we randomly
                   transform the latent representation and keep the
                   transformation the same between the corresponding inversion
                   and reconstruction time-step. It is motivated from a
                   statistical perspective that an ensemble of DDIM inversion
                   processes for multiple trajectories yields a smaller
                   trajectory mismatch error on expectation. Moreover, through
                   theoretical analysis and empirical study, we show that
                   FreeInv performs an efficient ensemble of multiple
                   trajectories. FreeInv can be freely integrated into existing
                   inversion-based image and video editing techniques.
                   Especially for inverting video sequences, it brings more
                   significant fidelity and efficiency improvements.
                   Comprehensive quantitative and qualitative evaluation on PIE
                   benchmark and DAVIS dataset shows that FreeInv remarkably
                   outperforms conventional DDIM inversion, and is competitive
                   among previous state-of-the-art inversion methods, with
                   superior computation efficiency.",
  month         =  "29~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.23035"
}

@ARTICLE{Feng2025-qv,
  title         = "Video-{R1}: Reinforcing Video Reasoning in {MLLMs}",
  author        = "Feng, Kaituo and Gong, Kaixiong and Li, Bohao and Guo,
                   Zonghao and Wang, Yibing and Peng, Tianshuo and Wang, Benyou
                   and Yue, Xiangyu",
  journal       = "arXiv [cs.CV]",
  abstract      = "Inspired by DeepSeek-R1's success in eliciting reasoning
                   abilities through rule-based reinforcement learning (RL), we
                   introduce Video-R1 as the first attempt to systematically
                   explore the R1 paradigm for eliciting video reasoning within
                   multimodal large language models (MLLMs). However, directly
                   applying RL training with the GRPO algorithm to video
                   reasoning presents two primary challenges: (i) a lack of
                   temporal modeling for video reasoning, and (ii) the scarcity
                   of high-quality video-reasoning data. To address these
                   issues, we first propose the T-GRPO algorithm, which
                   encourages models to utilize temporal information in videos
                   for reasoning. Additionally, instead of relying solely on
                   video data, we incorporate high-quality image-reasoning data
                   into the training process. We have constructed two datasets:
                   Video-R1-COT-165k for SFT cold start and Video-R1-260k for RL
                   training, both comprising image and video data. Experimental
                   results demonstrate that Video-R1 achieves significant
                   improvements on video reasoning benchmarks such as VideoMMMU
                   and VSI-Bench, as well as on general video benchmarks
                   including MVBench and TempCompass, etc. Notably, Video-R1-7B
                   attains a 35.8\% accuracy on video spatial reasoning
                   benchmark VSI-bench, surpassing the commercial proprietary
                   model GPT-4o. All codes, models, data are released.",
  month         =  "27~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.21776"
}

@ARTICLE{Jaywon2025-mw,
  title         = "Evaluating text-to-image synthesis with a Conditional Fréchet
                   Distance",
  author        = "Jaywon, Koo and Jefferson, Hernandez and Moayed, Haji-Ali and
                   Ziyan, Yang and Vicente, Ordonez",
  journal       = "arXiv [cs.CV]",
  abstract      = "Evaluating text-to-image synthesis is challenging due to
                   misalignment between established metrics and human
                   preferences. We propose cFreD, a metric based on the notion
                   of Conditional Fréchet Distance that explicitly accounts for
                   both visual fidelity and text-prompt alignment. Existing
                   metrics such as Inception Score (IS), Fréchet Inception
                   Distance (FID) and CLIPScore assess either image quality or
                   image-text alignment but not both which limits their
                   correlation with human preferences. Scoring models explicitly
                   trained to replicate human preferences require constant
                   updates and may not generalize to novel generation techniques
                   or out-of-domain inputs. Through extensive experiments across
                   multiple recently proposed text-to-image models and diverse
                   prompt datasets, we demonstrate that cFreD exhibits a higher
                   correlation with human judgments compared to statistical
                   metrics, including metrics trained with human preferences.
                   Our findings validate cFreD as a robust, future-proof metric
                   for the systematic evaluation of text-to-image models,
                   standardizing benchmarking in this rapidly evolving field. We
                   release our evaluation toolkit and benchmark in the appendix.",
  month         =  "27~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.21721"
}

@ARTICLE{Chihaoui2025-at,
  title         = "Diffusion Image Prior",
  author        = "Chihaoui, Hamadi and Favaro, Paolo",
  journal       = "arXiv [cs.CV]",
  abstract      = "Zero-shot image restoration (IR) methods based on pretrained
                   diffusion models have recently achieved significant success.
                   These methods typically require at least a parametric form of
                   the degradation model. However, in real-world scenarios, the
                   degradation may be too complex to define explicitly. To
                   handle this general case, we introduce the Diffusion Image
                   Prior (DIIP). We take inspiration from the Deep Image Prior
                   (DIP)[16], since it can be used to remove artifacts without
                   the need for an explicit degradation model. However, in
                   contrast to DIP, we find that pretrained diffusion models
                   offer a much stronger prior, despite being trained without
                   knowledge from corrupted data. We show that, the optimization
                   process in DIIP first reconstructs a clean version of the
                   image before eventually overfitting to the degraded input,
                   but it does so for a broader range of degradations than DIP.
                   In light of this result, we propose a blind image restoration
                   (IR) method based on early stopping, which does not require
                   prior knowledge of the degradation model. We validate DIIP on
                   various degradation-blind IR tasks, including JPEG artifact
                   removal, waterdrop removal, denoising and super-resolution
                   with state-of-the-art results.",
  month         =  "27~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.21410"
}

@ARTICLE{Costantino2025-aw,
  title     = "Partial information transfer from peripheral visual streams to
               foveal visual streams may be mediated through local primary
               visual circuits",
  author    = "Costantino, Andrea I and Turner, Benjamin O and Williams, Mark A
               and Crossley, Matthew J",
  journal   = "NeuroImage",
  publisher = "Academic Press",
  pages     =  121147,
  abstract  = "Visual object recognition is driven through the what pathway, a
               hierarchy of visual areas processing features of increasing
               complexity and abstractnes…",
  month     =  "26~" # mar,
  year      =  2025,
  keywords  = "\_To\_read;Project/Metamerism",
  doi       = "10.1016/j.neuroimage.2025.121147",
  issn      = "1053-8119,1095-9572"
}

@ARTICLE{Ilias2025-gf,
  title         = "Mind the gap: Benchmarking spatial reasoning in
                   Vision-Language Models",
  author        = "Ilias, Stogiannidis and Steven, Mcdonagh and Sotirios, A
                   Tsaftaris",
  journal       = "arXiv [cs.CV]",
  abstract      = "Vision-Language Models (VLMs) have recently emerged as
                   powerful tools, excelling in tasks that integrate visual and
                   textual comprehension, such as image captioning, visual
                   question answering, and image-text retrieval. However,
                   existing benchmarks for VLMs include spatial components,
                   which often fail to isolate spatial reasoning from related
                   tasks such as object detection or semantic comprehension. In
                   this paper, we address these deficiencies with a
                   multi-faceted approach towards understanding spatial
                   reasoning. Informed by the diverse and multi-dimensional
                   nature of human spatial reasoning abilities, we present a
                   detailed analysis that first delineates the core elements of
                   spatial reasoning: spatial relations, orientation and
                   navigation, mental rotation, and spatial visualization, and
                   then assesses the performance of these models in both
                   synthetic and real-world images, bridging controlled and
                   naturalistic contexts. We analyze 13 state-of-the-art
                   Vision-Language Models, uncovering pivotal insights into
                   their spatial reasoning performance. Our results reveal
                   profound shortcomings in current VLMs, with average accuracy
                   across the 13 models approximating random chance,
                   highlighting spatial reasoning as a persistent obstacle. This
                   work not only exposes the pressing need to advance spatial
                   reasoning within VLMs but also establishes a solid platform
                   for future exploration. Code available on GitHub
                   (https://github.com/stogiannidis/srbench) and dataset
                   available on HuggingFace
                   (https://huggingface.co/datasets/stogiannidis/srbench).",
  month         =  "25~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.19707"
}

@ARTICLE{Shijie2025-tp,
  title         = "{GenHancer}: Imperfect generative models are secretly strong
                   vision-centric enhancers",
  author        = "Shijie, Ma and Yuying, Ge and Teng, Wang and Yuxin, Guo and
                   Yixiao, Ge and Ying, Shan",
  journal       = "arXiv [cs.CV]",
  abstract      = "The synergy between generative and discriminative models
                   receives growing attention. While discriminative Contrastive
                   Language-Image Pre-Training (CLIP) excels in high-level
                   semantics, it struggles with perceiving fine-grained visual
                   details. Generally, to enhance representations, generative
                   models take CLIP's visual features as conditions for
                   reconstruction. However, the underlying principle remains
                   underexplored. In this work, we empirically found that
                   visually perfect generations are not always optimal for
                   representation enhancement. The essence lies in effectively
                   extracting fine-grained knowledge from generative models
                   while mitigating irrelevant information. To explore critical
                   factors, we delve into three aspects: (1) Conditioning
                   mechanisms: We found that even a small number of local tokens
                   can drastically reduce the difficulty of reconstruction,
                   leading to collapsed training. We thus conclude that
                   utilizing only global visual tokens as conditions is the most
                   effective strategy. (2) Denoising configurations: We observed
                   that end-to-end training introduces extraneous information.
                   To address this, we propose a two-stage training strategy to
                   prioritize learning useful visual knowledge. Additionally, we
                   demonstrate that lightweight denoisers can yield remarkable
                   improvements. (3) Generation paradigms: We explore both
                   continuous and discrete denoisers with desirable outcomes,
                   validating the versatility of our method. Through our
                   in-depth explorations, we have finally arrived at an
                   effective method, namely GenHancer, which consistently
                   outperforms prior arts on the MMVP-VLM benchmark, e.g., 6.0\%
                   on OpenAICLIP. The enhanced CLIP can be further plugged into
                   multimodal large language models for better vision-centric
                   performance. All the models and codes are made publicly
                   available.",
  month         =  "25~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.19480"
}

@ARTICLE{Kerssies2025-fk,
  title         = "Your {ViT} is Secretly an Image Segmentation Model",
  author        = "Kerssies, Tommie and Cavagnero, Niccolò and Hermans,
                   Alexander and Norouzi, Narges and Averta, Giuseppe and Leibe,
                   Bastian and Dubbelman, Gijs and Daan, de Geus",
  journal       = "arXiv [cs.CV]",
  abstract      = "Vision Transformers (ViTs) have shown remarkable performance
                   and scalability across various computer vision tasks. To
                   apply single-scale ViTs to image segmentation, existing
                   methods adopt a convolutional adapter to generate multi-scale
                   features, a pixel decoder to fuse these features, and a
                   Transformer decoder that uses the fused features to make
                   predictions. In this paper, we show that the inductive biases
                   introduced by these task-specific components can instead be
                   learned by the ViT itself, given sufficiently large models
                   and extensive pre-training. Based on these findings, we
                   introduce the Encoder-only Mask Transformer (EoMT), which
                   repurposes the plain ViT architecture to conduct image
                   segmentation. With large-scale models and pre-training, EoMT
                   obtains a segmentation accuracy similar to state-of-the-art
                   models that use task-specific components. At the same time,
                   EoMT is significantly faster than these methods due to its
                   architectural simplicity, e.g., up to 4x faster with ViT-L.
                   Across a range of model sizes, EoMT demonstrates an optimal
                   balance between segmentation accuracy and prediction speed,
                   suggesting that compute resources are better spent on scaling
                   the ViT itself rather than adding architectural complexity.
                   Code: https://www.tue-mps.org/eomt/.",
  month         =  "24~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.19108"
}

@ARTICLE{Li2025-pj,
  title         = "Vision: looking and seeing through our brain's information
                   bottleneck",
  author        = "Li, Zhaoping",
  journal       = "arXiv [q-bio.NC]",
  abstract      = "Our brain recognizes only a tiny fraction of sensory input,
                   due to an information processing bottleneck. This blinds us
                   to most visual inputs. Since we are blind to this blindness,
                   only a recent framework highlights this bottleneck by
                   formulating vision as mainly looking and seeing. Looking
                   selects a tiny fraction of visual information for progression
                   through the bottleneck, mainly by shifting gaze to center an
                   attentional spotlight. Seeing decodes, i.e., recognizes,
                   objects within the selected information. Since looking often
                   occurs before seeing and evokes limited awareness, humans
                   have the impression of seeing whole scenes clearly. According
                   to the new framework, the bottleneck starts from the output
                   of the primary visual cortex (V1) to downstream brain areas.
                   This is motivated by the evidence-backed V1 Saliency
                   Hypothesis (V1SH) that V1 creates a saliency map of the
                   visual field to guide looking. Massive visual information
                   loss downstream from V1 makes seeing vulnerable to ambiguity
                   and illusions (errors). To overcome this, feedback from
                   downstream to upstream areas such as V1 queries for
                   additional relevant information. An integral part of this
                   framework is the central-peripheral dichotomy (CPD) theory
                   proposing that vision in the peripheral and central visual
                   fields are specialized for looking (deciding where to shift
                   the gaze) and seeing, respectively, and that the feedback
                   query to aid seeing is mainly directed to the central visual
                   field. This V1SH-Bottleneck-CPD framework predicts that the
                   peripheral visual field, lacking feedback queries, is more
                   vulnerable to illusions, and that such illusions become
                   visible in the central visual field when the feedback query
                   is compromised. We present theoretical predictions,
                   experimental confirmations, a
                   Feedforward-Feedback-Verify-and-reWeight (FFVW) algorithm for
                   seeing through the bottleneck.",
  month         =  "24~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "2503.18804"
}

@ARTICLE{Miguel2025-zy,
  title         = "Bayesian generative models can flag performance loss, bias,
                   and out-of-distribution image content",
  author        = "Miguel, López-Pérez and Marco, Miani and Valery, Naranjo and
                   Søren, Hauberg and Aasa, Feragen",
  journal       = "arXiv [cs.LG]",
  abstract      = "Generative models are popular for medical imaging tasks such
                   as anomaly detection, feature extraction, data visualization,
                   or image generation. Since they are parameterized by deep
                   learning models, they are often sensitive to distribution
                   shifts and unreliable when applied to out-of-distribution
                   data, creating a risk of, e.g. underrepresentation bias. This
                   behavior can be flagged using uncertainty quantification
                   methods for generative models, but their availability remains
                   limited. We propose SLUG: A new UQ method for VAEs that
                   combines recent advances in Laplace approximations with
                   stochastic trace estimators to scale gracefully with image
                   dimensionality. We show that our UQ score -- unlike the VAE's
                   encoder variances -- correlates strongly with reconstruction
                   error and racial underrepresentation bias for dermatological
                   images. We also show how pixel-wise uncertainty can detect
                   out-of-distribution image content such as ink, rulers, and
                   patches, which is known to induce learning shortcuts in
                   predictive models.",
  month         =  "21~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2503.17477"
}

@ARTICLE{Yunhong2025-uo,
  title         = "{InPO}: Inversion preference optimization with reparametrized
                   {DDIM} for efficient diffusion model alignment",
  author        = "Yunhong, Lu and Qichao, Wang and Hengyuan, Cao and Xierui,
                   Wang and Xiaoyin, Xu and Min, Zhang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Without using explicit reward, direct preference optimization
                   (DPO) employs paired human preference data to fine-tune
                   generative models, a method that has garnered considerable
                   attention in large language models (LLMs). However,
                   exploration of aligning text-to-image (T2I) diffusion models
                   with human preferences remains limited. In comparison to
                   supervised fine-tuning, existing methods that align diffusion
                   model suffer from low training efficiency and subpar
                   generation quality due to the long Markov chain process and
                   the intractability of the reverse process. To address these
                   limitations, we introduce DDIM-InPO, an efficient method for
                   direct preference alignment of diffusion models. Our approach
                   conceptualizes diffusion model as a single-step generative
                   model, allowing us to fine-tune the outputs of specific
                   latent variables selectively. In order to accomplish this
                   objective, we first assign implicit rewards to any latent
                   variable directly via a reparameterization technique. Then we
                   construct an Inversion technique to estimate appropriate
                   latent variables for preference optimization. This
                   modification process enables the diffusion model to only
                   fine-tune the outputs of latent variables that have a strong
                   correlation with the preference dataset. Experimental results
                   indicate that our DDIM-InPO achieves state-of-the-art
                   performance with just 400 steps of fine-tuning, surpassing
                   all preference aligning baselines for T2I diffusion models in
                   human preference evaluation tasks.",
  month         =  "24~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.18454"
}

@ARTICLE{Qiao2025-cr,
  title         = "Expanding the boundaries of vision prior knowledge in
                   Multi-modal Large Language Models",
  author        = "Qiao, Liang and Yanjiang, Liu and Ben, He and Yaojie, Lu and
                   Hongyu, Lin and Jia, Zheng and Xianpei, Han and Le, Sun and
                   Yingfei, Sun",
  journal       = "arXiv [cs.CV]",
  abstract      = "Does the prior knowledge of the vision encoder constrain the
                   capability boundary of Multi-modal Large Language Models
                   (MLLMs)? While most existing research treats MLLMs as unified
                   systems optimized through end-to-end training, the impact of
                   vision encoder's prior knowledge is seldom investigated. In
                   this work, we introduce a novel metric, $Rank_e$, to quantify
                   the effect of the vision encoder's prior knowledge on MLLM
                   performance. Our analysis reveals a positive correlation
                   between prior knowledge and MLLM performance. Moreover, we
                   find that domain-specific fine-tuning using solely end-to-end
                   visual question answering (VQA) data is
                   insufficient--particularly for entities with low inherent
                   visual prior knowledge. To address this issue, we propose
                   VisPRE (Vision Prior Remediation), a two-stage training
                   framework that explicitly incorporates prior knowledge at the
                   vision encoder level. Experimental results demonstrate that
                   augmenting vision encoder's prior knowledge substantially
                   boosts the visual understanding capabilities of MLLMs,
                   offering a novel and effective strategy for improving
                   performance, especially in scenarios involving uncommon
                   visual entities.",
  month         =  "23~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.18034"
}

@ARTICLE{Yufei2025-de,
  title         = "Vision-{R1}: Evolving human-free alignment in large
                   vision-Language Models via vision-guided reinforcement
                   learning",
  author        = "Yufei, Zhan and Yousong, Zhu and Shurong, Zheng and Hongyin,
                   Zhao and Fan, Yang and Ming, Tang and Jinqiao, Wang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Large Vision-Language Models (LVLMs) typically follow a
                   two-stage training paradigm-pretraining and supervised
                   fine-tuning. Recently, preference optimization, derived from
                   the language domain, has emerged as an effective
                   post-training reinforcement strategy to enhance capabilities
                   of LVLMs. However, constructing high-quality human-annotated
                   preference data and developing robust reward models to mimic
                   these preferences are both costly and challenging. Motivated
                   by this observation, we propose Vision-R1, a novel
                   vision-guided R1-like reinforcement learning algorithm for
                   LVLMs that rewards models with definitive vision feedback. It
                   only leverages curated instruction data, eliminating the need
                   for specialized reward models and handcrafted preference
                   datasets. We incorporate a criterion-driven reward function
                   that further integrates multi-dimensional feedback to
                   evaluate model completions comprehensively based on the
                   vision task logic. Furthermore, we introduce a progressive
                   rule refinement strategy that dynamically adjusts the reward
                   criteria during training, enabling continuous model
                   improvement and mitigating reward hacking. Extensive
                   experiments on both in-distribution and out-of-distribution
                   benchmarks demonstrate that fine-tuning the 7B LVLMs with
                   Vision-R1 achieves consistent performance gains, with even up
                   to 50\% improvement and surpassing the state-of-the-art 10x
                   size model.",
  month         =  "23~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.18013"
}

@ARTICLE{Takahiro2025-un,
  title         = "Guided diffusion for the extension of machine vision to human
                   visual perception",
  author        = "Takahiro, Shindo and Yui, Tatsumi and Taiju, Watanabe and
                   Hiroshi, Watanabe",
  journal       = "arXiv [cs.CV]",
  abstract      = "Image compression technology eliminates redundant information
                   to enable efficient transmission and storage of images,
                   serving both machine vision and human visual perception. For
                   years, image coding focused on human perception has been
                   well-studied, leading to the development of various image
                   compression standards. On the other hand, with the rapid
                   advancements in image recognition models, image compression
                   for AI tasks, known as Image Coding for Machines (ICM), has
                   gained significant importance. Therefore, scalable image
                   coding techniques that address the needs of both machines and
                   humans have become a key area of interest. Additionally,
                   there is increasing demand for research applying the
                   diffusion model, which can generate human-viewable images
                   from a small amount of data to image compression methods for
                   human vision. Image compression methods that use diffusion
                   models can partially reconstruct the target image by guiding
                   the generation process with a small amount of conditioning
                   information. Inspired by the diffusion model's potential, we
                   propose a method for extending machine vision to human visual
                   perception using guided diffusion. Utilizing the diffusion
                   model guided by the output of the ICM method, we generate
                   images for human perception from random noise. Guided
                   diffusion acts as a bridge between machine vision and human
                   vision, enabling transitions between them without any
                   additional bitrate overhead. The generated images then
                   evaluated based on bitrate and image quality, and we compare
                   their compression performance with other scalable image
                   coding methods for humans and machines.",
  month         =  "23~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.17907"
}

@ARTICLE{Tianwen2025-ia,
  title         = "{ProDehaze}: Prompting diffusion models toward faithful image
                   dehazing",
  author        = "Tianwen, Zhou and Jing, Wang and Songtao, Wu and Kuanhong, Xu",
  journal       = "arXiv [cs.CV]",
  abstract      = "Recent approaches using large-scale pretrained diffusion
                   models for image dehazing improve perceptual quality but
                   often suffer from hallucination issues, producing unfaithful
                   dehazed image to the original one. To mitigate this, we
                   propose ProDehaze, a framework that employs internal image
                   priors to direct external priors encoded in pretrained
                   models. We introduce two types of \textit{selective} internal
                   priors that prompt the model to concentrate on critical image
                   areas: a Structure-Prompted Restorer in the latent space that
                   emphasizes structure-rich regions, and a Haze-Aware
                   Self-Correcting Refiner in the decoding process to align
                   distributions between clearer input regions and the output.
                   Extensive experiments on real-world datasets demonstrate that
                   ProDehaze achieves high-fidelity results in image dehazing,
                   particularly in reducing color shifts. Our code is at
                   https://github.com/TianwenZhou/ProDehaze.",
  month         =  "21~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.17488"
}

@ARTICLE{Bowers2025-js,
  title    = "Chromatic and Achromatic Contrast Sensitivity in the far Periphery",
  author   = "Bowers, Norick R and Gegenfurtner, Karl and Goettker, Alexander",
  journal  = "bioRxiv",
  pages    = "2025.03.22.644503",
  abstract = "The contrast sensitivity function (CSF) has been studied
              extensively since the 1960s, however most studies to date have
              focused on the central region of the visual field. The current
              study aims to address two gaps in previous measurements: First, it
              provides a detailed measurement of the CSF for achromatic and,
              importantly, also chromatic stimuli in the far periphery of up to
              90 degrees of visual angle. Second, we describe visual sensitivity
              around the monocular/binocular boundary that is naturally present
              in the far periphery. In the first experiment, the CSF was
              measured in 3 different conditions: Stimuli were either Achromatic
              (L+M), Red-Green (L-M) or and Yellow-Violet (S-(L+M)) gabor
              patches. Overall, results followed the expected patterns
              established in the near periphery, but achromatic sensitivity in
              the far periphery was mostly underestimated by current models of
              visual perception, the quick decay in sensitivity observed for
              red-green stimuli slows down in the periphery. The decay of
              sensitivity for yellow-violett stimuli roughly matches the decay
              for achromatic stimuli even up to the far periphery. For the
              second experiment, we compared binocular and monocular visual
              sensitivity at different locations in the visual field. We
              observed a consistent increase of visual sensitivity for binocular
              viewing in the central part of the visual field compared to
              monocular viewing, but this benefit already decreased within the
              binocular visual field in the periphery. Together, these data
              provide a detailed description of visual sensitivity in the far
              periphery. These measurements can help to improve current models
              of visual sensitivity and can be vital for applications in
              full-field visual displays in virtual and augmented reality.
              \#\#\# Competing Interest Statement This study was partially
              funded by Meta Platforms, Inc. As this is not evaluating or
              promoting any products or services, we do not have any competing
              or financial interests to report.",
  month    =  "24~" # mar,
  year     =  2025,
  doi      = "10.1101/2025.03.22.644503",
  language = "en"
}

@ARTICLE{Jerred2025-dt,
  title         = "Image as an {IMU}: Estimating camera motion from a single
                   motion-blurred image",
  author        = "Jerred, Chen and Ronald, Clark",
  journal       = "arXiv [cs.CV]",
  abstract      = "In many robotics and VR/AR applications, fast camera motions
                   cause a high level of motion blur, causing existing camera
                   pose estimation methods to fail. In this work, we propose a
                   novel framework that leverages motion blur as a rich cue for
                   motion estimation rather than treating it as an unwanted
                   artifact. Our approach works by predicting a dense motion
                   flow field and a monocular depth map directly from a single
                   motion-blurred image. We then recover the instantaneous
                   camera velocity by solving a linear least squares problem
                   under the small motion assumption. In essence, our method
                   produces an IMU-like measurement that robustly captures fast
                   and aggressive camera movements. To train our model, we
                   construct a large-scale dataset with realistic synthetic
                   motion blur derived from ScanNet++v2 and further refine our
                   model by training end-to-end on real data using our fully
                   differentiable pipeline. Extensive evaluations on real-world
                   benchmarks demonstrate that our method achieves
                   state-of-the-art angular and translational velocity
                   estimates, outperforming current methods like MASt3R and
                   COLMAP.",
  month         =  "21~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.17358"
}

@ARTICLE{Jichen2025-jc,
  title         = "Dereflection Any Image with diffusion priors and diversified
                   data",
  author        = "Jichen, Hu and Chen, Yang and Zanwei, Zhou and Jiemin, Fang
                   and Xiaokang, Yang and Qi, Tian and Wei, Shen",
  journal       = "arXiv [cs.CV]",
  abstract      = "Reflection removal of a single image remains a highly
                   challenging task due to the complex entanglement between
                   target scenes and unwanted reflections. Despite significant
                   progress, existing methods are hindered by the scarcity of
                   high-quality, diverse data and insufficient restoration
                   priors, resulting in limited generalization across various
                   real-world scenarios. In this paper, we propose Dereflection
                   Any Image, a comprehensive solution with an efficient data
                   preparation pipeline and a generalizable model for robust
                   reflection removal. First, we introduce a dataset named
                   Diverse Reflection Removal (DRR) created by randomly rotating
                   reflective mediums in target scenes, enabling variation of
                   reflection angles and intensities, and setting a new
                   benchmark in scale, quality, and diversity. Second, we
                   propose a diffusion-based framework with one-step diffusion
                   for deterministic outputs and fast inference. To ensure
                   stable learning, we design a three-stage progressive training
                   strategy, including reflection-invariant finetuning to
                   encourage consistent outputs across varying reflection
                   patterns that characterize our dataset. Extensive experiments
                   show that our method achieves SOTA performance on both common
                   benchmarks and challenging in-the-wild images, showing
                   superior generalization across diverse real-world scenes.",
  month         =  "21~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.17347"
}

@ARTICLE{Xiaofeng2025-rz,
  title         = "{EasyRobust}: A comprehensive and easy-to-use toolkit for
                   robust and generalized vision",
  author        = "Xiaofeng, Mao and Yuefeng, Chen and Rong, Zhang and Hui, Xue
                   and Zhao, Li and Hang, Su",
  journal       = "arXiv [cs.CV]",
  abstract      = "Deep neural networks (DNNs) has shown great promise in
                   computer vision tasks. However, machine vision achieved by
                   DNNs cannot be as robust as human perception. Adversarial
                   attacks and data distribution shifts have been known as two
                   major scenarios which degrade machine performance and
                   obstacle the wide deployment of machines ``in the wild''. In
                   order to break these obstructions and facilitate the research
                   of model robustness, we develop EasyRobust, a comprehensive
                   and easy-to-use toolkit for training, evaluation and analysis
                   of robust vision models. EasyRobust targets at two types of
                   robustness: 1) Adversarial robustness enables the model to
                   defense against malicious inputs crafted by worst-case
                   perturbations, also known as adversarial examples; 2)
                   Non-adversarial robustness enhances the model performance on
                   natural test images with corruptions or distribution shifts.
                   Thorough benchmarks on image classification enable EasyRobust
                   to provide an accurate robustness evaluation on vision
                   models. We wish our EasyRobust can help for training
                   practically-robust models and promote academic and industrial
                   progress in closing the gap between human and machine vision.
                   Codes and models of EasyRobust have been open-sourced in
                   https://github.com/alibaba/easyrobust.",
  month         =  "21~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.16975"
}

@ARTICLE{Yancheng2025-ym,
  title         = "{ElaTCSF}: A Temporal Contrast Sensitivity Function for
                   flicker detection and modeling variable refresh rate flicker",
  author        = "Yancheng, Cai and Ali, Bozorgian and Maliha, Ashraf and
                   Robert, Wanat and Rafał, K Mantiuk",
  journal       = "arXiv [cs.GR]",
  abstract      = "The perception of flicker has been a prominent concern in
                   illumination and electronic display fields for over a
                   century. Traditional approaches often rely on Critical
                   Flicker Frequency (CFF), primarily suited for high-contrast
                   (full-on, full-off) flicker. To tackle varying contrast
                   flicker, the International Committee for Display Metrology
                   (ICDM) introduced a Temporal Contrast Sensitivity Function
                   TCSF$_{IDMS}$ within the Information Display Measurements
                   Standard (IDMS). Nevertheless, this standard overlooks
                   crucial parameters: luminance, eccentricity, and area.
                   Existing models incorporating these parameters are inadequate
                   for flicker detection, especially at low spatial frequencies.
                   To address these limitations, we extend the TCSF$_{IDMS}$ and
                   combine it with a new spatial probability summation model to
                   incorporate the effects of luminance, eccentricity, and area
                   (elaTCSF). We train the elaTCSF on various flicker detection
                   datasets and establish the first variable refresh rate
                   flicker detection dataset for further verification.
                   Additionally, we contribute to resolving a longstanding
                   debate on whether the flicker is more visible in peripheral
                   vision. We demonstrate how elaTCSF can be used to predict
                   flicker due to low-persistence in VR headsets, identify
                   flicker-free VRR operational ranges, and determine flicker
                   sensitivity in lighting design.",
  month         =  "21~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.GR",
  eprint        = "2503.16759",
  doi           = "10.1145/3680528.3687586"
}

@ARTICLE{Robin2025-ki,
  title         = "Beyond accuracy: What matters in designing well-behaved
                   models?",
  author        = "Robin, Hesse and Doğukan, Bağcı and Bernt, Schiele and
                   Simone, Schaub-Meyer and Stefan, Roth",
  journal       = "arXiv [cs.CV]",
  abstract      = "Deep learning has become an essential part of computer
                   vision, with deep neural networks (DNNs) excelling in
                   predictive performance. However, they often fall short in
                   other critical quality dimensions, such as robustness,
                   calibration, or fairness. While existing studies have focused
                   on a subset of these quality dimensions, none have explored a
                   more general form of ``well-behavedness'' of DNNs. With this
                   work, we address this gap by simultaneously studying nine
                   different quality dimensions for image classification.
                   Through a large-scale study, we provide a bird's-eye view by
                   analyzing 326 backbone models and how different training
                   paradigms and model architectures affect the quality
                   dimensions. We reveal various new insights such that (i)
                   vision-language models exhibit high fairness on ImageNet-1k
                   classification and strong robustness against domain changes;
                   (ii) self-supervised learning is an effective training
                   paradigm to improve almost all considered quality dimensions;
                   and (iii) the training dataset size is a major driver for
                   most of the quality dimensions. We conclude our study by
                   introducing the QUBA score (Quality Understanding Beyond
                   Accuracy), a novel metric that ranks models across multiple
                   dimensions of quality, enabling tailored recommendations
                   based on specific user needs.",
  month         =  "21~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.17110"
}

@ARTICLE{Kroell2022-zp,
  title     = "Foveal vision anticipates defining features of eye movement
               targets",
  author    = "Kroell, Lisa M and Rolfs, Martin",
  journal   = "eLife",
  publisher = "eLife Sciences Publications, Ltd",
  volume    =  11,
  abstract  = "High-acuity foveal processing is vital for human vision.
               Nonetheless, little is known about how the preparation of
               large-scale rapid eye movements (saccades) affects visual
               sensitivity in the center of gaze. Based on findings from passive
               fixation tasks, we hypothesized that during saccade preparation,
               foveal processing anticipates soon-to-be fixated visual features.
               Using a dynamic large-field noise paradigm, we indeed demonstrate
               that defining features of an eye movement target are enhanced in
               the pre-saccadic center of gaze. Enhancement manifested as higher
               Hit Rates for foveal probes with target-congruent orientation and
               a sensitization to incidental, target-like orientation
               information in foveally presented noise. Enhancement was
               spatially confined to the center of gaze and its immediate
               vicinity, even after parafoveal task performance had been raised
               to a foveal level. Moreover, foveal enhancement during saccade
               preparation was more pronounced and developed faster than
               enhancement during passive fixation. Based on these findings, we
               suggest a crucial contribution of foveal processing to
               trans-saccadic visual continuity: Foveal processing of saccade
               targets commences before the movement is executed and thereby
               enables a seamless transition once the center of gaze reaches the
               target.",
  month     =  "9~" # sep,
  year      =  2022,
  keywords  = "active vision; foveal processing; human; neuroscience; saccadic
               eye movements; visual continuity;\_To\_read;Project/Metamerism",
  doi       = "10.7554/eLife.78106",
  pmc       = "PMC9581528",
  pmid      =  36082940,
  issn      = "2050-084X",
  language  = "en"
}

@ARTICLE{D-Agostino2025-lm,
  title    = "Openretina: Collaborative retina modelling across datasets and
              species",
  author   = "D'Agostino, Federico and Zenkel, Thomas and Gonschorek, Dominic
              and Höfling, Larissa and Euler, Thomas and Bethge, Matthias",
  journal  = "bioRxiv",
  pages    = "2025.03.07.642012",
  abstract = "AbstractStudying the retina plays a crucial role in understanding
              how the visual world is translated into the brain’s language. As a
              stand-alone neural circuit with easily controllable input, the
              retina provides a unique opportunity to develop a complete and
              quantitatively precise model of a computational module in the
              brain. However, decades of data and models remain fragmented
              across labs and approaches. To address this, we have launched an
              open-source retina modelling platform on a shared GitHub
              repository, aiming to provide a unified data and modelling
              framework across species, recording techniques, stimulus
              conditions, and use cases. Our initial release consists of a
              Python package,openretina, a modelling framework based on PyTorch,
              which we designed for optimal accessibility and extensibility. The
              package includes different variations on a basic “Core + Readout”
              model architecture, easily adaptable dataloaders, integration with
              modern deep learning libraries, and methods for
              performingin-silicoexperiments and analyses on the models. We
              illustrate the versatility of the package by providing dataloaders
              and pre-trained models for data from several laboratories and
              studies across species. With this starter pack in
              place,openretinacan be used within minutes. Through step-by-step
              examples, we here provide retina researchers of diverse
              backgrounds a hands-on introduction to modelling, including using
              models as tools for visualising retinal computations, generating
              and testing hypotheses, and guiding experimental design.",
  month    =  "11~" # mar,
  year     =  2025,
  keywords = "Project/Metamerism",
  doi      = "10.1101/2025.03.07.642012",
  language = "en"
}

@ARTICLE{Neehar2025-mv,
  title         = "Representational similarity via interpretable visual concepts",
  author        = "Neehar, Kondapaneni and Aodha, Oisin Mac and Pietro, Perona",
  journal       = "arXiv [cs.CV]",
  abstract      = "How do two deep neural networks differ in how they arrive at
                   a decision? Measuring the similarity of deep networks has
                   been a long-standing open question. Most existing methods
                   provide a single number to measure the similarity of two
                   networks at a given layer, but give no insight into what
                   makes them similar or dissimilar. We introduce an
                   interpretable representational similarity method (RSVC) to
                   compare two networks. We use RSVC to discover shared and
                   unique visual concepts between two models. We show that some
                   aspects of model differences can be attributed to unique
                   concepts discovered by one model that are not well
                   represented in the other. Finally, we conduct extensive
                   evaluation across different vision model architectures and
                   training protocols to demonstrate its effectiveness.",
  month         =  "19~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.15699"
}

@ARTICLE{Dounia2025-jx,
  title         = "Do image and video quality metrics model low-level human
                   vision?",
  author        = "Dounia, Hammou and Yancheng, Cai and Pavan, Madhusudanarao
                   and Christos, G Bampis and Rafał, K Mantiuk",
  journal       = "arXiv [eess.IV]",
  abstract      = "Image and video quality metrics, such as SSIM, LPIPS, and
                   VMAF, are aimed to predict the perceived quality of the
                   evaluated content and are often claimed to be ``perceptual''.
                   Yet, few metrics directly model human visual perception, and
                   most rely on hand-crafted formulas or training datasets to
                   achieve alignment with perceptual data. In this paper, we
                   propose a set of tests for full-reference quality metrics
                   that examine their ability to model several aspects of
                   low-level human vision: contrast sensitivity, contrast
                   masking, and contrast matching. The tests are meant to
                   provide additional scrutiny for newly proposed metrics. We
                   use our tests to analyze 33 existing image and video quality
                   metrics and find their strengths and weaknesses, such as the
                   ability of LPIPS and MS-SSIM to predict contrast masking and
                   poor performance of VMAF in this task. We further find that
                   the popular SSIM metric overemphasizes differences in high
                   spatial frequencies, but its multi-scale counterpart,
                   MS-SSIM, addresses this shortcoming. Such findings cannot be
                   easily made using existing evaluation protocols.",
  month         =  "20~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "eess.IV",
  eprint        = "2503.16264"
}

@ARTICLE{Tatsukawa2025-hk,
  title         = "{FontCraft}: Multimodal font design using interactive
                   Bayesian optimization",
  author        = "Tatsukawa, Yuki and Shen, I-Chao and Dogan, Mustafa Doga and
                   Qi, Anran and Koyama, Yuki and Shamir, Ariel and Igarashi,
                   Takeo",
  journal       = "arXiv [cs.HC]",
  abstract      = "Creating new fonts requires a lot of human effort and
                   professional typographic knowledge. Despite the rapid
                   advancements of automatic font generation models, existing
                   methods require users to prepare pre-designed characters with
                   target styles using font-editing software, which poses a
                   problem for non-expert users. To address this limitation, we
                   propose FontCraft, a system that enables font generation
                   without relying on pre-designed characters. Our approach
                   integrates the exploration of a font-style latent space with
                   human-in-the-loop preferential Bayesian optimization and
                   multimodal references, facilitating efficient exploration and
                   enhancing user control. Moreover, FontCraft allows users to
                   revisit previous designs, retracting their earlier choices in
                   the preferential Bayesian optimization process. Once users
                   finish editing the style of a selected character, they can
                   propagate it to the remaining characters and further refine
                   them as needed. The system then generates a complete outline
                   font in OpenType format. We evaluated the effectiveness of
                   FontCraft through a user study comparing it to a baseline
                   interface. Results from both quantitative and qualitative
                   evaluations demonstrate that FontCraft enables non-expert
                   users to design fonts efficiently.",
  month         =  "16~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2502.11399"
}

@ARTICLE{David2025-mk,
  title         = "Revisiting Image Fusion for Multi-Illuminant White-Balance
                   Correction",
  author        = "David, Serrano-Lozano and Aditya, Arora and Luis, Herranz and
                   Konstantinos, G Derpanis and Michael, S Brown and Javier,
                   Vazquez-Corral",
  journal       = "arXiv [cs.CV]",
  abstract      = "White balance (WB) correction in scenes with multiple
                   illuminants remains a persistent challenge in computer
                   vision. Recent methods explored fusion-based approaches,
                   where a neural network linearly blends multiple sRGB versions
                   of an input image, each processed with predefined WB presets.
                   However, we demonstrate that these methods are suboptimal for
                   common multi-illuminant scenarios. Additionally, existing
                   fusion-based methods rely on sRGB WB datasets lacking
                   dedicated multi-illuminant images, limiting both training and
                   evaluation. To address these challenges, we introduce two key
                   contributions. First, we propose an efficient
                   transformer-based model that effectively captures spatial
                   dependencies across sRGB WB presets, substantially improving
                   upon linear fusion techniques. Second, we introduce a
                   large-scale multi-illuminant dataset comprising over 16,000
                   sRGB images rendered with five different WB settings, along
                   with WB-corrected images. Our method achieves up to 100\%
                   improvement over existing techniques on our new
                   multi-illuminant image fusion dataset.",
  month         =  "18~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.14774"
}

@ARTICLE{Cho2024-qh,
  title         = "Zero-shot low light image enhancement with diffusion prior",
  author        = "Cho, Joshua and Aghajanzadeh, Sara and Zhu, Zhen and Forsyth,
                   D A",
  journal       = "arXiv [cs.CV]",
  abstract      = "Balancing aesthetic quality with fidelity when enhancing
                   images from challenging, degraded sources is a core objective
                   in computational photography. In this paper, we address low
                   light image enhancement (LLIE), a task in which dark images
                   often contain limited visible information. Diffusion models,
                   known for their powerful image enhancement capacities, are a
                   natural choice for this problem. However, their deep
                   generative priors can also lead to hallucinations,
                   introducing non-existent elements or substantially altering
                   the visual semantics of the original scene. In this work, we
                   introduce a novel zero-shot method for controlling and
                   refining the generative behavior of diffusion models for
                   dark-to-light image conversion tasks. Our method demonstrates
                   superior performance over existing state-of-the-art methods
                   in the task of low-light image enhancement, as evidenced by
                   both quantitative metrics and qualitative analysis.",
  month         =  "17~" # dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2412.13401"
}

@ARTICLE{Zeyi2025-kw,
  title         = "Do vision models develop human-like progressive difficulty
                   understanding?",
  author        = "Zeyi, Huang and Utkarsh, Ojha and Yuyang, Ji and Donghyun,
                   Lee and Lee, Yong Jae",
  journal       = "arXiv [cs.CV]",
  abstract      = "When a human undertakes a test, their responses likely follow
                   a pattern: if they answered an easy question $(2 \times 3)$
                   incorrectly, they would likely answer a more difficult one
                   $(2 \times 3 \times 4)$ incorrectly; and if they answered a
                   difficult question correctly, they would likely answer the
                   easy one correctly. Anything else hints at memorization. Do
                   current visual recognition models exhibit a similarly
                   structured learning capacity? In this work, we consider the
                   task of image classification and study if those models'
                   responses follow that pattern. Since real images aren't
                   labeled with difficulty, we first create a dataset of 100
                   categories, 10 attributes, and 3 difficulty levels using
                   recent generative models: for each category (e.g., dog) and
                   attribute (e.g., occlusion), we generate images of increasing
                   difficulty (e.g., a dog without occlusion, a dog only partly
                   visible). We find that most of the models do in fact behave
                   similarly to the aforementioned pattern around 80-90\% of the
                   time. Using this property, we then explore a new way to
                   evaluate those models. Instead of testing the model on every
                   possible test image, we create an adaptive test akin to GRE,
                   in which the model's performance on the current round of
                   images determines the test images in the next round. This
                   allows the model to skip over questions too easy/hard for
                   itself, and helps us get its overall performance in fewer
                   steps.",
  month         =  "17~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.13058"
}

@ARTICLE{Mo2025-pj,
  title         = "{ProbDiffFlow}: An efficient learning-free framework for
                   probabilistic single-image optical flow estimation",
  author        = "Mo, Zhou and Jianwei, Wang and Xuanmeng, Zhang and Dylan,
                   Campbell and Kai, Wang and Long, Yuan and Wenjie, Zhang and
                   Xuemin, Lin",
  journal       = "arXiv [cs.CV]",
  abstract      = "This paper studies optical flow estimation, a critical task
                   in motion analysis with applications in autonomous
                   navigation, action recognition, and film production.
                   Traditional optical flow methods require consecutive frames,
                   which are often unavailable due to limitations in data
                   acquisition or real-world scene disruptions. Thus,
                   single-frame optical flow estimation is emerging in the
                   literature. However, existing single-frame approaches suffer
                   from two major limitations: (1) they rely on labeled training
                   data, making them task-specific, and (2) they produce
                   deterministic predictions, failing to capture motion
                   uncertainty. To overcome these challenges, we propose
                   ProbDiffFlow, a training-free framework that estimates
                   optical flow distributions from a single image. Instead of
                   directly predicting motion, ProbDiffFlow follows an
                   estimation-by-synthesis paradigm: it first generates diverse
                   plausible future frames using a diffusion-based model, then
                   estimates motion from these synthesized samples using a
                   pre-trained optical flow model, and finally aggregates the
                   results into a probabilistic flow distribution. This design
                   eliminates the need for task-specific training while
                   capturing multiple plausible motions. Experiments on both
                   synthetic and real-world datasets demonstrate that
                   ProbDiffFlow achieves superior accuracy, diversity, and
                   efficiency, outperforming existing single-image and two-frame
                   baselines.",
  month         =  "16~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.12348"
}

@ARTICLE{Ruijie2025-bh,
  title         = "{TACO}: Taming diffusion for in-the-wild Video Amodal
                   Completion",
  author        = "Ruijie, Lu and Yixin, Chen and Yu, Liu and Jiaxiang, Tang and
                   Junfeng, Ni and Diwen, Wan and Gang, Zeng and Siyuan, Huang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Humans can infer complete shapes and appearances of objects
                   from limited visual cues, relying on extensive prior
                   knowledge of the physical world. However, completing
                   partially observable objects while ensuring consistency
                   across video frames remains challenging for existing models,
                   especially for unstructured, in-the-wild videos. This paper
                   tackles the task of Video Amodal Completion (VAC), which aims
                   to generate the complete object consistently throughout the
                   video given a visual prompt specifying the object of
                   interest. Leveraging the rich, consistent manifolds learned
                   by pre-trained video diffusion models, we propose a
                   conditional diffusion model, TACO, that repurposes these
                   manifolds for VAC. To enable its effective and robust
                   generalization to challenging in-the-wild scenarios, we
                   curate a large-scale synthetic dataset with multiple
                   difficulty levels by systematically imposing occlusions onto
                   un-occluded videos. Building on this, we devise a progressive
                   fine-tuning paradigm that starts with simpler recovery tasks
                   and gradually advances to more complex ones. We demonstrate
                   TACO's versatility on a wide range of in-the-wild videos from
                   Internet, as well as on diverse, unseen datasets commonly
                   used in autonomous driving, robotic manipulation, and scene
                   understanding. Moreover, we show that TACO can be effectively
                   applied to various downstream tasks like object
                   reconstruction and pose estimation, highlighting its
                   potential to facilitate physical world understanding and
                   reasoning. Our project page is available at
                   https://jason-aplp.github.io/TACO.",
  month         =  "15~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.12049"
}

@ARTICLE{Sinaei2025-hl,
  title         = "Artificial blur effect for optical see-through near-eye
                   displays",
  author        = "Sinaei, Shiva and Iwai, Daisuke and Sato, Kousuke",
  journal       = "arXiv [physics.optics]",
  abstract      = "Saliency modulation has significant potential for various
                   applications. In our pursuit of implementing saliency
                   modulation for optical see-through near-eye displays, we
                   decided to introduce a blur effect to reduce the sharpness of
                   specific areas while preserving the sharpness of others. In
                   this study, we used a digital micromirror device (DMD) to
                   separate the incoming light from a scene into sharp and
                   blurred areas. To achieve this, we integrated an electrically
                   tunable lens (ETL), which operates in its zero optical power
                   mode when the reflected light from the DMD represents the
                   sharp area (i.e., the blur area is masked). Conversely, when
                   the reflected light indicates the blur area, the ETL adjusts
                   to non-zero optical powers. Importantly, these modulations
                   occur at a speed that surpasses the critical flicker
                   frequency threshold of the human eye. Furthermore, we
                   proposed an algorithm to mitigate the artifacts around the
                   border area between the sharp and blur areas that are caused
                   by the magnification of the ETL. We have also developed a
                   prototype system to demonstrate the feasibility of our
                   method.",
  month         =  "17~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "physics.optics",
  eprint        = "2503.13003"
}

@ARTICLE{Yang2023-aq,
  title         = "Virtual Guidance as a mid-level representation for navigation",
  author        = "Yang, Hsuan-Kung and Chiang, Tsung-Chih and Liu, Ting-Ru and
                   Huang, Chun-Wei and Liu, Jou-Min and Lee, Chun-Yi",
  journal       = "arXiv [cs.LG]",
  abstract      = "In the context of autonomous navigation, effectively
                   conveying abstract navigational cues to agents in dynamic
                   environments poses challenges, particularly when the
                   navigation information is multimodal. To address this issue,
                   the paper introduces a novel technique termed ``Virtual
                   Guidance,'' which is designed to visually represent
                   non-visual instructional signals. These visual cues, rendered
                   as colored paths or spheres, are overlaid onto the agent's
                   camera view, serving as easily comprehensible navigational
                   instructions. We evaluate our proposed method through
                   experiments in both simulated and real-world settings. In the
                   simulated environments, our virtual guidance outperforms
                   baseline hybrid approaches in several metrics, including
                   adherence to planned routes and obstacle avoidance.
                   Furthermore, we extend the concept of virtual guidance to
                   transform text-prompt-based instructions into a visually
                   intuitive format for real-world experiments. Our results
                   validate the adaptability of virtual guidance and its
                   efficacy in enabling policy transfer from simulated scenarios
                   to real-world ones.",
  month         =  "5~" # mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2303.02731"
}

@ARTICLE{Hongyu2025-mw,
  title         = "Seeing and seeing through the glass: Real and synthetic data
                   for multi-layer depth estimation",
  author        = "Hongyu, Wen and Yiming, Zuo and Venkat, Subramanian and
                   Patrick, Chen and Jia, Deng",
  journal       = "arXiv [cs.CV]",
  abstract      = "Transparent objects are common in daily life, and
                   understanding their multi-layer depth information --
                   perceiving both the transparent surface and the objects
                   behind it -- is crucial for real-world applications that
                   interact with transparent materials. In this paper, we
                   introduce LayeredDepth, the first dataset with multi-layer
                   depth annotations, including a real-world benchmark and a
                   synthetic data generator, to support the task of multi-layer
                   depth estimation. Our real-world benchmark consists of 1,500
                   images from diverse scenes, and evaluating state-of-the-art
                   depth estimation methods on it reveals that they struggle
                   with transparent objects. The synthetic data generator is
                   fully procedural and capable of providing training data for
                   this task with an unlimited variety of objects and scene
                   compositions. Using this generator, we create a synthetic
                   dataset with 15,300 images. Baseline models training solely
                   on this synthetic dataset produce good cross-domain
                   multi-layer depth estimation. Fine-tuning state-of-the-art
                   single-layer depth models on it substantially improves their
                   performance on transparent objects, with quadruplet accuracy
                   on our benchmark increased from 55.14\% to 75.20\%. All
                   images and validation annotations are available under CC0 at
                   https://layereddepth.cs.princeton.edu.",
  month         =  "14~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.11633"
}

@ARTICLE{Du2025-xe,
  title         = "Toward generalized image quality assessment: Relaxing the
                   perfect reference quality assumption",
  author        = "Du, Chen and Tianhe, Wu and Kede, Ma and Lei, Zhang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Full-reference image quality assessment (FR-IQA) generally
                   assumes that reference images are of perfect quality.
                   However, this assumption is flawed due to the sensor and
                   optical limitations of modern imaging systems. Moreover,
                   recent generative enhancement methods are capable of
                   producing images of higher quality than their original. All
                   of these challenge the effectiveness and applicability of
                   current FR-IQA models. To relax the assumption of perfect
                   reference image quality, we build a large-scale IQA database,
                   namely DiffIQA, containing approximately 180,000 images
                   generated by a diffusion-based image enhancer with adjustable
                   hyper-parameters. Each image is annotated by human subjects
                   as either worse, similar, or better quality compared to its
                   reference. Building on this, we present a generalized FR-IQA
                   model, namely Adaptive Fidelity-Naturalness Evaluator
                   (A-FINE), to accurately assess and adaptively combine the
                   fidelity and naturalness of a test image. A-FINE aligns well
                   with standard FR-IQA when the reference image is much more
                   natural than the test image. We demonstrate by extensive
                   experiments that A-FINE surpasses standard FR-IQA models on
                   well-established IQA datasets and our newly created DiffIQA.
                   To further validate A-FINE, we additionally construct a
                   super-resolution IQA benchmark (SRIQA-Bench), encompassing
                   test images derived from ten state-of-the-art SR methods with
                   reliable human quality annotations. Tests on SRIQA-Bench
                   re-affirm the advantages of A-FINE. The code and dataset are
                   available at
                   https://tianhewu.github.io/A-FINE-page.github.io/.",
  month         =  "14~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.11221"
}

@ARTICLE{Haonan2025-xh,
  title         = "Neurons: Emulating the human visual cortex improves fidelity
                   and interpretability in {fMRI}-to-video reconstruction",
  author        = "Haonan, Wang and Qixiang, Zhang and Lehan, Wang and Xuanqi,
                   Huang and Xiaomeng, Li",
  journal       = "arXiv [cs.CV]",
  abstract      = "Decoding visual stimuli from neural activity is essential for
                   understanding the human brain. While fMRI methods have
                   successfully reconstructed static images, fMRI-to-video
                   reconstruction faces challenges due to the need for capturing
                   spatiotemporal dynamics like motion and scene transitions.
                   Recent approaches have improved semantic and perceptual
                   alignment but struggle to integrate coarse fMRI data with
                   detailed visual features. Inspired by the hierarchical
                   organization of the visual system, we propose NEURONS, a
                   novel framework that decouples learning into four correlated
                   sub-tasks: key object segmentation, concept recognition,
                   scene description, and blurry video reconstruction. This
                   approach simulates the visual cortex's functional
                   specialization, allowing the model to capture diverse video
                   content. In the inference stage, NEURONS generates robust
                   conditioning signals for a pre-trained text-to-video
                   diffusion model to reconstruct the videos. Extensive
                   experiments demonstrate that NEURONS outperforms
                   state-of-the-art baselines, achieving solid improvements in
                   video consistency (26.6\%) and semantic-level accuracy
                   (19.1\%). Notably, NEURONS shows a strong functional
                   correlation with the visual cortex, highlighting its
                   potential for brain-computer interfaces and clinical
                   applications. Code and model weights will be available at:
                   https://github.com/xmed-lab/NEURONS.",
  month         =  "14~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.11167"
}

@ARTICLE{Kammer2025-zy,
  title    = "Decoding peripheral saccade targets from foveal retinotopic cortex",
  author   = "Kämmer, Luca and Kroell, Lisa M and Knapen, Tomas and Rolfs,
              Martin and Hebart, Martin N",
  journal  = "bioRxiv",
  pages    = "2025.02.20.639262",
  abstract = "SummaryHuman vision is characterized by frequent eye movements1.
              This causes continuous shifts in visual input, yet visual
              perception appears highly stable2–4. A potential mechanism behind
              this stability is foveal prediction, involving feedback from
              higher cortical areas during saccade preparation.5,6However, it
              remains unknown (1) whether information is fed back to early
              visual areas, (2) whether feedback is specific to stimulus
              features, and (3) which brain regions mediate this effect. To
              dissociate neural processes associated with stimulus presentation
              from those related to foveal feedback, we designed a
              gaze-contingent fMRI paradigm, where saccade targets are removed
              before they can be foveated. To determine the content of the
              neural representation, we used natural images as saccade targets
              and independently manipulated object shape and category.
              Multivariate analyzes revealed reliable decoding of stimuli from
              foveal retinotopic areas as early as V1, even though the stimulus
              never appeared in the fovea. Decoding was sensitive to shape but
              not semantic category, indicating that only low-to-mid-level
              information is fed back. Cross-decoding to a control condition
              with foveal stimulus presentation yielded reliable decoding,
              indicating a similar neural representation between foveal feedback
              and direct stimulation. Eccentricity-dependent analyzes showed a
              u-shaped decoding curve, confirming that these results are not
              explained by spillover of peripheral activity or large receptive
              fields. Moreover, fluctuations in foveal decodability correlated
              with activity in the intraparietal sulcus, a candidate region for
              driving this foveal feedback. These findings go beyond
              trans-saccadic remapping7,8by suggesting that peripheral saccade
              targets are encoded in the foveal cortex in a feature-specific
              representation5.Highlights-Saccade targets decodable from foveal
              early visual areas without foveal stimulation.-Decoding accuracy
              is affected by stimulus shape, not semantic
              information.-Peripheral stimuli are foveally represented similarly
              to direct foveal stimuli.-Intraparietal sulcus activity is
              consistent with mediation of foveal feedback.",
  month    =  "25~" # feb,
  year     =  2025,
  keywords = "Project/Metamerism;\_To\_read",
  doi      = "10.1101/2025.02.20.639262",
  language = "en"
}

@ARTICLE{Swamy2025-ms,
  title         = "All roads lead to likelihood: The value of reinforcement
                   learning in fine-tuning",
  author        = "Swamy, Gokul and Choudhury, Sanjiban and Sun, Wen and Wu,
                   Zhiwei Steven and Bagnell, J Andrew",
  journal       = "arXiv [cs.LG]",
  abstract      = "From a first-principles perspective, it may seem odd that the
                   strongest results in foundation model fine-tuning (FT) are
                   achieved via a relatively complex, two-stage training
                   procedure. Specifically, one first trains a reward model (RM)
                   on some dataset (e.g. human preferences) before using it to
                   provide online feedback as part of a downstream reinforcement
                   learning (RL) procedure, rather than directly optimizing the
                   policy parameters on the dataset via offline maximum
                   likelihood estimation. In fact, from an information-theoretic
                   perspective, we can only lose information via passing through
                   a reward model and cannot create any new information via
                   on-policy sampling. To explain this discrepancy, we
                   scrutinize several hypotheses on the value of RL in FT
                   through both theoretical and empirical lenses. Of the
                   hypotheses considered, we find the most support for the
                   explanation that on problems with a generation-verification
                   gap, the combination of the ease of learning the relatively
                   simple RM (verifier) from the preference data, coupled with
                   the ability of the downstream RL procedure to then filter its
                   search space to the subset of policies (generators) that are
                   optimal for relatively simple verifiers is what leads to the
                   superior performance of online FT.",
  month         =  "2~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2503.01067",
  keywords      = "\_To\_read"
}

@ARTICLE{Xu2025-yo,
  title         = "Deciphering functions of neurons in vision-language models",
  author        = "Xu, Jiaqi and Lan, Cuiling and Chen, Xuejin and Lu, Yan",
  journal       = "arXiv [q-bio.NC]",
  abstract      = "The burgeoning growth of open-sourced vision-language models
                   (VLMs) has catalyzed a plethora of applications across
                   diverse domains. Ensuring the transparency and
                   interpretability of these models is critical for fostering
                   trustworthy and responsible AI systems. In this study, our
                   objective is to delve into the internals of VLMs to interpret
                   the functions of individual neurons. We observe the
                   activations of neurons with respects to the input visual
                   tokens and text tokens, and reveal some interesting findings.
                   Particularly, we found that there are neurons responsible for
                   only visual or text information, or both, respectively, which
                   we refer to them as visual neurons, text neurons, and
                   multi-modal neurons, respectively. We build a framework that
                   automates the explanation of neurons with the assistant of
                   GPT-4o. Meanwhile, for visual neurons, we propose an
                   activation simulator to assess the reliability of the
                   explanations for visual neurons. System statistical analyses
                   on top of one representative VLM of LLaVA, uncover the
                   behaviors/characteristics of different categories of neurons.",
  month         =  "10~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "2502.18485"
}

@ARTICLE{Sun2025-ws,
  title         = "Idiosyncrasies in Large Language Models",
  author        = "Sun, Mingjie and Yin, Yida and Xu, Zhiqiu and Kolter, J Zico
                   and Liu, Zhuang",
  journal       = "arXiv [cs.CL]",
  abstract      = "In this work, we unveil and study idiosyncrasies in Large
                   Language Models (LLMs) -- unique patterns in their outputs
                   that can be used to distinguish the models. To do so, we
                   consider a simple classification task: given a particular
                   text output, the objective is to predict the source LLM that
                   generates the text. We evaluate this synthetic task across
                   various groups of LLMs and find that simply fine-tuning
                   existing text embedding models on LLM-generated texts yields
                   excellent classification accuracy. Notably, we achieve 97.1\%
                   accuracy on held-out validation data in the five-way
                   classification problem involving ChatGPT, Claude, Grok,
                   Gemini, and DeepSeek. Our further investigation reveals that
                   these idiosyncrasies are rooted in word-level distributions.
                   These patterns persist even when the texts are rewritten,
                   translated, or summarized by an external LLM, suggesting that
                   they are also encoded in the semantic content. Additionally,
                   we leverage LLM as judges to generate detailed, open-ended
                   descriptions of each model's idiosyncrasies. Finally, we
                   discuss the broader implications of our findings,
                   particularly for training on synthetic data and inferring
                   model similarity. Code is available at
                   https://github.com/locuslab/llm-idiosyncrasies.",
  month         =  "17~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2502.12150"
}

@ARTICLE{Li2024-mv,
  title         = "Shift-{ConvNets}: Small convolutional kernel with large
                   kernel effects",
  author        = "Li, Dachong and Li, Li and Chen, Zhuangzhuang and Li,
                   Jianqiang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Recent studies reveal that the remarkable performance of
                   Vision transformers (ViTs) benefits from large receptive
                   fields. For this reason, the large convolutional kernel
                   design becomes an ideal solution to make Convolutional Neural
                   Networks (CNNs) great again. However, the typical large
                   convolutional kernels turn out to be hardware-unfriendly
                   operators, resulting in discount compatibility of various
                   hardware platforms. Thus, it is unwise to simply enlarge the
                   convolutional kernel size. In this paper, we reveal that
                   small convolutional kernels and convolution operations can
                   achieve the closing effects of large kernel sizes. Then, we
                   propose a shift-wise operator that ensures the CNNs capture
                   long-range dependencies with the help of the sparse
                   mechanism, while remaining hardware-friendly. Experimental
                   results show that our shift-wise operator significantly
                   improves the accuracy of a regular CNN while markedly
                   reducing computational requirements. On the ImageNet-1k, our
                   shift-wise enhanced CNN model outperforms the
                   state-of-the-art models. Code \& models at
                   https://github.com/lidc54/shift-wiseConv.",
  month         =  "23~" # jan,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2401.12736"
}

@ARTICLE{Jiachen2025-lz,
  title         = "Transformers without normalization",
  author        = "Jiachen, Zhu and Xinlei, Chen and Kaiming, He and Yann, Lecun
                   and Zhuang, Liu",
  journal       = "arXiv [cs.LG]",
  abstract      = "Normalization layers are ubiquitous in modern neural networks
                   and have long been considered essential. This work
                   demonstrates that Transformers without normalization can
                   achieve the same or better performance using a remarkably
                   simple technique. We introduce Dynamic Tanh (DyT), an
                   element-wise operation $DyT($x$) = \tanh(\alpha $x$)$, as a
                   drop-in replacement for normalization layers in Transformers.
                   DyT is inspired by the observation that layer normalization
                   in Transformers often produces tanh-like, $S$-shaped
                   input-output mappings. By incorporating DyT, Transformers
                   without normalization can match or exceed the performance of
                   their normalized counterparts, mostly without hyperparameter
                   tuning. We validate the effectiveness of Transformers with
                   DyT across diverse settings, ranging from recognition to
                   generation, supervised to self-supervised learning, and
                   computer vision to language models. These findings challenge
                   the conventional understanding that normalization layers are
                   indispensable in modern neural networks, and offer new
                   insights into their role in deep networks.",
  month         =  "13~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2503.10622"
}

@ARTICLE{Becker2025-jj,
  title         = "Controlling Latent Diffusion Using Latent {CLIP}",
  author        = "Becker, Jason and Wendler, Chris and Baylies, Peter and West,
                   Robert and Wressnegger, Christian",
  journal       = "arXiv [cs.CV]",
  abstract      = "Instead of performing text-conditioned denoising in the image
                   domain, latent diffusion models (LDMs) operate in latent
                   space of a variational autoencoder (VAE), enabling more
                   efficient processing at reduced computational costs. However,
                   while the diffusion process has moved to the latent space,
                   the contrastive language-image pre-training (CLIP) models, as
                   used in many image processing tasks, still operate in pixel
                   space. Doing so requires costly VAE-decoding of latent images
                   before they can be processed. In this paper, we introduce
                   Latent-CLIP, a CLIP model that operates directly in the
                   latent space. We train Latent-CLIP on 2.7B pairs of latent
                   images and descriptive texts, and show that it matches
                   zero-shot classification performance of similarly sized CLIP
                   models on both the ImageNet benchmark and a LDM-generated
                   version of it, demonstrating its effectiveness in assessing
                   both real and generated content. Furthermore, we construct
                   Latent-CLIP rewards for reward-based noise optimization
                   (ReNO) and show that they match the performance of their CLIP
                   counterparts on GenEval and T2I-CompBench while cutting the
                   cost of the total pipeline by 21\%. Finally, we use
                   Latent-CLIP to guide generation away from harmful content,
                   achieving strong performance on the inappropriate image
                   prompts (I2P) benchmark and a custom evaluation, without ever
                   requiring the costly step of decoding intermediate images.",
  month         =  "11~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.08455"
}

@ARTICLE{Raphi2025-nz,
  title         = "Is {CLIP} ideal? No. Can we fix it? Yes!",
  author        = "Raphi, Kang and Yue, Song and Georgia, Gkioxari and Pietro,
                   Perona",
  journal       = "arXiv [cs.LG]",
  abstract      = "Contrastive Language-Image Pre-Training (CLIP) is a popular
                   method for learning multimodal latent spaces with
                   well-organized semantics. Despite its wide range of
                   applications, CLIP's latent space is known to fail at
                   handling complex visual-textual interactions. Recent works
                   attempt to address its shortcomings with data-centric or
                   algorithmic approaches. But what if the problem is more
                   fundamental, and lies in the geometry of CLIP? Toward this
                   end, we rigorously analyze CLIP's latent space properties,
                   and prove that no CLIP-like joint embedding space exists
                   which can correctly do any two of the following at the same
                   time: 1. represent basic descriptions and image content, 2.
                   represent attribute binding, 3. represent spatial location
                   and relationships, 4. represent negation. Informed by this
                   analysis, we propose Dense Cosine Similarity Maps (DCSMs) as
                   a principled and interpretable scoring method for CLIP-like
                   models, which solves the fundamental limitations of CLIP by
                   retaining the semantic topology of the image patches and text
                   tokens. This method improves upon the performance of
                   classical CLIP-like joint encoder models on a wide array of
                   benchmarks. We share our code and data here for
                   reproducibility: https://github.com/Raphoo/DCSM\_Ideal\_CLIP",
  month         =  "10~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2503.08723"
}

@ARTICLE{Alessandro2025-jz,
  title         = "A {7T} {fMRI} dataset of synthetic images for
                   out-of-distribution modeling of vision",
  author        = "Alessandro, T Gifford and Radoslaw, M Cichy and Thomas,
                   Naselaris and Kendrick, Kay",
  journal       = "arXiv [q-bio.NC]",
  abstract      = "Large-scale visual neural datasets such as the Natural Scenes
                   Dataset (NSD) are boosting NeuroAI research by enabling
                   computational models of the brain with performances beyond
                   what was possible just a decade ago. However, these datasets
                   lack out-of-distribution (OOD) components, which are crucial
                   for the development of more robust models. Here, we address
                   this limitation by releasing NSD-synthetic, a dataset
                   consisting of 7T fMRI responses from the eight NSD subjects
                   for 284 carefully controlled synthetic images. We show that
                   NSD-synthetic's fMRI responses reliably encode
                   stimulus-related information and are OOD with respect to NSD.
                   Furthermore, OOD generalization tests on NSD-synthetic reveal
                   differences between models of the brain that are not detected
                   with NSD - specifically, self-supervised deep neural networks
                   better explain neural responses than their task-supervised
                   counterparts. These results showcase how NSD-synthetic
                   enables OOD generalization tests that facilitate the
                   development of more robust models of visual processing, and
                   the formulation of more accurate theories of human vision.",
  month         =  "8~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "2503.06286"
}

@ARTICLE{Hernandez2023-bt,
  title         = "Linearity of relation decoding in transformer language models",
  author        = "Hernandez, Evan and Sharma, Arnab Sen and Haklay, Tal and
                   Meng, Kevin and Wattenberg, Martin and Andreas, Jacob and
                   Belinkov, Yonatan and Bau, David",
  journal       = "arXiv [cs.CL]",
  abstract      = "Much of the knowledge encoded in transformer language models
                   (LMs) may be expressed in terms of relations: relations
                   between words and their synonyms, entities and their
                   attributes, etc. We show that, for a subset of relations,
                   this computation is well-approximated by a single linear
                   transformation on the subject representation. Linear relation
                   representations may be obtained by constructing a first-order
                   approximation to the LM from a single prompt, and they exist
                   for a variety of factual, commonsense, and linguistic
                   relations. However, we also identify many cases in which LM
                   predictions capture relational knowledge accurately, but this
                   knowledge is not linearly encoded in their representations.
                   Our results thus reveal a simple, interpretable, but
                   heterogeneously deployed knowledge representation strategy in
                   transformer LMs.",
  month         =  "17~" # aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2308.09124",
  keywords      = "\_To\_read"
}

@ARTICLE{Yang2003-oy,
  title     = "A statistical explanation of visual space",
  author    = "Yang, Zhiyong and Purves, Dale",
  journal   = "Nature neuroscience",
  publisher = "Springer Science and Business Media LLC",
  volume    =  6,
  number    =  6,
  pages     = "632--640",
  abstract  = "The subjective visual space perceived by humans does not reflect
               a simple transformation of objective physical space; rather,
               perceived space has an idiosyncratic relationship with the real
               world. To date, there is no consensus about either the genesis of
               perceived visual space or the implications of its peculiar
               characteristics for visually guided behavior. Here we used laser
               range scanning to measure the actual distances from the image
               plane of all unoccluded points in a series of natural scenes. We
               then asked whether the differences between real and apparent
               distances could be explained by the statistical relationship of
               scene geometry and the observer. We were able to predict
               perceived distances in a variety of circumstances from the
               probability distribution of physical distances. This finding
               lends support to the idea that the characteristics of human
               visual space are determined probabilistically.",
  month     =  "18~" # jun,
  year      =  2003,
  keywords  = "Project/DepthModel",
  doi       = "10.1038/nn1059",
  pmid      =  12754512,
  issn      = "1097-6256,1546-1726",
  language  = "en"
}

@ARTICLE{Sansone2023-ht,
  title         = "Unifying self-supervised clustering and energy-based models",
  author        = "Sansone, Emanuele and Manhaeve, Robin",
  journal       = "arXiv [cs.LG]",
  abstract      = "Self-supervised learning excels at learning representations
                   from large amounts of data. At the same time, generative
                   models offer the complementary property of learning
                   information about the underlying data generation process. In
                   this study, we aim at establishing a principled connection
                   between these two paradigms and highlight the benefits of
                   their complementarity. In particular, we perform an analysis
                   of self-supervised learning objectives, elucidating the
                   underlying probabilistic graphical models and presenting a
                   standardized methodology for their derivation from first
                   principles. The analysis suggests a natural means of
                   integrating self-supervised learning with likelihood-based
                   generative models. We instantiate this concept within the
                   realm of cluster-based self-supervised learning and energy
                   models, introducing a lower bound proven to reliably penalize
                   the most important failure modes. Our theoretical findings
                   are substantiated through experiments on synthetic and
                   real-world data, including SVHN, CIFAR10, and CIFAR100,
                   demonstrating that our objective function allows to jointly
                   train a backbone network in a discriminative and generative
                   fashion, consequently outperforming existing self-supervised
                   learning strategies in terms of clustering, generation and
                   out-of-distribution detection performance by a wide margin.
                   We also demonstrate that the solution can be integrated into
                   a neuro-symbolic framework to tackle a simple yet non-trivial
                   instantiation of the symbol grounding problem.",
  month         =  "29~" # dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2401.00873"
}

@ARTICLE{Krish2025-ad,
  title         = "{LapLoss}: Laplacian Pyramid-based Multiscale loss for Image
                   Translation",
  author        = "Krish, Didwania and Ishaan, Gakhar and Prakhar, Arya and
                   Sanskriti, Labroo",
  journal       = "arXiv [eess.IV]",
  abstract      = "Contrast enhancement, a key aspect of image-to-image
                   translation (I2IT), improves visual quality by adjusting
                   intensity differences between pixels. However, many existing
                   methods struggle to preserve fine-grained details, often
                   leading to the loss of low-level features. This paper
                   introduces LapLoss, a novel approach designed for I2IT
                   contrast enhancement, based on the Laplacian pyramid-centric
                   networks, forming the core of our proposed methodology. The
                   proposed approach employs a multiple discriminator
                   architecture, each operating at a different resolution to
                   capture high-level features, in addition to maintaining
                   low-level details and textures under mixed lighting
                   conditions. The proposed methodology computes the loss at
                   multiple scales, balancing reconstruction accuracy and
                   perceptual quality to enhance overall image generation. The
                   distinct blend of the loss calculation at each level of the
                   pyramid, combined with the architecture of the Laplacian
                   pyramid enables LapLoss to exceed contemporary contrast
                   enhancement techniques. This framework achieves
                   state-of-the-art results, consistently performing well across
                   different lighting conditions in the SICE dataset.",
  month         =  "7~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "eess.IV",
  eprint        = "2503.05974"
}

@ARTICLE{Selcuk2025-gd,
  title         = "Illuminant and light direction estimation using Wasserstein
                   distance method",
  author        = "Selcuk, Yazar",
  journal       = "arXiv [eess.IV]",
  abstract      = "Illumination estimation remains a pivotal challenge in image
                   processing, particularly for robotics, where robust
                   environmental perception is essential under varying lighting
                   conditions. Traditional approaches, such as RGB histograms
                   and GIST descriptors, often fail in complex scenarios due to
                   their sensitivity to illumination changes. This study
                   introduces a novel method utilizing the Wasserstein distance,
                   rooted in optimal transport theory, to estimate illuminant
                   and light direction in images. Experiments on diverse images
                   indoor scenes, black-and-white photographs, and night images
                   demonstrate the method's efficacy in detecting dominant light
                   sources and estimating their directions, outperforming
                   traditional statistical methods in complex lighting
                   environments. The approach shows promise for applications in
                   light source localization, image quality assessment, and
                   object detection enhancement. Future research may explore
                   adaptive thresholding and integrate gradient analysis to
                   enhance accuracy, offering a scalable solution for real-world
                   illumination challenges in robotics and beyond.",
  month         =  "3~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "eess.IV",
  eprint        = "2503.05802"
}

@ARTICLE{Zhihao2025-qi,
  title         = "{NFIG}: Autoregressive image generation with next-frequency
                   prediction",
  author        = "Zhihao, Huang and Xi, Qiu and Yukuo, Ma and Yifu, Zhou and
                   Chi, Zhang and Xuelong, Li",
  journal       = "arXiv [cs.CV]",
  abstract      = "Autoregressive models have achieved promising results in
                   natural language processing. However, for image generation
                   tasks, they encounter substantial challenges in effectively
                   capturing long-range dependencies, managing computational
                   costs, and most crucially, defining meaningful autoregressive
                   sequences that reflect natural image hierarchies. To address
                   these issues, we present \textbf{N}ext-\textbf{F}requency
                   \textbf{I}mage \textbf{G}eneration (\textbf{NFIG}), a novel
                   framework that decomposes the image generation process into
                   multiple frequency-guided stages. Our approach first
                   generates low-frequency components to establish global
                   structure with fewer tokens, then progressively adds
                   higher-frequency details, following the natural spectral
                   hierarchy of images. This principled autoregressive sequence
                   not only improves the quality of generated images by better
                   capturing true causal relationships between image components,
                   but also significantly reduces computational overhead during
                   inference. Extensive experiments demonstrate that NFIG
                   achieves state-of-the-art performance with fewer steps,
                   offering a more efficient solution for image generation, with
                   1.25$\times$ speedup compared to VAR-d20 while achieving
                   better performance (FID: 2.81) on the ImageNet-256 benchmark.
                   We hope that our insight of incorporating frequency-domain
                   knowledge to guide autoregressive sequence design will shed
                   light on future research. We will make our code publicly
                   available upon acceptance of the paper.",
  month         =  "10~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.07076"
}

@ARTICLE{Benjamin2025-gx,
  title         = "Investigating image manifolds of {3D} objects: Learning,
                   shape analysis, and comparisons",
  author        = "Benjamin, Beaudett and Shenyuan, Liang and Anuj, Srivastava",
  journal       = "arXiv [cs.CV]",
  abstract      = "Despite high-dimensionality of images, the sets of images of
                   3D objects have long been hypothesized to form
                   low-dimensional manifolds. What is the nature of such
                   manifolds? How do they differ across objects and object
                   classes? Answering these questions can provide key insights
                   in explaining and advancing success of machine learning
                   algorithms in computer vision. This paper investigates dual
                   tasks -- learning and analyzing shapes of image manifolds --
                   by revisiting a classical problem of manifold learning but
                   from a novel geometrical perspective. It uses
                   geometry-preserving transformations to map the pose image
                   manifolds, sets of images formed by rotating 3D objects, to
                   low-dimensional latent spaces. The pose manifolds of
                   different objects in latent spaces are found to be nonlinear,
                   smooth manifolds. The paper then compares shapes of these
                   manifolds for different objects using Kendall's shape
                   analysis, modulo rigid motions and global scaling, and
                   clusters objects according to these shape metrics.
                   Interestingly, pose manifolds for objects from the same
                   classes are frequently clustered together. The geometries of
                   image manifolds can be exploited to simplify vision and image
                   processing tasks, to predict performances, and to provide
                   insights into learning methods.",
  month         =  "9~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.06773"
}

@ARTICLE{Shum2025-il,
  title         = "Color Alignment in Diffusion",
  author        = "Shum, Ka Chun and Binh-Son, Hua and Nguyen, Duc Thanh and
                   Sai-Kit, Yeung",
  journal       = "arXiv [cs.CV]",
  abstract      = "Diffusion models have shown great promise in synthesizing
                   visually appealing images. However, it remains challenging to
                   condition the synthesis at a fine-grained level, for
                   instance, synthesizing image pixels following some generic
                   color pattern. Existing image synthesis methods often produce
                   contents that fall outside the desired pixel conditions. To
                   address this, we introduce a novel color alignment algorithm
                   that confines the generative process in diffusion models
                   within a given color pattern. Specifically, we project
                   diffusion terms, either imagery samples or latent
                   representations, into a conditional color space to align with
                   the input color distribution. This strategy simplifies the
                   prediction in diffusion models within a color manifold while
                   still allowing plausible structures in generated contents,
                   thus enabling the generation of diverse contents that comply
                   with the target color pattern. Experimental results
                   demonstrate our state-of-the-art performance in conditioning
                   and controlling of color pixels, while maintaining on-par
                   generation quality and diversity in comparison with regular
                   diffusion models.",
  month         =  "9~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.06746"
}

@ARTICLE{Chen2025-jy,
  title         = "Generative Video Bi-flow",
  author        = "Chen, Liu and Tobias, Ritschel",
  journal       = "arXiv [cs.CV]",
  abstract      = "We propose a novel generative video model by robustly
                   learning temporal change as a neural Ordinary Differential
                   Equation (ODE) flow with a bilinear objective of combining
                   two aspects: The first is to map from the past into future
                   video frames directly. Previous work has mapped the noise to
                   new frames, a more computationally expensive process.
                   Unfortunately, starting from the previous frame, instead of
                   noise, is more prone to drifting errors. Hence, second, we
                   additionally learn how to remove the accumulated errors as
                   the joint objective by adding noise during training. We
                   demonstrate unconditional video generation in a streaming
                   manner for various video datasets, all at competitive quality
                   compared to a baseline conditional diffusion but with higher
                   speed, i.e., fewer ODE solver steps.",
  month         =  "9~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.06364"
}

@ARTICLE{Cagatan2025-et,
  title         = "Adversarial robustness of discriminative self-supervised
                   learning in vision",
  author        = "Çağatan, Ömer Veysel and Tal, Ömer Faruk and Emre Gürsoy, M",
  journal       = "arXiv [cs.CV]",
  abstract      = "Self-supervised learning (SSL) has advanced significantly in
                   visual representation learning, yet comprehensive evaluations
                   of its adversarial robustness remain limited. In this study,
                   we evaluate the adversarial robustness of seven
                   discriminative self-supervised models and one supervised
                   model across diverse tasks, including ImageNet
                   classification, transfer learning, segmentation, and
                   detection. Our findings suggest that discriminative SSL
                   models generally exhibit better robustness to adversarial
                   attacks compared to their supervised counterpart on ImageNet,
                   with this advantage extending to transfer learning when using
                   linear evaluation. However, when fine-tuning is applied, the
                   robustness gap between SSL and supervised models narrows
                   considerably. Similarly, this robustness advantage diminishes
                   in segmentation and detection tasks. We also investigate how
                   various factors might influence adversarial robustness,
                   including architectural choices, training duration, data
                   augmentations, and batch sizes. Our analysis contributes to
                   the ongoing exploration of adversarial robustness in visual
                   self-supervised representation systems.",
  month         =  "8~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.06361"
}

@ARTICLE{Seil2025-zo,
  title         = "Your Large Vision-language model only needs A few attention
                   heads for visual grounding",
  author        = "Seil, Kang and Jinyeong, Kim and Junhyeok, Kim and Hwang,
                   Seong Jae",
  journal       = "arXiv [cs.CV]",
  abstract      = "Visual grounding seeks to localize the image region
                   corresponding to a free-form text description. Recently, the
                   strong multimodal capabilities of Large Vision-Language
                   Models (LVLMs) have driven substantial improvements in visual
                   grounding, though they inevitably require fine-tuning and
                   additional model components to explicitly generate bounding
                   boxes or segmentation masks. However, we discover that a few
                   attention heads in frozen LVLMs demonstrate strong visual
                   grounding capabilities. We refer to these heads, which
                   consistently capture object locations related to text
                   semantics, as localization heads. Using localization heads,
                   we introduce a straightforward and effective training-free
                   visual grounding framework that utilizes text-to-image
                   attention maps from localization heads to identify the target
                   objects. Surprisingly, only three out of thousands of
                   attention heads are sufficient to achieve competitive
                   localization performance compared to existing LVLM-based
                   visual grounding methods that require fine-tuning. Our
                   findings suggest that LVLMs can innately ground objects based
                   on a deep comprehension of the text-image relationship, as
                   they implicitly focus on relevant image regions to generate
                   informative text outputs. All the source codes will be made
                   available to the public.",
  month         =  "8~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.06287"
}

@ARTICLE{Wei-En2025-ig,
  title         = "Segment anything, even occluded",
  author        = "Wei-En, Tai and Yu-Lin, Shih and Cheng, Sun and Wang,
                   Yu-Chiang Frank and Hwann-Tzong, Chen",
  journal       = "arXiv [cs.CV]",
  abstract      = "Amodal instance segmentation, which aims to detect and
                   segment both visible and invisible parts of objects in
                   images, plays a crucial role in various applications
                   including autonomous driving, robotic manipulation, and scene
                   understanding. While existing methods require training both
                   front-end detectors and mask decoders jointly, this approach
                   lacks flexibility and fails to leverage the strengths of
                   pre-existing modal detectors. To address this limitation, we
                   propose SAMEO, a novel framework that adapts the Segment
                   Anything Model (SAM) as a versatile mask decoder capable of
                   interfacing with various front-end detectors to enable mask
                   prediction even for partially occluded objects. Acknowledging
                   the constraints of limited amodal segmentation datasets, we
                   introduce Amodal-LVIS, a large-scale synthetic dataset
                   comprising 300K images derived from the modal LVIS and LVVIS
                   datasets. This dataset significantly expands the training
                   data available for amodal segmentation research. Our
                   experimental results demonstrate that our approach, when
                   trained on the newly extended dataset, including Amodal-LVIS,
                   achieves remarkable zero-shot performance on both COCOA-cls
                   and D2SA benchmarks, highlighting its potential for
                   generalization to unseen scenarios.",
  month         =  "8~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.06261"
}

@ARTICLE{Xiang2025-nl,
  title         = "{PTDiffusion}: Free lunch for generating optical illusion
                   hidden pictures with phase-transferred diffusion model",
  author        = "Xiang, Gao and Shuai, Yang and Jiaying, Liu",
  journal       = "arXiv [cs.CV]",
  abstract      = "Optical illusion hidden picture is an interesting visual
                   perceptual phenomenon where an image is cleverly integrated
                   into another picture in a way that is not immediately obvious
                   to the viewer. Established on the off-the-shelf text-to-image
                   (T2I) diffusion model, we propose a novel training-free
                   text-guided image-to-image (I2I) translation framework dubbed
                   as \textbf{P}hase-\textbf{T}ransferred \textbf{Diffusion}
                   Model (PTDiffusion) for hidden art syntheses. PTDiffusion
                   embeds an input reference image into arbitrary scenes as
                   described by the text prompts, while exhibiting hidden visual
                   cues of the reference image. At the heart of our method is a
                   plug-and-play phase transfer mechanism that dynamically and
                   progressively transplants diffusion features' phase spectrum
                   from the denoising process to reconstruct the reference image
                   into the one to sample the generated illusion image,
                   realizing harmonious fusion of the reference structural
                   information and the textual semantic information.
                   Furthermore, we propose asynchronous phase transfer to enable
                   flexible control to the degree of hidden content
                   discernability. Our method bypasses any model training and
                   fine-tuning, all while substantially outperforming related
                   methods in image quality, text fidelity, visual
                   discernibility, and contextual naturalness for illusion
                   picture synthesis, as demonstrated by extensive qualitative
                   and quantitative experiments.",
  month         =  "8~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.06186"
}

@ARTICLE{Xiaohao2025-mi,
  title         = "Towards ambiguity-free spatial foundation model: Rethinking
                   and decoupling depth ambiguity",
  author        = "Xiaohao, Xu and Feng, Xue and Xiang, Li and Haowei, Li and
                   Shusheng, Yang and Tianyi, Zhang and Matthew,
                   Johnson-Roberson and Xiaonan, Huang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Depth ambiguity is a fundamental challenge in spatial scene
                   understanding, especially in transparent scenes where
                   single-depth estimates fail to capture full 3D structure.
                   Existing models, limited to deterministic predictions,
                   overlook real-world multi-layer depth. To address this, we
                   introduce a paradigm shift from single-prediction to
                   multi-hypothesis spatial foundation models. We first present
                   \texttt{MD-3k}, a benchmark exposing depth biases in expert
                   and foundational models through multi-layer spatial
                   relationship labels and new metrics. To resolve depth
                   ambiguity, we propose Laplacian Visual Prompting (LVP), a
                   training-free spectral prompting technique that extracts
                   hidden depth from pre-trained models via
                   Laplacian-transformed RGB inputs. By integrating LVP-inferred
                   depth with standard RGB-based estimates, our approach elicits
                   multi-layer depth without model retraining. Extensive
                   experiments validate the effectiveness of LVP in zero-shot
                   multi-layer depth estimation, unlocking more robust and
                   comprehensive geometry-conditioned visual generation,
                   3D-grounded spatial reasoning, and temporally consistent
                   video-level depth inference. Our benchmark and code will be
                   available at
                   https://github.com/Xiaohao-Xu/Ambiguity-in-Space.",
  month         =  "8~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.06014"
}

@ARTICLE{Ming2025-qv,
  title         = "Is your video language model a reliable judge?",
  author        = "Ming, Liu and Wensheng, Zhang",
  journal       = "arXiv [cs.CV]",
  abstract      = "As video language models (VLMs) gain more applications in
                   various scenarios, the need for robust and scalable
                   evaluation of their performance becomes increasingly
                   critical. The traditional human expert-based evaluation of
                   VLMs has limitations in consistency and scalability, which
                   sparked interest in automatic methods such as employing VLMs
                   to evaluate VLMs. However, the reliability of VLMs as judges
                   remains underexplored. Existing methods often rely on a
                   single VLM as the evaluator. However, this approach can be
                   unreliable or biased because such a model may lack the
                   ability to fully understand the content and may have inherent
                   biases, ultimately compromising evaluation reliability. A
                   remedy is to apply the principle of collective thoughts,
                   aggregating evaluations from multiple VLMs to enhance
                   reliability. This study investigates the efficacy of such
                   approaches, particularly when the pool of judges includes
                   both reliable and unreliable models. Our findings reveal that
                   incorporating collective judgments from such a mixed pool
                   does not necessarily improve the accuracy of the final
                   evaluation. The inclusion of less reliable judges can
                   introduce noise, undermining the overall reliability of the
                   outcomes. To explore the factors that impact evaluation
                   reliability, we fine-tune an underperforming VLM judge,
                   Video-LLaVA, and observe that improved understanding ability
                   alone is insufficient to make VLM judges more reliable. These
                   findings stress the limitations of collective thought
                   approaches and highlight the need for more advanced methods
                   that can account for the reliability of individual models.
                   Our study promotes the development of more reliable
                   evaluation methods for VLMs",
  month         =  "7~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.05977"
}

@ARTICLE{Scarfe2017-ic,
  title     = "A Bayesian model of distance perception from ocular convergence",
  author    = "Scarfe, P and Hibbard, P",
  journal   = "Journal of Vision",
  publisher = "The Association for Research in Vision and Ophthalmology",
  volume    =  17,
  number    =  10,
  pages     = "159--159",
  month     =  "1~" # sep,
  year      =  2017,
  doi       = "10.1167/17.10.159"
}

@ARTICLE{Graham2025-ms,
  title         = "Three tiers of computation in transformers and in brain
                   architectures",
  author        = "Graham, E and Granger, R",
  journal       = "arXiv [cs.CL]",
  abstract      = "Specific empirical phenomena spanning human natural language,
                   and mathematical and logical abilities, are rigorously
                   situated in the well-studied grammar-automata (G-A)
                   hierarchy. We identify three tiers and corresponding two
                   transitions within the hierarchy and show their
                   correspondence to the emergence of particular abilities in
                   humans and in transformer-based language models (LMs). These
                   emergent abilities have often been described in terms of
                   ``scaling''; we show that it is the transition between tiers,
                   rather than size itself, that determines a system's
                   capabilities. Specifically, humans effortlessly process
                   language yet require specific training to perform arithmetic
                   or logical reasoning tasks; and LMs possess language
                   abilities absent from predecessor systems yet still struggle
                   with logical processing. The resulting principled analyses
                   provide underlying explanatory accounts of both the abilities
                   and shortfalls of these systems, and suggest actionable
                   insights into the expansion of logic abilities in AI systems.",
  month         =  "5~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2503.04848"
}

@ARTICLE{Imam2024-xp,
  title         = "{CLIP} meets {DINO} for Tuning Zero-Shot Classifier using
                   Unlabeled Image Collections",
  author        = "Imam, Mohamed Fazli and Marew, Rufael Fedaku and Hassan,
                   Jameel and Fiaz, Mustansar and Aji, Alham Fikri and
                   Cholakkal, Hisham",
  journal       = "arXiv [cs.CV]",
  abstract      = "In the era of foundation models, CLIP has emerged as a
                   powerful tool for aligning text and visual modalities into a
                   common embedding space. However, the alignment objective used
                   to train CLIP often results in subpar visual features for
                   fine-grained tasks. In contrast, SSL-pretrained models like
                   DINO excel at extracting rich visual features due to their
                   specialized training paradigm. Yet, these SSL models require
                   an additional supervised linear probing step, which relies on
                   fully labeled data which is often expensive and difficult to
                   obtain at scale. In this paper, we propose a label-free
                   prompt-tuning method that leverages the rich visual features
                   of self-supervised learning models (DINO) and the broad
                   textual knowledge of large language models (LLMs) to largely
                   enhance CLIP-based image classification performance using
                   unlabeled images. Our approach unfolds in three key steps:
                   (1) We generate robust textual feature embeddings that more
                   accurately represent object classes by leveraging
                   class-specific descriptions from LLMs, enabling more
                   effective zero-shot classification compared to CLIP's default
                   name-specific prompts. (2) These textual embeddings are then
                   used to produce pseudo-labels to train an alignment module
                   that integrates the complementary strengths of LLM
                   description-based textual embeddings and DINO's visual
                   features. (3) Finally, we prompt-tune CLIP's vision encoder
                   through DINO-assisted supervision using the trained alignment
                   module. This three-step process allows us to harness the best
                   of visual and textual foundation models, resulting in a
                   powerful and efficient approach that surpasses
                   state-of-the-art label-free classification methods. Notably,
                   our framework, NoLA (No Labels Attached), achieves an average
                   absolute gain of 3.6\% over the state-of-the-art LaFter
                   across 11 diverse image classification datasets.",
  month         =  "28~" # nov,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2411.19346"
}

@ARTICLE{Martin2025-uf,
  title         = "{PhysicsGen}: Can generative models learn from images to
                   predict complex physical relations?",
  author        = "Martin, Spitznagel and Jan, Vaillant and Janis, Keuper",
  journal       = "arXiv [cs.CV]",
  abstract      = "The image-to-image translation abilities of generative
                   learning models have recently made significant progress in
                   the estimation of complex (steered) mappings between image
                   distributions. While appearance based tasks like image
                   in-painting or style transfer have been studied at length, we
                   propose to investigate the potential of generative models in
                   the context of physical simulations. Providing a dataset of
                   300k image-pairs and baseline evaluations for three different
                   physical simulation tasks, we propose a benchmark to
                   investigate the following research questions: i) are
                   generative models able to learn complex physical relations
                   from input-output image pairs? ii) what speedups can be
                   achieved by replacing differential equation based
                   simulations? While baseline evaluations of different current
                   models show the potential for high speedups (ii), these
                   results also show strong limitations toward the physical
                   correctness (i). This underlines the need for new methods to
                   enforce physical correctness. Data, baseline models and
                   evaluation code http://www.physics-gen.org.",
  month         =  "7~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.05333"
}

@ARTICLE{Zhang2025-cy,
  title    = "Signaling of trans-saccadic prediction error by foveal neurons of
              the monkey superior colliculus",
  author   = "Zhang, Tong and Bogadhi, Amarender and Hafed, Ziad M",
  journal  = "bioRxiv",
  pages    = "2025.03.02.641039",
  abstract = "Across saccades, neurons in retinotopically organized visual
              representations experience drastically different images, but
              visual percepts remain stable. Here we investigated whether such
              stability can be mediated, in part, via prediction-error signaling
              by neurons processing post-saccadic visual images. We specifically
              recorded from foveal superior colliculus (SC) neurons when a
              visual image only overlapped with their response fields (RF's)
              after foveating saccades but not pre-saccadically. When we rapidly
              changed the target features intra-saccadically, the foveal
              neurons' post-saccadic visual reafferent responses were elevated,
              even though the neurons did not directly sample the pre-saccadic
              extrafoveal target features. This effect did not occur in the
              absence of saccades, and it also scaled with the extent of the
              introduced intra-saccadic image feature discrepancies. These
              results suggest that foveal SC neurons may signal a trans-saccadic
              prediction error when the foveated image stimulating them is
              inconsistent with that expected from pre-saccadic extrafoveal
              representations, a potential perceptual stability mechanism.",
  month    =  "7~" # mar,
  year     =  2025,
  doi      = "10.1101/2025.03.02.641039"
}

@ARTICLE{Cina2024-vq,
  title         = "$σ$-zero: Gradient-based Optimization of $\ell_0$-norm
                   Adversarial Examples",
  author        = "Cinà, Antonio Emanuele and Francesco, Villani and Maura,
                   Pintor and Lea, Schönherr and Battista, Biggio and Marcello,
                   Pelillo",
  journal       = "arXiv [cs.LG]",
  abstract      = "Evaluating the adversarial robustness of deep networks to
                   gradient-based attacks is challenging. While most attacks
                   consider $\ell_2$- and $\ell_\infty$-norm constraints to
                   craft input perturbations, only a few investigate sparse
                   $\ell_1$- and $\ell_0$-norm attacks. In particular,
                   $\ell_0$-norm attacks remain the least studied due to the
                   inherent complexity of optimizing over a non-convex and
                   non-differentiable constraint. However, evaluating
                   adversarial robustness under these attacks could reveal
                   weaknesses otherwise left untested with more conventional
                   $\ell_2$- and $\ell_\infty$-norm attacks. In this work, we
                   propose a novel $\ell_0$-norm attack, called $\sigma$-zero,
                   which leverages a differentiable approximation of the
                   $\ell_0$ norm to facilitate gradient-based optimization, and
                   an adaptive projection operator to dynamically adjust the
                   trade-off between loss minimization and perturbation
                   sparsity. Extensive evaluations using MNIST, CIFAR10, and
                   ImageNet datasets, involving robust and non-robust models,
                   show that $\sigma$\texttt{-zero} finds minimum $\ell_0$-norm
                   adversarial examples without requiring any time-consuming
                   hyperparameter tuning, and that it outperforms all competing
                   sparse attacks in terms of success rate, perturbation size,
                   and efficiency.",
  month         =  "2~" # feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2402.01879"
}

@ARTICLE{Wu2025-gx,
  title     = "Broken time-reversal symmetry in visual motion detection",
  author    = "Wu, Nathan and Zhou, Baohua and Agrochao, Margarida and Clark,
               Damon A",
  journal   = "Proceedings of the National Academy of Sciences of the United
               States of America",
  publisher = "Proceedings of the National Academy of Sciences",
  volume    =  122,
  number    =  10,
  pages     = "e2410768122",
  abstract  = "Our intuition suggests that when a movie is played in reverse,
               our perception of motion at each location in the reversed movie
               will be perfectly inverted compared to the original. This
               intuition is also reflected in classical theoretical and
               practical models of motion estimation, in which velocity flow
               fields invert when inputs are reversed in time. However, here we
               report that this symmetry of motion perception upon time reversal
               is broken in real visual systems. We designed a set of visual
               stimuli to investigate time reversal symmetry breaking in the
               fruit fly Drosophila ’s well-studied optomotor rotation behavior.
               We identified a suite of stimuli with a wide variety of
               properties that can uncover broken time reversal symmetry in fly
               behavioral responses. We then trained neural network models to
               predict the velocity of scenes with both natural and artificial
               contrast distributions. Training with naturalistic contrast
               distributions yielded models that broke time reversal symmetry,
               even when the training data themselves were time reversal
               symmetric. We show analytically and numerically that the breaking
               of time reversal symmetry in the model responses can arise from
               contrast asymmetry in the training data, but can also arise from
               other features of the contrast distribution. Furthermore,
               shallower neural network models can exhibit stronger symmetry
               breaking than deeper ones, suggesting that less flexible neural
               networks may be more prone to time reversal symmetry breaking.
               Overall, these results reveal a surprising feature of biological
               motion detectors and suggest that it could arise from constrained
               optimization in natural environments.",
  month     =  "11~" # mar,
  year      =  2025,
  doi       = "10.1073/pnas.2410768122",
  issn      = "0027-8424,1091-6490",
  language  = "en"
}

@ARTICLE{Xiaoning2022-fo,
  title         = "Perceptual Multi-Exposure Fusion",
  author        = "Xiaoning, Liu",
  journal       = "arXiv [cs.CV]",
  abstract      = "As an ever-increasing demand for high dynamic range (HDR)
                   scene shooting, multi-exposure image fusion (MEF) technology
                   has abounded. In recent years, multi-scale exposure fusion
                   approaches based on detail-enhancement have led the way for
                   improvement in highlight and shadow details. Most of such
                   methods, however, are too computationally expensive to be
                   deployed on mobile devices. This paper presents a perceptual
                   multi-exposure fusion method that not just ensures fine
                   shadow/highlight details but with lower complexity than
                   detailenhanced methods. We analyze the potential defects of
                   three classical exposure measures in lieu of using
                   detail-enhancement component and improve two of them, namely
                   adaptive Wellexposedness (AWE) and the gradient of color
                   images (3-D gradient). AWE designed in YCbCr color space
                   considers the difference between varying exposure images. 3-D
                   gradient is employed to extract fine details. We build a
                   large-scale multiexposure benchmark dataset suitable for
                   static scenes, which contains 167 image sequences all told.
                   Experiments on the constructed dataset demonstrate that the
                   proposed method exceeds existing eight state-of-the-art
                   approaches in terms of visually and MEF-SSIM value. Moreover,
                   our approach can achieve a better improvement for current
                   image enhancement techniques, ensuring fine detail in bright
                   light.",
  month         =  "18~" # oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2210.09604"
}

@ARTICLE{Shunxin2025-qn,
  title         = "Do {ImageNet}-trained models learn shortcuts? The impact of
                   frequency shortcuts on generalization",
  author        = "Shunxin, Wang and Raymond, Veldhuis and Nicola, Strisciuglio",
  journal       = "arXiv [cs.CV]",
  abstract      = "Frequency shortcuts refer to specific frequency patterns that
                   models heavily rely on for correct classification. Previous
                   studies have shown that models trained on small image
                   datasets often exploit such shortcuts, potentially impairing
                   their generalization performance. However, existing methods
                   for identifying frequency shortcuts require expensive
                   computations and become impractical for analyzing models
                   trained on large datasets. In this work, we propose the first
                   approach to more efficiently analyze frequency shortcuts at a
                   larger scale. We show that both CNN and transformer models
                   learn frequency shortcuts on ImageNet. We also expose that
                   frequency shortcut solutions can yield good performance on
                   out-of-distribution (OOD) test sets which largely retain
                   texture information. However, these shortcuts, mostly aligned
                   with texture patterns, hinder model generalization on
                   rendition-based OOD test sets. These observations suggest
                   that current OOD evaluations often overlook the impact of
                   frequency shortcuts on model generalization. Future
                   benchmarks could thus benefit from explicitly assessing and
                   accounting for these shortcuts to build models that
                   generalize across a broader range of OOD scenarios.",
  month         =  "5~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.03519"
}

@ARTICLE{Masatoshi2025-dv,
  title         = "{WgPy}: {GPU}-accelerated {NumPy}-like array library for web
                   browsers",
  author        = "Masatoshi, Hidaka and Tatsuya, Harada",
  journal       = "arXiv [cs.DC]",
  abstract      = "To execute scientific computing programs such as deep
                   learning at high speed, GPU acceleration is a powerful
                   option. With the recent advancements in web technologies,
                   interfaces like WebGL and WebGPU, which utilize GPUs on the
                   client side of web applications, have become available. On
                   the other hand, Pyodide, a Python runtime that operates on
                   web browsers, allows web applications to be written in
                   Python, but it can only utilize the CPU, leaving room for
                   acceleration. Our proposed new library, WgPy, provides array
                   computation capabilities on the GPU with a NumPy-compatible
                   interface in the web browser. This library not only
                   implements array operations such as matrix multiplication on
                   WebGL and WebGPU, but also allows the users to write custom
                   kernels that can run on GPUs with minimal syntax knowledge,
                   allowing you to run a variety of algorithms with minimal
                   overhead. WgPy also implements a special thread
                   synchronization mechanism, which bridges asynchronous
                   semantics of JavaScript with Python's synchronous semantics,
                   allows code written for CuPy, the NumPy-compatible array
                   library for CUDA, to run directly in a web browser. In
                   experiments involving training a CNN model, it achieved
                   processing at 95 times the speed compared to CPU execution.",
  month         =  "1~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DC",
  eprint        = "2503.00279"
}

@INPROCEEDINGS{Mayo2023-qm,
  title     = "How hard are computer vision datasets? Calibrating dataset
               difficulty to viewing time",
  author    = "Mayo, David and Cummings, Jesse and Lin, Xinyu and Gutfreund, Dan
               and Katz, Boris and Barbu, Andrei",
  editor    = "Oh, A and Naumann, T and Globerson, A and Saenko, K and Hardt, M
               and Levine, S",
  booktitle = "Advances in Neural Information Processing Systems",
  publisher = "Curran Associates, Inc.",
  volume    =  36,
  pages     = "11008--11036",
  year      =  2023
}

@ARTICLE{Badr2025-tf,
  title         = "From language to cognition: How {LLMs} outgrow the human
                   language network",
  author        = "Badr, Alkhamissi and Greta, Tuckute and Yingtian, Tang and
                   Taha, Binhuraib and Antoine, Bosselut and Martin, Schrimpf",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LLMs) exhibit remarkable similarity to
                   neural activity in the human language network. However, the
                   key properties of language shaping brain-like
                   representations, and their evolution during training as a
                   function of different tasks remain unclear. We here benchmark
                   34 training checkpoints spanning 300B tokens across 8
                   different model sizes to analyze how brain alignment relates
                   to linguistic competence. Specifically, we find that brain
                   alignment tracks the development of formal linguistic
                   competence -- i.e., knowledge of linguistic rules -- more
                   closely than functional linguistic competence. While
                   functional competence, which involves world knowledge and
                   reasoning, continues to develop throughout training, its
                   relationship with brain alignment is weaker, suggesting that
                   the human language network primarily encodes formal
                   linguistic structure rather than broader cognitive functions.
                   We further show that model size is not a reliable predictor
                   of brain alignment when controlling for feature size and find
                   that the correlation between next-word prediction, behavioral
                   alignment and brain alignment fades once models surpass human
                   language proficiency. Finally, using the largest set of
                   rigorous neural language benchmarks to date, we show that
                   language brain alignment benchmarks remain unsaturated,
                   highlighting opportunities for improving future models. Taken
                   together, our findings suggest that the human language
                   network is best modeled by formal, rather than functional,
                   aspects of language.",
  month         =  "3~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2503.01830"
}

@ARTICLE{Yang2023-by,
  title     = "Psychophysical measurement of perceived motion flow of
               naturalistic scenes",
  author    = "Yang, Yung-Hao and Fukiage, Taiki and Sun, Zitang and Nishida,
               Shin'ya",
  journal   = "iScience",
  publisher = "Elsevier BV",
  volume    =  26,
  number    =  12,
  pages     =  108307,
  abstract  = "The neural and computational mechanisms underlying visual motion
               perception have been extensively investigated over several
               decades, but little attempt has been made to measure and analyze,
               how human observers perceive the map of motion vectors, or
               optical flow, in complex naturalistic scenes. Here, we developed
               a psychophysical method to assess human-perceived motion flows
               using local vector matching and a flash probe. The estimated
               perceived flow for naturalistic movies agreed with the physically
               correct flow (ground truth) at many points, but also showed
               consistent deviations from the ground truth (flow illusions) at
               other points. Comparisons with the predictions of various
               computational models, including cutting-edge computer vision
               algorithms and coordinate transformation models, indicated that
               some flow illusions are attributable to lower-level factors such
               as spatiotemporal pooling and signal loss, while others reflect
               higher-level computations, including vector decomposition. Our
               study demonstrates a promising data-driven psychophysical
               paradigm for an advanced understanding of visual motion
               perception.",
  month     =  "15~" # dec,
  year      =  2023,
  keywords  = "Applied computing; Biological sciences; Computer science;
               Neuroscience;MyPapers",
  doi       = "10.1016/j.isci.2023.108307",
  pmc       = "PMC10679809",
  pmid      =  38025782,
  issn      = "2589-0042",
  language  = "en"
}

@ARTICLE{He2025-er,
  title     = "The Boynton Illusion: Chromatic edge attraction to a luminance
               contour",
  author    = "He, Jingyi and Taveras-Cruz, Yesenia and Eskew, Rhea T",
  journal   = "Journal of vision",
  publisher = "The Association for Research in Vision and Ophthalmology",
  volume    =  25,
  number    =  3,
  pages     = "3--3",
  month     =  "3~" # mar,
  year      =  2025,
  doi       = "10.1167/jov.25.3.3",
  issn      = "1534-7362"
}

@ARTICLE{Rucci2025-wc,
  title     = "The visual system does not operate like a camera",
  author    = "Rucci, Michele and Ahissar, Ehud and Burr, David C and Kagan,
               Igor and Poletti, Martina and Victor, Jonathan D",
  journal   = "Journal of vision",
  publisher = "Association for Research in Vision and Ophthalmology (ARVO)",
  volume    =  25,
  number    =  3,
  pages     =  2,
  month     =  "4~" # mar,
  year      =  2025,
  doi       = "10.1167/jov.25.3.2",
  issn      = "1534-7362",
  language  = "en"
}

@ARTICLE{Zheng2025-jy,
  title         = "Direct Discriminative Optimization: Your likelihood-based
                   visual generative model is secretly a {GAN} discriminator",
  author        = "Zheng, Kaiwen and Chen, Yongxin and Chen, Huayu and He,
                   Guande and Liu, Ming-Yu and Zhu, Jun and Zhang, Qinsheng",
  journal       = "arXiv [cs.CV]",
  abstract      = "While likelihood-based generative models, particularly
                   diffusion and autoregressive models, have achieved remarkable
                   fidelity in visual generation, the maximum likelihood
                   estimation (MLE) objective inherently suffers from a
                   mode-covering tendency that limits the generation quality
                   under limited model capacity. In this work, we propose Direct
                   Discriminative Optimization (DDO) as a unified framework that
                   bridges likelihood-based generative training and the GAN
                   objective to bypass this fundamental constraint. Our key
                   insight is to parameterize a discriminator implicitly using
                   the likelihood ratio between a learnable target model and a
                   fixed reference model, drawing parallels with the philosophy
                   of Direct Preference Optimization (DPO). Unlike GANs, this
                   parameterization eliminates the need for joint training of
                   generator and discriminator networks, allowing for direct,
                   efficient, and effective finetuning of a well-trained model
                   to its full potential beyond the limits of MLE. DDO can be
                   performed iteratively in a self-play manner for progressive
                   model refinement, with each round requiring less than 1\% of
                   pretraining epochs. Our experiments demonstrate the
                   effectiveness of DDO by significantly advancing the previous
                   SOTA diffusion model EDM, reducing FID scores from 1.79/1.58
                   to new records of 1.30/0.97 on CIFAR-10/ImageNet-64 datasets,
                   and by consistently improving both guidance-free and
                   CFG-enhanced FIDs of visual autoregressive models on ImageNet
                   256$\times$256.",
  month         =  "3~" # mar,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.01103",
  keywords      = "\_To\_read;Project/Density\_IQA"
}

@ARTICLE{Bromley2025-je,
  title         = "An Analysis of Segment Anything 2",
  author        = "Bromley, Clayton and Moore, Alexander and Saini, Amar and
                   Poland, Doug and Carrano, Carmen",
  journal       = "arXiv [cs.CV]",
  abstract      = "Video object segmentation (VOS) is a critical task in the
                   development of video perception and understanding. The
                   Segment-Anything Model 2 (SAM 2), released by Meta AI, is the
                   current state-of-the-art architecture for end-to-end VOS. SAM
                   2 performs very well on both clean video data and augmented
                   data, and completely intelligent video perception requires an
                   understanding of how this architecture is capable of
                   achieving such quality results. To better understand how each
                   step within the SAM 2 architecture permits high-quality video
                   segmentation, we pass a variety of complex video
                   transformations through the architecture and measure the
                   impact at each stage of the process. We observe that each
                   progressive stage enables the filtering of complex
                   transformation noise and the emphasis of the object of
                   interest. Our contributions include the creation of complex
                   transformation video datasets, an analysis of how each stage
                   of the SAM 2 architecture interprets these transformations,
                   and visualizations of segmented objects through each stage.
                   By better understanding how each model structure impacts
                   overall video understanding, VOS development can work to
                   improve real-world applicability and performance tracking,
                   localizing, and segmenting objects despite complex cluttered
                   scenes and obscurations.",
  month         =  "25~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.00042"
}

@ARTICLE{Del_Rosario2025-uh,
  title     = "Lateral inhibition in {V1} controls neural and perceptual
               contrast sensitivity",
  author    = "Del Rosario, Joseph and Coletta, Stefano and Kim, Soon Ho and
               Mobille, Zach and Peelman, Kayla and Williams, Brice and Otsuki,
               Alan J and Del Castillo Valerio, Alejandra and Worden, Kendell
               and Blanpain, Lou T and Lovell, Lyndah and Choi, Hannah and
               Haider, Bilal",
  journal   = "Nature neuroscience",
  publisher = "Springer Science and Business Media LLC",
  pages     = "1--12",
  abstract  = "Lateral inhibition is a central principle in sensory system
               function. It is thought to operate by the activation of
               inhibitory neurons that restrict the spatial spread of sensory
               excitation. However, the neurons, computations and mechanisms
               underlying cortical lateral inhibition remain debated, and its
               importance for perception remains unknown. Here we show that
               lateral inhibition from parvalbumin neurons in mouse primary
               visual cortex reduced neural and perceptual sensitivity to visual
               contrast in a uniform subtractive manner, whereas lateral
               inhibition from somatostatin neurons more effectively changed the
               slope (or gain) of neural and perceptual contrast sensitivity. A
               neural circuit model, anatomical tracing and direct subthreshold
               measurements indicated that the larger spatial footprint for
               somatostatin versus parvalbumin synaptic inhibition explains this
               difference. Together, these results define cell-type-specific
               computational roles for lateral inhibition in primary visual
               cortex, and establish their unique consequences on sensitivity to
               contrast, a fundamental aspect of the visual world. The role of
               lateral inhibition for perception and neural computation remains
               unsolved. Del Rosario et al. show that distinct types of cortical
               interneurons in V1 drive lateral inhibition that causes
               subtraction or division of visual sensitivity.",
  month     =  "3~" # mar,
  year      =  2025,
  doi       = "10.1038/s41593-025-01888-4",
  issn      = "1097-6256,1546-1726",
  language  = "en"
}

@ARTICLE{Schwetlick2025-vk,
  title     = "Does surface completion fail to support uncrowding?",
  author    = "Schwetlick, Lisa and Manassi, Mauro and Herzog, Michael H and
               Francis, Gregory",
  journal   = "Journal of vision",
  publisher = "Association for Research in Vision and Ophthalmology (ARVO)",
  volume    =  25,
  number    =  3,
  pages     =  1,
  month     =  "3~" # mar,
  year      =  2025,
  keywords  = "crowding; cues; perception; pixel; visual fields",
  doi       = "10.1167/jov.25.3.1",
  issn      = "1534-7362",
  language  = "en"
}

@ARTICLE{Degeorge2025-iy,
  title         = "How far can we go with {ImageNet} for Text-to-Image
                   generation?",
  author        = "Degeorge, L and Ghosh, A and Dufour, N and Picard, D and
                   Kalogeiton, V",
  journal       = "arXiv [cs.CV]",
  abstract      = "Recent text-to-image (T2I) generation models have achieved
                   remarkable results by training on billion-scale datasets,
                   following a `bigger is better' paradigm that prioritizes data
                   quantity over quality. We challenge this established paradigm
                   by demonstrating that strategic data augmentation of small,
                   well-curated datasets can match or outperform models trained
                   on massive web-scraped collections. Using only ImageNet
                   enhanced with well-designed text and image augmentations, we
                   achieve a +2 overall score over SD-XL on GenEval and +5 on
                   DPGBench while using just 1/10th the parameters and 1/1000th
                   the training images. Our results suggest that strategic data
                   augmentation, rather than massive datasets, could offer a
                   more sustainable path forward for T2I generation.",
  month         =  "28~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.21318"
}

@ARTICLE{Jingtao2025-ab,
  title         = "Intelligence Test",
  author        = "Jingtao, Zhan and Jiahao, Zhao and Jiayu, Li and Yiqun, Liu
                   and Bo, Zhang and Qingyao, Ai and Jiaxin, Mao and Hongning,
                   Wang and Min, Zhang and Shaoping, Ma",
  journal       = "arXiv [cs.AI]",
  abstract      = "How does intelligence emerge? We propose that intelligence is
                   not a sudden gift or random occurrence, but rather a
                   necessary trait for species to survive through Natural
                   Selection. If a species passes the test of Natural Selection,
                   it demonstrates the intelligence to survive in nature.
                   Extending this perspective, we introduce Intelligence Test, a
                   method to quantify the intelligence of any subject on any
                   task. Like how species evolve by trial and error,
                   Intelligence Test quantifies intelligence by the number of
                   failed attempts before success. Fewer failures correspond to
                   higher intelligence. When the expectation and variance of
                   failure counts are both finite, it signals the achievement of
                   an autonomous level of intelligence. Using Intelligence Test,
                   we comprehensively evaluate existing AI systems. Our results
                   show that while AI systems achieve a level of autonomy in
                   simple tasks, they are still far from autonomous in more
                   complex tasks, such as vision, search, recommendation, and
                   language. While scaling model size might help, this would
                   come at an astronomical cost. Projections suggest that
                   achieving general autonomy would require unimaginable
                   $10^{26}$ parameters. Even if Moore's Law continuously holds,
                   such a parameter scale would take $70$ years. This staggering
                   cost highlights the complexity of human tasks and the
                   inadequacies of current AI. To further understand this
                   phenomenon, we conduct a theoretical analysis. Our
                   simulations suggest that human tasks possess a criticality
                   property. As a result, autonomy requires a deep understanding
                   of the task's underlying mechanisms. Current AI, however,
                   does not fully grasp these mechanisms and instead relies on
                   superficial mimicry, making it difficult to reach an
                   autonomous level. We believe Intelligence Test can not only
                   guide the future development of AI but also offer profound
                   insights into the intelligence of humans ourselves.",
  month         =  "26~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2502.18858"
}

@ARTICLE{Da_Silva2025-tq,
  title         = "Back to the Future Cyclopean Stereo: a human perception
                   approach unifying deep and geometric constraints",
  author        = "da Silva, Sherlon Almeida and Davi, Geiger and Luiz, Velho
                   and Ponti, Moacir Antonelli",
  journal       = "arXiv [cs.CV]",
  abstract      = "We innovate in stereo vision by explicitly providing
                   analytical 3D surface models as viewed by a cyclopean eye
                   model that incorporate depth discontinuities and occlusions.
                   This geometrical foundation combined with learned stereo
                   features allows our system to benefit from the strengths of
                   both approaches. We also invoke a prior monocular model of
                   surfaces to fill in occlusion regions or texture-less regions
                   where data matching is not sufficient. Our results already
                   are on par with the state-of-the-art purely data-driven
                   methods and are of much better visual quality, emphasizing
                   the importance of the 3D geometrical model to capture
                   critical visual information. Such qualitative improvements
                   may find applicability in virtual reality, for a better human
                   experience, as well as in robotics, for reducing critical
                   errors. Our approach aims to demonstrate that understanding
                   and modeling geometrical properties of 3D surfaces is
                   beneficial to computer vision research.",
  month         =  "28~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.21280"
}

@ARTICLE{Zihan2025-wl,
  title         = "Towards high-performance Spiking Transformers from {ANN} to
                   {SNN} conversion",
  author        = "Zihan, Huang and Xinyu, Shi and Zecheng, Hao and Tong, Bu and
                   Jianhao, Ding and Zhaofei, Yu and Tiejun, Huang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Spiking neural networks (SNNs) show great potential due to
                   their energy efficiency, fast processing capabilities, and
                   robustness. There are two main approaches to constructing
                   SNNs. Direct training methods require much memory, while
                   conversion methods offer a simpler and more efficient option.
                   However, current conversion methods mainly focus on
                   converting convolutional neural networks (CNNs) to SNNs.
                   Converting Transformers to SNN is challenging because of the
                   presence of non-linear modules. In this paper, we propose an
                   Expectation Compensation Module to preserve the accuracy of
                   the conversion. The core idea is to use information from the
                   previous T time-steps to calculate the expected output at
                   time-step T. We also propose a Multi-Threshold Neuron and the
                   corresponding Parallel Parameter normalization to address the
                   challenge of large time steps needed for high accuracy,
                   aiming to reduce network latency and power consumption. Our
                   experimental results demonstrate that our approach achieves
                   state-of-the-art performance. For example, we achieve a top-1
                   accuracy of 88.60\% with only a 1\% loss in accuracy using 4
                   time steps while consuming only 35\% of the original power of
                   the Transformer. To our knowledge, this is the first
                   successful Artificial Neural Network (ANN) to SNN conversion
                   for Spiking Transformers that achieves high accuracy, low
                   latency, and low power consumption on complex datasets. The
                   source codes of the proposed method are available at
                   https://github.com/h-z-h-cell/Transformer-to-SNN-ECMT.",
  month         =  "28~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.21193"
}

@ARTICLE{Christopher2025-zx,
  title         = "Spatial reasoning with denoising models",
  author        = "Christopher, Wewer and Bart, Pogodzinski and Bernt, Schiele
                   and Lenssen, Jan Eric",
  journal       = "arXiv [cs.CV]",
  abstract      = "We introduce Spatial Reasoning Models (SRMs), a framework to
                   perform reasoning over sets of continuous variables via
                   denoising generative models. SRMs infer continuous
                   representations on a set of unobserved variables, given
                   observations on observed variables. Current generative models
                   on spatial domains, such as diffusion and flow matching
                   models, often collapse to hallucination in case of complex
                   distributions. To measure this, we introduce a set of
                   benchmark tasks that test the quality of complex reasoning in
                   generative models and can quantify hallucination. The SRM
                   framework allows to report key findings about importance of
                   sequentialization in generation, the associated order, as
                   well as the sampling strategies during training. It
                   demonstrates, for the first time, that order of generation
                   can successfully be predicted by the denoising network
                   itself. Using these findings, we can increase the accuracy of
                   specific reasoning tasks from 50\%.",
  month         =  "28~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.21075"
}

@ARTICLE{Dacheng2025-lz,
  title         = "{WorldModelBench}: Judging video generation models as world
                   models",
  author        = "Dacheng, Li and Yunhao, Fang and Yukang, Chen and Shuo, Yang
                   and Shiyi, Cao and Justin, Wong and Michael, Luo and
                   Xiaolong, Wang and Hongxu, Yin and Joseph, E Gonzalez and
                   Ion, Stoica and Song, Han and Yao, Lu",
  journal       = "arXiv [cs.CV]",
  abstract      = "Video generation models have rapidly progressed, positioning
                   themselves as video world models capable of supporting
                   decision-making applications like robotics and autonomous
                   driving. However, current benchmarks fail to rigorously
                   evaluate these claims, focusing only on general video
                   quality, ignoring important factors to world models such as
                   physics adherence. To bridge this gap, we propose
                   WorldModelBench, a benchmark designed to evaluate the world
                   modeling capabilities of video generation models in
                   application-driven domains. WorldModelBench offers two key
                   advantages: (1) Against to nuanced world modeling violations:
                   By incorporating instruction-following and physics-adherence
                   dimensions, WorldModelBench detects subtle violations, such
                   as irregular changes in object size that breach the mass
                   conservation law - issues overlooked by prior benchmarks. (2)
                   Aligned with large-scale human preferences: We crowd-source
                   67K human labels to accurately measure 14 frontier models.
                   Using our high-quality human labels, we further fine-tune an
                   accurate judger to automate the evaluation procedure,
                   achieving 8.6\% higher average accuracy in predicting world
                   modeling violations than GPT-4o with 2B parameters. In
                   addition, we demonstrate that training to align human
                   annotations by maximizing the rewards from the judger
                   noticeably improve the world modeling capability. The website
                   is available at https://worldmodelbench-team.github.io.",
  month         =  "28~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.20694"
}

@ARTICLE{Li-Wei2025-tv,
  title         = "On the role of individual differences in current approaches
                   to computational image aesthetics",
  author        = "Li-Wei, Chen and Ombretta, Strafforello and Anne-Sofie,
                   Maerten and Tinne, Tuytelaars and Johan, Wagemans",
  journal       = "arXiv [cs.CV]",
  abstract      = "Image aesthetic assessment (IAA) evaluates image aesthetics,
                   a task complicated by image diversity and user subjectivity.
                   Current approaches address this in two stages: Generic IAA
                   (GIAA) models estimate mean aesthetic scores, while Personal
                   IAA (PIAA) models adapt GIAA using transfer learning to
                   incorporate user subjectivity. However, a theoretical
                   understanding of transfer learning between GIAA and PIAA,
                   particularly concerning the impact of group composition,
                   group size, aesthetic differences between groups and
                   individuals, and demographic correlations, is lacking. This
                   work establishes a theoretical foundation for IAA, proposing
                   a unified model that encodes individual characteristics in a
                   distributional format for both individual and group
                   assessments. We show that transferring from GIAA to PIAA
                   involves extrapolation, while the reverse involves
                   interpolation, which is generally more effective for machine
                   learning. Experiments with varying group compositions,
                   including sub-sampling by group size and disjoint
                   demographics, reveal significant performance variation even
                   for GIAA, indicating that mean scores do not fully eliminate
                   individual subjectivity. Performance variations and Gini
                   index analysis reveal education as the primary factor
                   influencing aesthetic differences, followed by photography
                   and art experience, with stronger individual subjectivity
                   observed in artworks than in photos. Our model uniquely
                   supports both GIAA and PIAA, enhancing generalization across
                   demographics.",
  month         =  "27~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.20518"
}

@ARTICLE{Szegedy2013-rj,
  title         = "Intriguing properties of neural networks",
  author        = "Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya
                   and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and
                   Fergus, Rob",
  journal       = "arXiv [cs.CV]",
  abstract      = "Deep neural networks are highly expressive models that have
                   recently achieved state of the art performance on speech and
                   visual recognition tasks. While their expressiveness is the
                   reason they succeed, it also causes them to learn
                   uninterpretable solutions that could have counter-intuitive
                   properties. In this paper we report two such properties.
                   First, we find that there is no distinction between
                   individual high level units and random linear combinations of
                   high level units, according to various methods of unit
                   analysis. It suggests that it is the space, rather than the
                   individual units, that contains of the semantic information
                   in the high layers of neural networks. Second, we find that
                   deep neural networks learn input-output mappings that are
                   fairly discontinuous to a significant extend. We can cause
                   the network to misclassify an image by applying a certain
                   imperceptible perturbation, which is found by maximizing the
                   network's prediction error. In addition, the specific nature
                   of these perturbations is not a random artifact of learning:
                   the same perturbation can cause a different network, that was
                   trained on a different subset of the dataset, to misclassify
                   the same input.",
  month         =  "20~" # dec,
  year          =  2013,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1312.6199"
}

@ARTICLE{Jacob2021-wr,
  title     = "Qualitative similarities and differences in visual object
               representations between brains and deep networks",
  author    = "Jacob, Georgin and Pramod, R T and Katti, Harish and Arun, S P",
  journal   = "Nature communications",
  publisher = "Springer Science and Business Media LLC",
  volume    =  12,
  number    =  1,
  pages     =  1872,
  abstract  = "Deep neural networks have revolutionized computer vision, and
               their object representations across layers match coarsely with
               visual cortical areas in the brain. However, whether these
               representations exhibit qualitative patterns seen in human
               perception or brain representations remains unresolved. Here, we
               recast well-known perceptual and neural phenomena in terms of
               distance comparisons, and ask whether they are present in
               feedforward deep neural networks trained for object recognition.
               Some phenomena were present in randomly initialized networks,
               such as the global advantage effect, sparseness, and relative
               size. Many others were present after object recognition training,
               such as the Thatcher effect, mirror confusion, Weber's law,
               relative size, multiple object normalization and correlated
               sparseness. Yet other phenomena were absent in trained networks,
               such as 3D shape processing, surface invariance, occlusion,
               natural parts and the global advantage. These findings indicate
               sufficient conditions for the emergence of these phenomena in
               brains and deep networks, and offer clues to the properties that
               could be incorporated to improve deep networks.",
  month     =  "25~" # mar,
  year      =  2021,
  keywords  = "\_To\_read;Project/Density\_IQA",
  doi       = "10.1038/s41467-021-22078-3",
  pmc       = "PMC7994307",
  pmid      =  33767141,
  issn      = "2041-1723",
  language  = "en"
}

@ARTICLE{Qingsen2025-ee,
  title         = "{HVI}: A New color space for Low-light Image Enhancement",
  author        = "Qingsen, Yan and Yixu, Feng and Cheng, Zhang and Guansong,
                   Pang and Kangbiao, Shi and Peng, Wu and Wei, Dong and Jinqiu,
                   Sun and Yanning, Zhang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Low-Light Image Enhancement (LLIE) is a crucial computer
                   vision task that aims to restore detailed visual information
                   from corrupted low-light images. Many existing LLIE methods
                   are based on standard RGB (sRGB) space, which often produce
                   color bias and brightness artifacts due to inherent high
                   color sensitivity in sRGB. While converting the images using
                   Hue, Saturation and Value (HSV) color space helps resolve the
                   brightness issue, it introduces significant red and black
                   noise artifacts. To address this issue, we propose a new
                   color space for LLIE, namely Horizontal/Vertical-Intensity
                   (HVI), defined by polarized HS maps and learnable intensity.
                   The former enforces small distances for red coordinates to
                   remove the red artifacts, while the latter compresses the
                   low-light regions to remove the black artifacts. To fully
                   leverage the chromatic and intensity information, a novel
                   Color and Intensity Decoupling Network (CIDNet) is further
                   introduced to learn accurate photometric mapping function
                   under different lighting conditions in the HVI space.
                   Comprehensive results from benchmark and ablation experiments
                   show that the proposed HVI color space with CIDNet
                   outperforms the state-of-the-art methods on 10 datasets. The
                   code is available at https://github.com/Fediory/HVI-CIDNet.",
  month         =  "27~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.20272"
}

@ARTICLE{Yancheng2025-us,
  title         = "Do computer vision foundation models learn the low-level
                   characteristics of the human visual system?",
  author        = "Yancheng, Cai and Fei, Yin and Dounia, Hammou and Rafal,
                   Mantiuk",
  journal       = "arXiv [cs.CV]",
  abstract      = "Computer vision foundation models, such as DINO or OpenCLIP,
                   are trained in a self-supervised manner on large image
                   datasets. Analogously, substantial evidence suggests that the
                   human visual system (HVS) is influenced by the statistical
                   distribution of colors and patterns in the natural world,
                   characteristics also present in the training data of
                   foundation models. The question we address in this paper is
                   whether foundation models trained on natural images mimic
                   some of the low-level characteristics of the human visual
                   system, such as contrast detection, contrast masking, and
                   contrast constancy. Specifically, we designed a protocol
                   comprising nine test types to evaluate the image encoders of
                   45 foundation and generative models. Our results indicate
                   that some foundation models (e.g., DINO, DINOv2, and
                   OpenCLIP), share some of the characteristics of human vision,
                   but other models show little resemblance. Foundation models
                   tend to show smaller sensitivity to low contrast and rather
                   irregular responses to contrast across frequencies. The
                   foundation models show the best agreement with human data in
                   terms of contrast masking. Our findings suggest that human
                   vision and computer vision may take both similar and
                   different paths when learning to interpret images of the real
                   world. Overall, while differences remain, foundation models
                   trained on vision tasks start to align with low-level human
                   vision, with DINOv2 showing the closest resemblance.",
  month         =  "27~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.20256",
  keywords      = "\_To\_read"
}

@ARTICLE{Miaomiao2025-pr,
  title         = "Autoregressive Image Generation guided by chains of Thought",
  author        = "Miaomiao, Cai and Guanjie, Wang and Wei, Li and Zhijun, Tu
                   and Hanting, Chen and Shaohui, Lin and Jie, Hu",
  journal       = "arXiv [cs.CV]",
  abstract      = "In the field of autoregressive (AR) image generation, models
                   based on the 'next-token prediction' paradigm of LLMs have
                   shown comparable performance to diffusion models by reducing
                   inductive biases. However, directly applying LLMs to complex
                   image generation can struggle with reconstructing the
                   structure and details of the image, impacting the accuracy
                   and stability of generation. Additionally, the 'next-token
                   prediction' paradigm in the AR model does not align with the
                   contextual scanning and logical reasoning processes involved
                   in human visual perception, limiting effective image
                   generation. Chain-of-Thought (CoT), as a key reasoning
                   capability of LLMs, utilizes reasoning prompts to guide the
                   model, improving reasoning performance on complex natural
                   language process (NLP) tasks, enhancing accuracy and
                   stability of generation, and helping the model maintain
                   contextual coherence and logical consistency, similar to
                   human reasoning. Inspired by CoT from the field of NLP, we
                   propose autoregressive Image Generation with Thoughtful
                   Reasoning (IGTR) to enhance autoregressive image generation.
                   IGTR adds reasoning prompts without modifying the model
                   structure or raster generation order. Specifically, we design
                   specialized image-related reasoning prompts for AR image
                   generation to simulate the human reasoning process, which
                   enhances contextual reasoning by allowing the model to first
                   perceive overall distribution information before generating
                   the image, and improve generation stability by increasing the
                   inference steps. Compared to the AR method without prompts,
                   our method shows outstanding performance and achieves an
                   approximate improvement of 20\%.",
  month         =  "24~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.16965"
}

@ARTICLE{Xiankang2025-zz,
  title         = "Distill any depth: Distillation creates a stronger monocular
                   depth estimator",
  author        = "Xiankang, He and Dongyan, Guo and Hongji, Li and Ruibo, Li
                   and Ying, Cui and Chi, Zhang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Monocular depth estimation (MDE) aims to predict scene depth
                   from a single RGB image and plays a crucial role in 3D scene
                   understanding. Recent advances in zero-shot MDE leverage
                   normalized depth representations and distillation-based
                   learning to improve generalization across diverse scenes.
                   However, current depth normalization methods for
                   distillation, relying on global normalization, can amplify
                   noisy pseudo-labels, reducing distillation effectiveness. In
                   this paper, we systematically analyze the impact of different
                   depth normalization strategies on pseudo-label distillation.
                   Based on our findings, we propose Cross-Context Distillation,
                   which integrates global and local depth cues to enhance
                   pseudo-label quality. Additionally, we introduce a
                   multi-teacher distillation framework that leverages
                   complementary strengths of different depth estimation models,
                   leading to more robust and accurate depth predictions.
                   Extensive experiments on benchmark datasets demonstrate that
                   our approach significantly outperforms state-of-the-art
                   methods, both quantitatively and qualitatively.",
  month         =  "26~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.19204"
}

@ARTICLE{Mennatullah2025-rg,
  title         = "{PixFoundation}: Are we heading in the right direction with
                   pixel-level vision foundation models?",
  author        = "Mennatullah, Siam",
  journal       = "arXiv [cs.CV]",
  abstract      = "Multiple works have emerged to push the boundaries on
                   multi-modal large language models (MLLMs) towards pixel-level
                   understanding. Such approaches have shown strong performance
                   on benchmarks for referring expression segmentation and
                   grounded conversation generation. The current trend in
                   pixel-level MLLMs is to train with pixel-level grounding
                   supervision on large-scale labelled data. However, we show
                   that such MLLMs when evaluated on recent challenging vision
                   centric benchmarks, exhibit a weak ability in visual question
                   answering. Surprisingly, some of these methods even downgrade
                   the grounding ability of MLLMs that were never trained with
                   such supervision. In this work, we propose two novel
                   challenging benchmarks and show that MLLMs without
                   pixel-level grounding supervision can outperform the state of
                   the art in such tasks when evaluating both the pixel-level
                   grounding and visual question answering. We propose simple
                   baselines to extract the grounding information that can be
                   plugged into any MLLM, which we call as PixFoundation. More
                   importantly, we study the research question of ``When does
                   grounding emerge in MLLMs that are not trained with
                   pixel-level grounding supervision?'' We show that grounding
                   can coincide with object parts or location/appearance
                   information. Code repository is at
                   https://github.com/MSiam/PixFoundation/.",
  month         =  "6~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.04192"
}

@ARTICLE{Chen2023-zg,
  title         = "Language-guided diffusion model for visual grounding",
  author        = "Chen, Sijia and Li, Baochun",
  journal       = "arXiv [cs.CV]",
  abstract      = "Visual grounding (VG) tasks involve explicit cross-modal
                   alignment, as semantically corresponding image regions are to
                   be located for the language phrases provided. Existing
                   approaches complete such visual-text reasoning in a
                   single-step manner. Their performance causes high demands on
                   large-scale anchors and over-designed multi-modal fusion
                   modules based on human priors, leading to complicated
                   frameworks that may be difficult to train and overfit to
                   specific scenarios. Even worse, such once-for-all reasoning
                   mechanisms are incapable of refining boxes continuously to
                   enhance query-region matching. In contrast, in this paper, we
                   formulate an iterative reasoning process by denoising
                   diffusion modeling. Specifically, we propose a
                   language-guided diffusion framework for visual grounding,
                   LG-DVG, which trains the model to progressively reason
                   queried object boxes by denoising a set of noisy boxes with
                   the language guide. To achieve this, LG-DVG gradually
                   perturbs query-aligned ground truth boxes to noisy ones and
                   reverses this process step by step, conditional on query
                   semantics. Extensive experiments for our proposed framework
                   on five widely used datasets validate the superior
                   performance of solving visual grounding, a cross-modal
                   alignment task, in a generative way. The source codes are
                   available at
                   https://github.com/iQua/vgbase/tree/main/examples/DiffusionVG.",
  month         =  "18~" # aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2308.09599"
}

@ARTICLE{Mingkun2025-to,
  title         = "{CLIPure}: Purification in latent space via {CLIP} for
                   adversarially robust zero-shot classification",
  author        = "Mingkun, Zhang and Keping, Bi and Wei, Chen and Jiafeng, Guo
                   and Xueqi, Cheng",
  journal       = "arXiv [cs.CV]",
  abstract      = "In this paper, we aim to build an adversarially robust
                   zero-shot image classifier. We ground our work on CLIP, a
                   vision-language pre-trained encoder model that can perform
                   zero-shot classification by matching an image with text
                   prompts ``a photo of a .''. Purification is the path we
                   choose since it does not require adversarial training on
                   specific attack types and thus can cope with any foreseen
                   attacks. We then formulate purification risk as the KL
                   divergence between the joint distributions of the
                   purification process of denoising the adversarial samples and
                   the attack process of adding perturbations to benign samples,
                   through bidirectional Stochastic Differential Equations
                   (SDEs). The final derived results inspire us to explore
                   purification in the multi-modal latent space of CLIP. We
                   propose two variants for our CLIPure approach: CLIPure-Diff
                   which models the likelihood of images' latent vectors with
                   the DiffusionPrior module in DaLLE-2 (modeling the generation
                   process of CLIP's latent vectors), and CLIPure-Cos which
                   models the likelihood with the cosine similarity between the
                   embeddings of an image and ``a photo of a.''. As far as we
                   know, CLIPure is the first purification method in multi-modal
                   latent space and CLIPure-Cos is the first purification method
                   that is not based on generative models, which substantially
                   improves defense efficiency. We conducted extensive
                   experiments on CIFAR-10, ImageNet, and 13 datasets that
                   previous CLIP-based defense methods used for evaluating
                   zero-shot classification robustness. Results show that
                   CLIPure boosts the SOTA robustness by a large margin, e.g.,
                   from 71.7\% to 91.1\% on CIFAR10, from 59.6\% to 72.6\% on
                   ImageNet, and 108\% relative improvements of average
                   robustness on the 13 datasets over previous SOTA. The code is
                   available at https://github.com/TMLResearchGroup-CAS/CLIPure.",
  month         =  "25~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.18176",
  keywords      = "\_To\_read"
}

@ARTICLE{Mingyuan2025-ro,
  title         = "Optimal Brain Apoptosis",
  author        = "Mingyuan, Sun and Zheng, Fang and Jiaxu, Wang and Junjie,
                   Jiang and Delei, Kong and Chenming, Hu and Yuetong, Fang and
                   Renjing, Xu",
  journal       = "arXiv [cs.CV]",
  abstract      = "The increasing complexity and parameter count of
                   Convolutional Neural Networks (CNNs) and Transformers pose
                   challenges in terms of computational efficiency and resource
                   demands. Pruning has been identified as an effective strategy
                   to address these challenges by removing redundant elements
                   such as neurons, channels, or connections, thereby enhancing
                   computational efficiency without heavily compromising
                   performance. This paper builds on the foundational work of
                   Optimal Brain Damage (OBD) by advancing the methodology of
                   parameter importance estimation using the Hessian matrix.
                   Unlike previous approaches that rely on approximations, we
                   introduce Optimal Brain Apoptosis (OBA), a novel pruning
                   method that calculates the Hessian-vector product value
                   directly for each parameter. By decomposing the Hessian
                   matrix across network layers and identifying conditions under
                   which inter-layer Hessian submatrices are non-zero, we
                   propose a highly efficient technique for computing the
                   second-order Taylor expansion of parameters. This approach
                   allows for a more precise pruning process, particularly in
                   the context of CNNs and Transformers, as validated in our
                   experiments including VGG19, ResNet32, ResNet50, and ViT-B/16
                   on CIFAR10, CIFAR100 and Imagenet datasets. Our code is
                   available at https://github.com/NEU-REAL/OBA.",
  month         =  "25~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.17941"
}

@ARTICLE{Tianhong2025-jk,
  title         = "Fractal Generative Models",
  author        = "Tianhong, Li and Qinyi, Sun and Lijie, Fan and Kaiming, He",
  journal       = "arXiv [cs.LG]",
  abstract      = "Modularization is a cornerstone of computer science,
                   abstracting complex functions into atomic building blocks. In
                   this paper, we introduce a new level of modularization by
                   abstracting generative models into atomic generative modules.
                   Analogous to fractals in mathematics, our method constructs a
                   new type of generative model by recursively invoking atomic
                   generative modules, resulting in self-similar fractal
                   architectures that we call fractal generative models. As a
                   running example, we instantiate our fractal framework using
                   autoregressive models as the atomic generative modules and
                   examine it on the challenging task of pixel-by-pixel image
                   generation, demonstrating strong performance in both
                   likelihood estimation and generation quality. We hope this
                   work could open a new paradigm in generative modeling and
                   provide a fertile ground for future research. Code is
                   available at https://github.com/LTH14/fractalgen.",
  month         =  "24~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2502.17437"
}

@ARTICLE{Yichi2025-el,
  title         = "Towards hierarchical rectified flow",
  author        = "Yichi, Zhang and Yici, Yan and Alex, Schwing and Zhizhen,
                   Zhao",
  journal       = "arXiv [cs.LG]",
  abstract      = "We formulate a hierarchical rectified flow to model data
                   distributions. It hierarchically couples multiple ordinary
                   differential equations (ODEs) and defines a
                   time-differentiable stochastic process that generates a data
                   distribution from a known source distribution. Each ODE
                   resembles the ODE that is solved in a classic rectified flow,
                   but differs in its domain, i.e., location, velocity,
                   acceleration, etc. Unlike the classic rectified flow
                   formulation, which formulates a single ODE in the location
                   domain and only captures the expected velocity field
                   (sufficient to capture a multi-modal data distribution), the
                   hierarchical rectified flow formulation models the
                   multi-modal random velocity field, acceleration field, etc.,
                   in their entirety. This more faithful modeling of the random
                   velocity field enables integration paths to intersect when
                   the underlying ODE is solved during data generation.
                   Intersecting paths in turn lead to integration trajectories
                   that are more straight than those obtained in the classic
                   rectified flow formulation, where integration paths cannot
                   intersect. This leads to modeling of data distributions with
                   fewer neural function evaluations. We empirically verify this
                   on synthetic 1D and 2D data as well as MNIST, CIFAR-10, and
                   ImageNet-32 data. Code is available at:
                   https://riccizz.github.io/HRF/.",
  month         =  "24~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2502.17436"
}

@ARTICLE{Jiaqi2025-zy,
  title         = "Unraveling the geometry of visual relational reasoning",
  author        = "Jiaqi, Shang and Gabriel, Kreiman and Haim, Sompolinsky",
  journal       = "arXiv [q-bio.NC]",
  abstract      = "Humans and other animals readily generalize abstract
                   relations, such as recognizing constant in shape or color,
                   whereas neural networks struggle. To investigate how neural
                   networks generalize abstract relations, we introduce
                   SimplifiedRPM, a novel benchmark for systematic evaluation.
                   In parallel, we conduct human experiments to benchmark
                   relational difficulty, enabling direct model-human
                   comparisons. Testing four architectures--ResNet-50, Vision
                   Transformer, Wild Relation Network, and Scattering
                   Compositional Learner (SCL)--we find that SCL best aligns
                   with human behavior and generalizes best. Building on a
                   geometric theory of neural representations, we show
                   representational geometries that predict generalization.
                   Layer-wise analysis reveals distinct relational reasoning
                   strategies across models and suggests a trade-off where
                   unseen rule representations compress into training-shaped
                   subspaces. Guided by our geometric perspective, we propose
                   and evaluate SNRloss, a novel objective balancing
                   representation geometry. Our findings offer geometric
                   insights into how neural networks generalize abstract
                   relations, paving the way for more human-like visual
                   reasoning in AI.",
  month         =  "24~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "2502.17382"
}

@ARTICLE{Zekun2025-xk,
  title         = "Improved diffusion-based generative model with better
                   adversarial robustness",
  author        = "Zekun, Wang and Mingyang, Yi and Shuchen, Xue and Zhenguo, Li
                   and Ming, Liu and Bing, Qin and Zhi-Ming, Ma",
  journal       = "arXiv [cs.LG]",
  abstract      = "Diffusion Probabilistic Models (DPMs) have achieved
                   significant success in generative tasks. However, their
                   training and sampling processes suffer from the issue of
                   distribution mismatch. During the denoising process, the
                   input data distributions differ between the training and
                   inference stages, potentially leading to inaccurate data
                   generation. To obviate this, we analyze the training
                   objective of DPMs and theoretically demonstrate that this
                   mismatch can be alleviated through Distributionally Robust
                   Optimization (DRO), which is equivalent to performing
                   robustness-driven Adversarial Training (AT) on DPMs.
                   Furthermore, for the recently proposed Consistency Model
                   (CM), which distills the inference process of the DPM, we
                   prove that its training objective also encounters the
                   mismatch issue. Fortunately, this issue can be mitigated by
                   AT as well. Based on these insights, we propose to conduct
                   efficient AT on both DPM and CM. Finally, extensive empirical
                   studies validate the effectiveness of AT in diffusion-based
                   models. The code is available at
                   https://github.com/kugwzk/AT\_Diff.",
  month         =  "24~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2502.17099"
}

@ARTICLE{Canyu2025-eb,
  title         = "{DICEPTION}: A generalist diffusion model for visual
                   perceptual tasks",
  author        = "Canyu, Zhao and Mingyu, Liu and Huanyi, Zheng and Muzhi, Zhu
                   and Zhiyue, Zhao and Hao, Chen and Tong, He and Chunhua, Shen",
  journal       = "arXiv [cs.CV]",
  abstract      = "Our primary goal here is to create a good, generalist
                   perception model that can tackle multiple tasks, within
                   limits on computational resources and training data. To
                   achieve this, we resort to text-to-image diffusion models
                   pre-trained on billions of images. Our exhaustive evaluation
                   metrics demonstrate that DICEPTION effectively tackles
                   multiple perception tasks, achieving performance on par with
                   state-of-the-art models. We achieve results on par with
                   SAM-vit-h using only 0.06\% of their data (e.g., 600K vs. 1B
                   pixel-level annotated images). Inspired by Wang et al.,
                   DICEPTION formulates the outputs of various perception tasks
                   using color encoding; and we show that the strategy of
                   assigning random colors to different instances is highly
                   effective in both entity segmentation and semantic
                   segmentation. Unifying various perception tasks as
                   conditional image generation enables us to fully leverage
                   pre-trained text-to-image models. Thus, DICEPTION can be
                   efficiently trained at a cost of orders of magnitude lower,
                   compared to conventional models that were trained from
                   scratch. When adapting our model to other tasks, it only
                   requires fine-tuning on as few as 50 images and 1\% of its
                   parameters. DICEPTION provides valuable insights and a more
                   promising solution for visual generalist models.",
  month         =  "24~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.17157"
}

@ARTICLE{Eleftherios2025-vt,
  title         = "{PQDAST}: Depth-aware arbitrary style transfer for games via
                   perceptual quality-guided distillation",
  author        = "Eleftherios, Ioannou and Steve, Maddock",
  journal       = "arXiv [cs.CV]",
  abstract      = "Artistic style transfer is concerned with the generation of
                   imagery that combines the content of an image with the style
                   of an artwork. In the realm of computer games, most work has
                   focused on post-processing video frames. Some recent work has
                   integrated style transfer into the game pipeline, but it is
                   limited to single styles. Integrating an arbitrary style
                   transfer method into the game pipeline is challenging due to
                   the memory and speed requirements of games. We present
                   PQDAST, the first solution to address this. We use a
                   perceptual quality-guided knowledge distillation framework
                   and train a compressed model using the FLIP evaluator, which
                   substantially reduces both memory usage and processing time
                   with limited impact on stylisation quality. For better
                   preservation of depth and fine details, we utilise a
                   synthetic dataset with depth and temporal considerations
                   during training. The developed model is injected into the
                   rendering pipeline to further enforce temporal stability and
                   avoid diminishing post-process effects. Quantitative and
                   qualitative experiments demonstrate that our approach
                   achieves superior performance in temporal consistency, with
                   comparable style transfer quality, to state-of-the-art image,
                   video and in-game methods.",
  month         =  "24~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.16996"
}

@ARTICLE{Kedi2025-dv,
  title         = "{HVIS}: A human-like vision and Inference System for human
                   motion prediction",
  author        = "Kedi, Lyu and Haipeng, Chen and Zhenguang, Liu and Yifang,
                   Yin and Yukang, Lin and Yingying, Jiao",
  journal       = "arXiv [cs.CV]",
  abstract      = "Grasping the intricacies of human motion, which involve
                   perceiving spatio-temporal dependence and multi-scale
                   effects, is essential for predicting human motion. While
                   humans inherently possess the requisite skills to navigate
                   this issue, it proves to be markedly more challenging for
                   machines to emulate. To bridge the gap, we propose the
                   Human-like Vision and Inference System (HVIS) for human
                   motion prediction, which is designed to emulate human
                   observation and forecast future movements. HVIS comprises two
                   components: the human-like vision encode (HVE) module and the
                   human-like motion inference (HMI) module. The HVE module
                   mimics and refines the human visual process, incorporating a
                   retina-analog component that captures spatiotemporal
                   information separately to avoid unnecessary crosstalk.
                   Additionally, a visual cortex-analogy component is designed
                   to hierarchically extract and treat complex motion features,
                   focusing on both global and local features of human poses.
                   The HMI is employed to simulate the multi-stage learning
                   model of the human brain. The spontaneous learning network
                   simulates the neuronal fracture generation process for the
                   adversarial generation of future motions. Subsequently, the
                   deliberate learning network is optimized for hard-to-train
                   joints to prevent misleading learning. Experimental results
                   demonstrate that our method achieves new state-of-the-art
                   performance, significantly outperforming existing methods by
                   19.8\% on Human3.6M, 15.7\% on CMU Mocap, and 11.1\% on G3D.",
  month         =  "24~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.16913"
}

@ARTICLE{Weiyu2025-ml,
  title         = "A survey of {fMRI} to image reconstruction",
  author        = "Weiyu, Guo and Guoying, Sun and JianXiang, He and Tong, Shao
                   and Shaoguang, Wang and Ziyang, Chen and Meisheng, Hong and
                   Ying, Sun and Hui, Xiong",
  journal       = "arXiv [cs.CV]",
  abstract      = "Functional magnetic resonance imaging (fMRI) based image
                   reconstruction plays a pivotal role in decoding human
                   perception, with applications in neuroscience and
                   brain-computer interfaces. While recent advancements in deep
                   learning and large-scale datasets have driven progress,
                   challenges such as data scarcity, cross-subject variability,
                   and low semantic consistency persist. To address these
                   issues, we introduce the concept of fMRI-to-Image Learning
                   (fMRI2Image) and present the first systematic review in this
                   field. This review highlights key challenges, categorizes
                   methodologies such as fMRI signal encoding, feature mapping,
                   and image generator. Finally, promising research directions
                   are proposed to advance this emerging frontier, providing a
                   reference for future studies.",
  month         =  "24~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.16861"
}

@INPROCEEDINGS{Zhang2024-he,
  title     = "Scaling In-the-Wild Training for Diffusion-based Illumination
               Harmonization and Editing by Imposing Consistent Light Transport",
  author    = "Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh",
  booktitle = "The Thirteenth International Conference on Learning
               Representations",
  abstract  = "Diffusion-based image generators are becoming unique methods for
               illumination harmonization and editing. The current bottleneck in
               scaling up the training of diffusion-based illumination editing
               models is mainly in the difficulty of preserving the underlying
               image details and maintaining intrinsic properties, such as
               albedos, unchanged. Without appropriate constraints, directly
               training the latest large image models with complex, varied, or
               in-the-wild data is likely to produce a structure-guided random
               image generator, rather than achieving the intended goal of
               precise illumination manipulation. We propose Imposing Consistent
               Light (IC-Light) transport during training, rooted in the
               physical principle that the linear blending of an object's
               appearances under different illumination conditions is consistent
               with its appearance under mixed illumination. This consistency
               allows for stable and scalable illumination learning, uniform
               handling of various data sources, and facilitates a physically
               grounded model behavior that modifies only the illumination of
               images while keeping other intrinsic properties unchanged. Based
               on this method, we can scale up the training of diffusion-based
               illumination editing models to large data quantities (> 10
               million), across all available data types (real light stages,
               rendered samples, in-the-wild synthetic augmentations, etc), and
               using strong backbones (SDXL, Flux, etc). We also demonstrate
               that this approach reduces uncertainties and mitigates artifacts
               such as mismatched materials or altered albedos.",
  month     =  "4~" # oct,
  year      =  2024
}

@ARTICLE{Siting2024-pn,
  title         = "Exploring how Generative {MLLMs} perceive more than {CLIP}
                   with the same vision encoder",
  author        = "Siting, Li and Koh, Pang Wei and Du, Simon Shaolei",
  journal       = "arXiv [cs.LG]",
  abstract      = "Recent research has shown that CLIP models struggle with
                   visual reasoning tasks that require grounding
                   compositionality, understanding spatial relationships, or
                   capturing fine-grained details. One natural hypothesis is
                   that the CLIP vision encoder does not embed essential
                   information for these tasks. However, we find that this is
                   not always the case: The encoder gathers query-relevant
                   visual information, while CLIP fails to extract it. In
                   particular, we show that another branch of Vision-Language
                   Models (VLMs), Generative Multimodal Large Language Models
                   (MLLMs), achieve significantly higher accuracy than CLIP in
                   many of these tasks using the same vision encoder and
                   weights, indicating that these Generative MLLMs perceive more
                   -- as they extract and utilize visual information more
                   effectively. We conduct a series of controlled experiments
                   and reveal that their success is attributed to multiple key
                   design choices, including patch tokens, position embeddings,
                   and prompt-based weighting. On the other hand, enhancing the
                   training data alone or applying a stronger text encoder does
                   not suffice to solve the task, and additional text tokens
                   offer little benefit. Interestingly, we find that
                   fine-grained visual reasoning is not exclusive to generative
                   models trained by an autoregressive loss: When converted into
                   CLIP-like encoders by contrastive finetuning, these MLLMs
                   still outperform CLIP under the same cosine similarity-based
                   evaluation protocol. Our study highlights the importance of
                   VLM architectural choices and suggests directions for
                   improving the performance of CLIP-like contrastive VLMs.",
  month         =  "7~" # nov,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2411.05195"
}

@ARTICLE{Wronski2025-js,
  title         = "{GPU}-Friendly Laplacian Texture Blending",
  author        = "Wronski, Bartlomiej",
  journal       = "arXiv [cs.GR]",
  abstract      = "Texture and material blending is one of the leading methods
                   for adding variety to rendered virtual worlds, creating
                   composite materials, and generating procedural content. When
                   done naively, it can introduce either visible seams or
                   contrast loss, leading to an unnatural look not
                   representative of blended textures. Earlier work proposed
                   addressing this problem through careful manual parameter
                   tuning, lengthy per-texture statistics precomputation,
                   look-up tables, or training deep neural networks. In this
                   work, we propose an alternative approach based on insights
                   from image processing and Laplacian pyramid blending. Our
                   approach does not require any precomputation or increased
                   memory usage (other than the presence of a regular,
                   non-Laplacian, texture mipmap chain), does not produce
                   ghosting, preserves sharp local features, and can run in real
                   time on the GPU at the cost of a few additional lower mipmap
                   texture taps.",
  month         =  "19~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.GR",
  eprint        = "2502.13945"
}

@ARTICLE{Khoei2017-qr,
  title     = "The flash-lag effect as a motion-based predictive shift",
  author    = "Khoei, Mina A and Masson, Guillaume S and Perrinet, Laurent U",
  journal   = "PLoS computational biology",
  publisher = "Public Library of Science",
  volume    =  13,
  number    =  1,
  pages     = "e1005068",
  abstract  = "Due to its inherent neural delays, the visual system has an
               outdated access to sensory information about the current position
               of moving objects. In contrast, living organisms are remarkably
               able to track and intercept moving objects under a large range of
               challenging environmental conditions. Physiological, behavioral
               and psychophysical evidences strongly suggest that position
               coding is extrapolated using an explicit and reliable
               representation of object's motion but it is still unclear how
               these two representations interact. For instance, the so-called
               flash-lag effect supports the idea of a differential processing
               of position between moving and static objects. Although
               elucidating such mechanisms is crucial in our understanding of
               the dynamics of visual processing, a theory is still missing to
               explain the different facets of this visual illusion. Here, we
               reconsider several of the key aspects of the flash-lag effect in
               order to explore the role of motion upon neural coding of
               objects' position. First, we formalize the problem using a
               Bayesian modeling framework which includes a graded
               representation of the degree of belief about visual motion. We
               introduce a motion-based prediction model as a candidate
               explanation for the perception of coherent motion. By including
               the knowledge of a fixed delay, we can model the dynamics of
               sensory information integration by extrapolating the information
               acquired at previous instants in time. Next, we simulate the
               optimal estimation of object position with and without delay
               compensation and compared it with human perception under a broad
               range of different psychophysical conditions. Our computational
               study suggests that the explicit, probabilistic representation of
               velocity information is crucial in explaining position coding,
               and therefore the flash-lag effect. We discuss these theoretical
               results in light of the putative corrective mechanisms that can
               be used to cancel out the detrimental effects of neural delays
               and illuminate the more general question of the dynamical
               representation at the present time of spatial information in the
               visual pathways.",
  month     =  "26~" # jan,
  year      =  2017,
  keywords  = "\_To\_read",
  doi       = "10.1371/journal.pcbi.1005068",
  pmc       = "PMC5268412",
  pmid      =  28125585,
  issn      = "1553-7358,1553-734X",
  language  = "en"
}

@ARTICLE{Gianluca2025-ae,
  title         = "Leveraging intermediate representations for better
                   Out-of-Distribution detection",
  author        = "Gianluca, Guglielmo and Marc, Masana",
  journal       = "arXiv [cs.LG]",
  abstract      = "In real-world applications, machine learning models must
                   reliably detect Out-of-Distribution (OoD) samples to prevent
                   unsafe decisions. Current OoD detection methods often rely on
                   analyzing the logits or the embeddings of the penultimate
                   layer of a neural network. However, little work has been
                   conducted on the exploitation of the rich information encoded
                   in intermediate layers. To address this, we analyze the
                   discriminative power of intermediate layers and show that
                   they can positively be used for OoD detection. Therefore, we
                   propose to regularize intermediate layers with an
                   energy-based contrastive loss, and by grouping multiple
                   layers in a single aggregated response. We demonstrate that
                   intermediate layer activations improves OoD detection
                   performance by running a comprehensive evaluation across
                   multiple datasets.",
  month         =  "18~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2502.12849"
}

@ARTICLE{Jinfan2025-xg,
  title         = "Revisiting the generalization problem of low-level vision
                   models through the lens of image deraining",
  author        = "Jinfan, Hu and Zhiyuan, You and Jinjin, Gu and Kaiwen, Zhu
                   and Tianfan, Xue and Chao, Dong",
  journal       = "arXiv [cs.CV]",
  abstract      = "Generalization remains a significant challenge for low-level
                   vision models, which often struggle with unseen degradations
                   in real-world scenarios despite their success in controlled
                   benchmarks. In this paper, we revisit the generalization
                   problem in low-level vision models. Image deraining is
                   selected as a case study due to its well-defined and easily
                   decoupled structure, allowing for more effective observation
                   and analysis. Through comprehensive experiments, we reveal
                   that the generalization issue is not primarily due to limited
                   network capacity but rather the failure of existing training
                   strategies, which leads networks to overfit specific
                   degradation patterns. Our findings show that guiding networks
                   to focus on learning the underlying image content, rather
                   than the degradation patterns, is key to improving
                   generalization. We demonstrate that balancing the complexity
                   of background images and degradations in the training data
                   helps networks better fit the image distribution.
                   Furthermore, incorporating content priors from pre-trained
                   generative models significantly enhances generalization.
                   Experiments on both image deraining and image denoising
                   validate the proposed strategies. We believe the insights and
                   solutions will inspire further research and improve the
                   generalization of low-level vision models.",
  month         =  "18~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.12600"
}

@ARTICLE{Gannamaneni2025-nc,
  title         = "Detecting systematic weaknesses in vision models along
                   predefined human-understandable dimensions",
  author        = "Gannamaneni, Sujan Sai and Rao, Rohil Prakash and Michael,
                   Mock and Maram, Akila and Stefan, Wrobel",
  journal       = "arXiv [cs.CV]",
  abstract      = "Studying systematic weaknesses of DNNs has gained prominence
                   in the last few years with the rising focus on building safe
                   AI systems. Slice discovery methods (SDMs) are prominent
                   algorithmic approaches for finding such systematic
                   weaknesses. They identify top-k semantically coherent
                   slices/subsets of data where a DNN-under-test has low
                   performance. For being directly useful, e.g., as evidences in
                   a safety argumentation, slices should be aligned with
                   human-understandable (safety-relevant) dimensions, which, for
                   example, are defined by safety and domain experts as parts of
                   the operational design domain (ODD). While straightforward
                   for structured data, the lack of semantic metadata makes
                   these investigations challenging for unstructured data.
                   Therefore, we propose a complete workflow which combines
                   contemporary foundation models with algorithms for
                   combinatorial search that consider structured data and DNN
                   errors for finding systematic weaknesses in images. In
                   contrast to existing approaches, ours identifies weak slices
                   that are in line with predefined human-understandable
                   dimensions. As the workflow includes foundation models, its
                   intermediate and final results may not always be exact.
                   Therefore, we build into our workflow an approach to address
                   the impact of noisy metadata. We evaluate our approach w.r.t.
                   its quality on four popular computer vision datasets,
                   including autonomous driving datasets like Cityscapes,
                   BDD100k, and RailSem19, while using multiple state-of-the-art
                   models as DNNs-under-test.",
  month         =  "17~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.12360"
}

@ARTICLE{Negar2025-hr,
  title         = "Characterizing photorealism and artifacts in diffusion
                   model-generated images",
  author        = "Negar, Kamali and Karyn, Nakamura and Aakriti, Kumar and
                   Angelos, Chatzimparmpas and Jessica, Hullman and Matthew,
                   Groh",
  journal       = "arXiv [cs.HC]",
  abstract      = "Diffusion model-generated images can appear indistinguishable
                   from authentic photographs, but these images often contain
                   artifacts and implausibilities that reveal their AI-generated
                   provenance. Given the challenge to public trust in media
                   posed by photorealistic AI-generated images, we conducted a
                   large-scale experiment measuring human detection accuracy on
                   450 diffusion-model generated images and 149 real images.
                   Based on collecting 749,828 observations and 34,675 comments
                   from 50,444 participants, we find that scene complexity of an
                   image, artifact types within an image, display time of an
                   image, and human curation of AI-generated images all play
                   significant roles in how accurately people distinguish real
                   from AI-generated images. Additionally, we propose a taxonomy
                   characterizing artifacts often appearing in images generated
                   by diffusion models. Our empirical observations and taxonomy
                   offer nuanced insights into the capabilities and limitations
                   of diffusion models to generate photorealistic images in
                   2024.",
  month         =  "17~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2502.11989",
  keywords      = "Project/Density\_IQA"
}

@ARTICLE{Marina2025-rf,
  title         = "A comparison of human and machine learning errors in face
                   recognition",
  author        = "Marina, Estévez-Almenzar and Ricardo, Baeza-Yates and Carlos,
                   Castillo",
  journal       = "arXiv [cs.HC]",
  abstract      = "Machine learning applications in high-stakes scenarios should
                   always operate under human oversight. Developing an optimal
                   combination of human and machine intelligence requires an
                   understanding of their complementarities, particularly
                   regarding the similarities and differences in the way they
                   make mistakes. We perform extensive experiments in the area
                   of face recognition and compare two automated face
                   recognition systems against human annotators through a
                   demographically balanced user study. Our research uncovers
                   important ways in which machine learning errors and human
                   errors differ from each other, and suggests potential
                   strategies in which human-machine collaboration can improve
                   accuracy in face recognition.",
  month         =  "17~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2502.11337"
}

@ARTICLE{Zhicong2025-xt,
  title         = "Diffusion models without Classifier-free guidance",
  author        = "Zhicong, Tang and Jianmin, Bao and Dong, Chen and Baining,
                   Guo",
  journal       = "arXiv [cs.CV]",
  abstract      = "This paper presents Model-guidance (MG), a novel objective
                   for training diffusion model that addresses and removes of
                   the commonly used Classifier-free guidance (CFG). Our
                   innovative approach transcends the standard modeling of
                   solely data distribution to incorporating the posterior
                   probability of conditions. The proposed technique originates
                   from the idea of CFG and is easy yet effective, making it a
                   plug-and-play module for existing models. Our method
                   significantly accelerates the training process, doubles the
                   inference speed, and achieve exceptional quality that
                   parallel and even surpass concurrent diffusion models with
                   CFG. Extensive experiments demonstrate the effectiveness,
                   efficiency, scalability on different models and datasets.
                   Finally, we establish state-of-the-art performance on
                   ImageNet 256 benchmarks with an FID of 1.34. Our code is
                   available at https://github.com/tzco/Diffusion-wo-CFG.",
  month         =  "17~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.12154"
}

@ARTICLE{Wenrui2025-my,
  title         = "Defining and evaluating Visual Language Models' Basic Spatial
                   Abilities: A perspective from psychometrics",
  author        = "Wenrui, Xu and Dalin, Lyu and Weihang, Wang and Jie, Feng and
                   Chen, Gao and Yong, Li",
  journal       = "arXiv [cs.CV]",
  abstract      = "The Theory of Multiple Intelligences underscores the
                   hierarchical nature of cognitive capabilities. To advance
                   Spatial Artificial Intelligence, we pioneer a psychometric
                   framework defining five Basic Spatial Abilities (BSAs) in
                   Visual Language Models (VLMs): Spatial Perception, Spatial
                   Relation, Spatial Orientation, Mental Rotation, and Spatial
                   Visualization. Benchmarking 13 mainstream VLMs through nine
                   validated psychometric experiments reveals significant gaps
                   versus humans (average score 24.95 vs. 68.38), with three key
                   findings: 1) VLMs mirror human hierarchies (strongest in 2D
                   orientation, weakest in 3D rotation) with independent BSAs
                   (Pearson's r<0.4); 2) Smaller models such as Qwen2-VL-7B
                   surpass larger counterparts, with Qwen leading (30.82) and
                   InternVL2 lagging (19.6); 3) Interventions like
                   chain-of-thought (0.100 accuracy gain) and 5-shot training
                   (0.259 improvement) show limits from architectural
                   constraints. Identified barriers include weak geometry
                   encoding and missing dynamic simulation. By linking
                   psychometric BSAs to VLM capabilities, we provide a
                   diagnostic toolkit for spatial intelligence evaluation,
                   methodological foundations for embodied AI development, and a
                   cognitive science-informed roadmap for achieving human-like
                   spatial intelligence.",
  month         =  "17~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.11859"
}

@ARTICLE{Yanbiao2025-wi,
  title         = "Revealing bias formation in deep neural networks through the
                   geometric mechanisms of human visual decoupling",
  author        = "Yanbiao, Ma and Bowei, Liu and Wei, Dai and Jiayi, Chen and
                   Shuo, Li",
  journal       = "arXiv [cs.CV]",
  abstract      = "Deep neural networks (DNNs) often exhibit biases toward
                   certain categories during object recognition, even under
                   balanced training data conditions. The intrinsic mechanisms
                   underlying these biases remain unclear. Inspired by the human
                   visual system, which decouples object manifolds through
                   hierarchical processing to achieve object recognition, we
                   propose a geometric analysis framework linking the geometric
                   complexity of class-specific perceptual manifolds in DNNs to
                   model bias. Our findings reveal that differences in geometric
                   complexity can lead to varying recognition capabilities
                   across categories, introducing biases. To support this
                   analysis, we present the Perceptual-Manifold-Geometry
                   library, designed for calculating the geometric properties of
                   perceptual manifolds.",
  month         =  "17~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.11809"
}

@ARTICLE{Yasir2025-em,
  title         = "Deep neural networks for accurate depth estimation with
                   latent space features",
  author        = "Yasir, Siddiqui Muhammad and Hyunsik, Ahn",
  journal       = "arXiv [cs.CV]",
  abstract      = "Depth estimation plays a pivotal role in advancing
                   human-robot interactions, especially in indoor environments
                   where accurate 3D scene reconstruction is essential for tasks
                   like navigation and object handling. Monocular depth
                   estimation, which relies on a single RGB camera, offers a
                   more affordable solution compared to traditional methods that
                   use stereo cameras or LiDAR. However, despite recent
                   progress, many monocular approaches struggle with accurately
                   defining depth boundaries, leading to less precise
                   reconstructions. In response to these challenges, this study
                   introduces a novel depth estimation framework that leverages
                   latent space features within a deep convolutional neural
                   network to enhance the precision of monocular depth maps. The
                   proposed model features dual encoder-decoder architecture,
                   enabling both color-to-depth and depth-to-depth
                   transformations. This structure allows for refined depth
                   estimation through latent space encoding. To further improve
                   the accuracy of depth boundaries and local features, a new
                   loss function is introduced. This function combines latent
                   loss with gradient loss, helping the model maintain the
                   integrity of depth boundaries. The framework is thoroughly
                   tested using the NYU Depth V2 dataset, where it sets a new
                   benchmark, particularly excelling in complex indoor
                   scenarios. The results clearly show that this approach
                   effectively reduces depth ambiguities and blurring, making it
                   a promising solution for applications in human-robot
                   interaction and 3D scene reconstruction.",
  month         =  "17~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.11777"
}

@ARTICLE{Quentin2025-af,
  title         = "Intuitive physics understanding emerges from self-supervised
                   pretraining on natural videos",
  author        = "Quentin, Garrido and Nicolas, Ballas and Mahmoud, Assran and
                   Adrien, Bardes and Laurent, Najman and Michael, Rabbat and
                   Emmanuel, Dupoux and Yann, Lecun",
  journal       = "arXiv [cs.CV]",
  abstract      = "We investigate the emergence of intuitive physics
                   understanding in general-purpose deep neural network models
                   trained to predict masked regions in natural videos.
                   Leveraging the violation-of-expectation framework, we find
                   that video prediction models trained to predict outcomes in a
                   learned representation space demonstrate an understanding of
                   various intuitive physics properties, such as object
                   permanence and shape consistency. In contrast, video
                   prediction in pixel space and multimodal large language
                   models, which reason through text, achieve performance closer
                   to chance. Our comparisons of these architectures reveal that
                   jointly learning an abstract representation space while
                   predicting missing parts of sensory input, akin to predictive
                   coding, is sufficient to acquire an understanding of
                   intuitive physics, and that even models trained on one week
                   of unique video achieve above chance performance. This
                   challenges the idea that core knowledge -- a set of innate
                   systems to help understand the world -- needs to be hardwired
                   to develop an understanding of intuitive physics.",
  month         =  "17~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.11831",
  keywords      = "\_To\_read;Project/Density\_IQA"
}

@ARTICLE{Francesco2025-cp,
  title         = "Adversarially robust {CLIP} models can induce better (robust)
                   perceptual metrics",
  author        = "Francesco, Croce and Christian, Schlarmann and Singh, Naman
                   Deep and Matthias, Hein",
  journal       = "arXiv [cs.CV]",
  abstract      = "Measuring perceptual similarity is a key tool in computer
                   vision. In recent years perceptual metrics based on features
                   extracted from neural networks with large and diverse
                   training sets, e.g. CLIP, have become popular. At the same
                   time, the metrics extracted from features of neural networks
                   are not adversarially robust. In this paper we show that
                   adversarially robust CLIP models, called R-CLIP$_\textrm{F}$,
                   obtained by unsupervised adversarial fine-tuning induce a
                   better and adversarially robust perceptual metric that
                   outperforms existing metrics in a zero-shot setting, and
                   further matches the performance of state-of-the-art metrics
                   while being robust after fine-tuning. Moreover, our
                   perceptual metric achieves strong performance on related
                   tasks such as robust image-to-image retrieval, which becomes
                   especially relevant when applied to ``Not Safe for Work''
                   (NSFW) content detection and dataset filtering. While
                   standard perceptual metrics can be easily attacked by a small
                   perturbation completely degrading NSFW detection, our robust
                   perceptual metric maintains high accuracy under an attack
                   while having similar performance for unperturbed images.
                   Finally, perceptual metrics induced by robust CLIP models
                   have higher interpretability: feature inversion can show
                   which images are considered similar, while text inversion can
                   find what images are associated to a given prompt. This also
                   allows us to visualize the very rich visual concepts learned
                   by a CLIP model, including memorized persons, paintings and
                   complex queries.",
  month         =  "17~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.11725",
  keywords      = "Project/Metamerism"
}

@ARTICLE{Jonathan2025-wh,
  title         = "A recurrent vision transformer shows signatures of primate
                   visual attention",
  author        = "Jonathan, Morgan and Badr, Albanna and James, P Herman",
  journal       = "arXiv [cs.CV]",
  abstract      = "Attention is fundamental to both biological and artificial
                   intelligence, yet research on animal attention and AI self
                   attention remains largely disconnected. We propose a
                   Recurrent Vision Transformer (Recurrent ViT) that integrates
                   self-attention with recurrent memory, allowing both current
                   inputs and stored information to guide attention allocation.
                   Trained solely via sparse reward feedback on a spatially cued
                   orientation change detection task, a paradigm used in primate
                   studies, our model exhibits primate like signatures of
                   attention, including improved accuracy and faster responses
                   for cued stimuli that scale with cue validity. Analysis of
                   self-attention maps reveals dynamic spatial prioritization
                   with reactivation prior to expected changes, and targeted
                   perturbations produce performance shifts similar to those
                   observed in primate frontal eye fields and superior
                   colliculus. These findings demonstrate that incorporating
                   recurrent feedback into self attention can capture key
                   aspects of primate visual attention.",
  month         =  "16~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.10955"
}

@ARTICLE{Saphra2024-de,
  title         = "Mechanistic?",
  author        = "Saphra, Naomi and Wiegreffe, Sarah",
  journal       = "arXiv [cs.AI]",
  abstract      = "The rise of the term ``mechanistic interpretability'' has
                   accompanied increasing interest in understanding neural
                   models -- particularly language models. However, this jargon
                   has also led to a fair amount of confusion. So, what does it
                   mean to be ``mechanistic''? We describe four uses of the term
                   in interpretability research. The most narrow technical
                   definition requires a claim of causality, while a broader
                   technical definition allows for any exploration of a model's
                   internals. However, the term also has a narrow cultural
                   definition describing a cultural movement. To understand this
                   semantic drift, we present a history of the NLP
                   interpretability community and the formation of the separate,
                   parallel ``mechanistic'' interpretability community. Finally,
                   we discuss the broad cultural definition -- encompassing the
                   entire field of interpretability -- and why the traditional
                   NLP interpretability community has come to embrace it. We
                   argue that the polysemy of ``mechanistic'' is the product of
                   a critical divide within the interpretability community.",
  month         =  "7~" # oct,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2410.09087"
}

@ARTICLE{Tariq2024-vw,
  title     = "Towards motion metamers for foveated rendering",
  author    = "Tariq, Taimoor and Didyk, Piotr",
  journal   = "ACM transactions on graphics",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  43,
  number    =  4,
  pages     = "1--10",
  abstract  = "Foveated rendering takes advantage of the reduced spatial
               sensitivity in peripheral vision to greatly reduce rendering cost
               without noticeable spatial quality degradation. Due to its
               benefits, it has emerged as a key enabler for real-time
               high-quality virtual and augmented realities. Interestingly
               though, a large body of work advocates that a key role of
               peripheral vision may be motion detection, yet foveated rendering
               lowers the image quality in these regions, which may impact our
               ability to detect and quantify motion. The problem is critical
               for immersive simulations where the ability to detect and
               quantify movement drives actions and decisions. In this work, we
               diverge from the contemporary approach towards the goal of
               foveated graphics, and demonstrate that a loss of high-frequency
               spatial details in the periphery inhibits motion perception,
               leading to underestimating motion cues such as velocity.
               Furthermore, inspired by an interesting visual illusion, we
               design a perceptually motivated real-time technique that
               synthesizes controlled spatio-temporal motion energy to offset
               the loss in motion perception. Finally, we perform user
               experiments demonstrating our method's effectiveness in
               recovering motion cues without introducing objectionable quality
               degradation.",
  month     =  "19~" # jul,
  year      =  2024,
  keywords  = "\_To\_read",
  doi       = "10.1145/3658141",
  issn      = "0730-0301,1557-7368",
  language  = "en"
}

@ARTICLE{Haoran2025-kr,
  title         = "Probing perceptual constancy in large Vision Language Models",
  author        = "Haoran, Sun and Suyang, Yu and Yijiang, Li and Qingying, Gao
                   and Haiyun, Lyu and Hokin, Deng and Dezhi, Luo",
  journal       = "arXiv [cs.CV]",
  abstract      = "Perceptual constancy is the ability to maintain stable
                   perceptions of objects despite changes in sensory input, such
                   as variations in distance, angle, or lighting. This ability
                   is crucial for recognizing visual information in a dynamic
                   world, making it essential for Vision-Language Models (VLMs).
                   However, whether VLMs are currently and theoretically capable
                   of mastering this ability remains underexplored. In this
                   study, we evaluated 33 VLMs using 253 experiments across
                   three domains: color, size, and shape constancy. The
                   experiments included single-image and video adaptations of
                   classic cognitive tasks, along with novel tasks in
                   in-the-wild conditions, to evaluate the models' recognition
                   of object properties under varying conditions. We found
                   significant variability in VLM performance, with models
                   performance in shape constancy clearly dissociated from that
                   of color and size constancy.",
  month         =  "14~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.10273"
}

@ARTICLE{Gandelsman2024-nh,
  title         = "Interpreting the second-order effects of neurons in {CLIP}",
  author        = "Gandelsman, Yossi and A Efros, Alexei and Steinhardt, Jacob",
  journal       = "arXiv [cs.CV]",
  abstract      = "We interpret the function of individual neurons in CLIP by
                   automatically describing them using text. Analyzing the
                   direct effects (i.e. the flow from a neuron through the
                   residual stream to the output) or the indirect effects
                   (overall contribution) fails to capture the neurons' function
                   in CLIP. Therefore, we present the ``second-order lens'',
                   analyzing the effect flowing from a neuron through the later
                   attention heads, directly to the output. We find that these
                   effects are highly selective: for each neuron, the effect is
                   significant for <2\% of the images. Moreover, each effect can
                   be approximated by a single direction in the text-image space
                   of CLIP. We describe neurons by decomposing these directions
                   into sparse sets of text representations. The sets reveal
                   polysemantic behavior - each neuron corresponds to multiple,
                   often unrelated, concepts (e.g. ships and cars). Exploiting
                   this neuron polysemy, we mass-produce ``semantic''
                   adversarial examples by generating images with concepts
                   spuriously correlated to the incorrect class. Additionally,
                   we use the second-order effects for zero-shot segmentation,
                   outperforming previous methods. Our results indicate that an
                   automated interpretation of neurons can be used for model
                   deception and for introducing new model capabilities.",
  month         =  "6~" # jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2406.04341"
}

@ARTICLE{De_Jong2025-od,
  title         = "Computational techniques enabling the perception of virtual
                   images exclusive to the retinal afterimage",
  author        = "de Jong, Staas and van der Veer, Gerrit",
  journal       = "arXiv [cs.HC]",
  abstract      = "The retinal afterimage is a widely known effect in the human
                   visual system, which has been studied and used in the context
                   of a number of major art movements. Therefore, when
                   considering the general role of computation in the visual
                   arts, this begs the question whether this effect, too, may be
                   induced using partly automated techniques. If so, it may
                   become a computationally controllable ingredient of
                   (interactive) visual art, and thus take its place among the
                   many other aspects of visual perception which already have
                   preceded it in this sense. The present moment provides
                   additional inspiration to lay the groundwork for extending
                   computer graphics in general with the retinal afterimage:
                   Historically, we are in a phase where some head-mounted
                   stereoscopic AR/VR technologies are now providing eye
                   tracking by default, thereby allowing realtime monitoring of
                   the processes of visual fixation that can induce the retinal
                   afterimage. A logical starting point for general
                   investigation is then shape display via the retinal
                   afterimage, since shape recognition lends itself well to
                   unambiguous reporting. Shape recognition, however, may also
                   occur due to normal vision, which happens simultaneously.
                   Carefully and rigorously excluding this possibility, we
                   develop computational techniques enabling shape display
                   exclusive to the retinal afterimage.",
  month         =  "13~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2502.09435"
}

@ARTICLE{Pirzada2025-jv,
  title         = "Shortcut learning susceptibility in vision classifiers",
  author        = "Pirzada, Suhail and Amit, Sethi",
  journal       = "arXiv [cs.LG]",
  abstract      = "Shortcut learning, where machine learning models exploit
                   spurious correlations in data instead of capturing meaningful
                   features, poses a significant challenge to building robust
                   and generalizable models. This phenomenon is prevalent across
                   various machine learning applications, including vision,
                   natural language processing, and speech recognition, where
                   models may find unintended cues that minimize training loss
                   but fail to capture the underlying structure of the data.
                   Vision classifiers such as Convolutional Neural Networks
                   (CNNs), Multi-Layer Perceptrons (MLPs), and Vision
                   Transformers (ViTs) leverage distinct architectural
                   principles to process spatial and structural information,
                   making them differently susceptible to shortcut learning. In
                   this study, we systematically evaluate these architectures by
                   introducing deliberate shortcuts into the dataset that are
                   positionally correlated with class labels, creating a
                   controlled setup to assess whether models rely on these
                   artificial cues or learn actual distinguishing features. We
                   perform both quantitative evaluation by training on the
                   shortcut-modified dataset and testing them on two different
                   test sets -- one containing the same shortcuts and another
                   without them -- to determine the extent of reliance on
                   shortcuts. Additionally, qualitative evaluation is performed
                   by using network inversion-based reconstruction techniques to
                   analyze what the models internalize in their weights, aiming
                   to reconstruct the training data as perceived by the
                   classifiers. We evaluate shortcut learning behavior across
                   multiple benchmark datasets, including MNIST, Fashion-MNIST,
                   SVHN, and CIFAR-10, to compare the susceptibility of
                   different vision classifier architectures to shortcut
                   reliance and assess their varying degrees of sensitivity to
                   spurious correlations.",
  month         =  "13~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2502.09150"
}

@ARTICLE{Schramm2025-ue,
  title         = "Blending the worlds: World-fixed visual appearances in
                   automotive Augmented Reality",
  author        = "Schramm, Robin Connor and Markus, Sasalovici and Freiwald,
                   Jann Philipp and Michael, Otto and Melissa, Reinelt and
                   Ulrich, Schwanecke",
  journal       = "arXiv [cs.HC]",
  abstract      = "With the transition to fully autonomous vehicles, non-driving
                   related tasks (NDRTs) become increasingly important, allowing
                   passengers to use their driving time more efficiently. In-car
                   Augmented Reality (AR) gives the possibility to engage in
                   NDRTs while also allowing passengers to engage with their
                   surroundings, for example, by displaying world-fixed points
                   of interest (POIs). This can lead to new discoveries, provide
                   information about the environment, and improve locational
                   awareness. To explore the optimal visualization of POIs using
                   in-car AR, we conducted a field study (N = 38) examining six
                   parameters: positioning, scaling, rotation, render distance,
                   information density, and appearance. We also asked for
                   intention of use, preferred seat positions and preferred
                   automation level for the AR function in a post-study
                   questionnaire. Our findings reveal user preferences and
                   general acceptance of the AR functionality. Based on these
                   results, we derived UX-guidelines for the visual appearance
                   and behavior of location-based POIs in in-car AR.",
  month         =  "12~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2502.08442"
}

@ARTICLE{Yawei2021-ji,
  title         = "{LocalViT}: Analyzing locality in vision transformers",
  author        = "Yawei, Li and Kai, Zhang and Jiezhang, Cao and Radu, Timofte
                   and Michele, Magno and Luca, Benini and Van Gool, Luc",
  journal       = "arXiv [cs.CV]",
  abstract      = "The aim of this paper is to study the influence of locality
                   mechanisms in vision transformers. Transformers originated
                   from machine translation and are particularly good at
                   modelling long-range dependencies within a long sequence.
                   Although the global interaction between the token embeddings
                   could be well modelled by the self-attention mechanism of
                   transformers, what is lacking is a locality mechanism for
                   information exchange within a local region. In this paper,
                   locality mechanism is systematically investigated by
                   carefully designed controlled experiments. We add locality to
                   vision transformers into the feed-forward network. This
                   seemingly simple solution is inspired by the comparison
                   between feed-forward networks and inverted residual blocks.
                   The importance of locality mechanisms is validated in two
                   ways: 1) A wide range of design choices (activation function,
                   layer placement, expansion ratio) are available for
                   incorporating locality mechanisms and proper choices can lead
                   to a performance gain over the baseline, and 2) The same
                   locality mechanism is successfully applied to vision
                   transformers with different architecture designs, which shows
                   the generalization of the locality concept. For ImageNet2012
                   classification, the locality-enhanced transformers outperform
                   the baselines Swin-T, DeiT-T, and PVT-T by 1.0\%, 2.6\% and
                   3.1\% with a negligible increase in the number of parameters
                   and computational effort. Code is available at
                   https://github.com/ofsoundof/LocalViT.",
  month         =  "12~" # apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2104.05707"
}

@ARTICLE{Haoran2025-zl,
  title         = "Pre-trained video generative models as world simulators",
  author        = "Haoran, He and Yang, Zhang and Liang, Lin and Zhongwen, Xu
                   and Ling, Pan",
  journal       = "arXiv [cs.CV]",
  abstract      = "Video generative models pre-trained on large-scale internet
                   datasets have achieved remarkable success, excelling at
                   producing realistic synthetic videos. However, they often
                   generate clips based on static prompts (e.g., text or
                   images), limiting their ability to model interactive and
                   dynamic scenarios. In this paper, we propose Dynamic World
                   Simulation (DWS), a novel approach to transform pre-trained
                   video generative models into controllable world simulators
                   capable of executing specified action trajectories. To
                   achieve precise alignment between conditioned actions and
                   generated visual changes, we introduce a lightweight,
                   universal action-conditioned module that seamlessly
                   integrates into any existing model. Instead of focusing on
                   complex visual details, we demonstrate that consistent
                   dynamic transition modeling is the key to building powerful
                   world simulators. Building upon this insight, we further
                   introduce a motion-reinforced loss that enhances action
                   controllability by compelling the model to capture dynamic
                   changes more effectively. Experiments demonstrate that DWS
                   can be versatilely applied to both diffusion and
                   autoregressive transformer models, achieving significant
                   improvements in generating action-controllable, dynamically
                   consistent videos across games and robotics domains.
                   Moreover, to facilitate the applications of the learned world
                   simulator in downstream tasks such as model-based
                   reinforcement learning, we propose prioritized imagination to
                   improve sample efficiency, demonstrating competitive
                   performance compared with state-of-the-art methods.",
  month         =  "10~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.07825"
}

@ARTICLE{Ziqi2025-gc,
  title         = "{IllusionCAPTCHA}: A {CAPTCHA} based on Visual Illusion",
  author        = "Ziqi, Ding and Gelei, Deng and Yi, Liu and Junchen, Ding and
                   Jieshan, Chen and Yulei, Sui and Yuekang, Li",
  journal       = "arXiv [cs.CR]",
  abstract      = "CAPTCHAs have long been essential tools for protecting
                   applications from automated bots. Initially designed as
                   simple questions to distinguish humans from bots, they have
                   become increasingly complex to keep pace with the
                   proliferation of CAPTCHA-cracking techniques employed by
                   malicious actors. However, with the advent of advanced large
                   language models (LLMs), the effectiveness of existing
                   CAPTCHAs is now being undermined. To address this issue, we
                   have conducted an empirical study to evaluate the performance
                   of multimodal LLMs in solving CAPTCHAs and to assess how many
                   attempts human users typically need to pass them. Our
                   findings reveal that while LLMs can solve most CAPTCHAs, they
                   struggle with those requiring complex reasoning type of
                   CAPTCHA that also presents significant challenges for human
                   users. Interestingly, our user study shows that the majority
                   of human participants require a second attempt to pass these
                   reasoning CAPTCHAs, a finding not reported in previous
                   research. Based on empirical findings, we present
                   IllusionCAPTCHA, a novel security mechanism employing the
                   ``Human-Easy but AI-Hard'' paradigm. This new CAPTCHA employs
                   visual illusions to create tasks that are intuitive for
                   humans but highly confusing for AI models. Furthermore, we
                   developed a structured, step-by-step method that generates
                   misleading options, which particularly guide LLMs towards
                   making incorrect choices and reduce their chances of
                   successfully solving CAPTCHAs. Our evaluation shows that
                   IllusionCAPTCHA can effectively deceive LLMs 100\% of the
                   time. Moreover, our structured design significantly increases
                   the likelihood of AI errors when attempting to solve these
                   challenges. Results from our user study indicate that 86.95\%
                   of participants successfully passed the CAPTCHA on their
                   first attempt, outperforming other CAPTCHA systems.",
  month         =  "8~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CR",
  eprint        = "2502.05461"
}

@ARTICLE{Huang2025-jk,
  title     = "Comprehensive exploration of visual working memory mechanisms
               using large-scale behavioral experiment",
  author    = "Huang, Liqiang",
  journal   = "Nature communications",
  publisher = "Springer Science and Business Media LLC",
  volume    =  16,
  number    =  1,
  pages     =  1383,
  abstract  = "Two decades of research on visual working memory have produced
               substantial yet fragmented knowledge. This study aims to
               integrate these findings into a cohesive framework. Drawing on a
               large-scale behavioral experiment involving 40 million responses
               to 10,000 color patterns, a quasi-comprehensive exploration model
               of visual working memory, termed QCE-VWM, is developed. Despite
               its significantly reduced complexity (57 parameters versus
               30,796), QCE-VWM outperforms neural networks in data fitting. The
               model provides an integrative framework for understanding human
               visual working memory, incorporating a dozen mechanisms-some
               directly adopted from previous studies, some modified, and others
               newly identified. This work underscores the value of large-scale
               behavioral experiments in advancing comprehensive models of
               cognitive mechanisms.",
  month     =  "5~" # feb,
  year      =  2025,
  doi       = "10.1038/s41467-025-56700-5",
  pmc       = "PMC11799313",
  pmid      =  39910088,
  issn      = "2041-1723,2041-1723",
  language  = "en"
}

@ARTICLE{Choi2024-wa,
  title         = "Unlearning-based neural interpretations",
  author        = "Choi, Ching Lam and Alexandre, Duplessis and Serge, Belongie",
  journal       = "arXiv [cs.LG]",
  abstract      = "Gradient-based interpretations often require an anchor point
                   of comparison to avoid saturation in computing feature
                   importance. We show that current baselines defined using
                   static functions--constant mapping, averaging or
                   blurring--inject harmful colour, texture or frequency
                   assumptions that deviate from model behaviour. This leads to
                   accumulation of irregular gradients, resulting in attribution
                   maps that are biased, fragile and manipulable. Departing from
                   the static approach, we propose UNI to compute an
                   (un)learnable, debiased and adaptive baseline by perturbing
                   the input towards an unlearning direction of steepest ascent.
                   Our method discovers reliable baselines and succeeds in
                   erasing salient features, which in turn locally smooths the
                   high-curvature decision boundaries. Our analyses point to
                   unlearning as a promising avenue for generating faithful,
                   efficient and robust interpretations.",
  month         =  "10~" # oct,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2410.08069"
}

@ARTICLE{Stanislav2025-ny,
  title         = "Direct Ascent Synthesis: Revealing hidden generative
                   capabilities in discriminative models",
  author        = "Stanislav, Fort and Jonathan, Whitaker",
  journal       = "arXiv [cs.CV]",
  abstract      = "We demonstrate that discriminative models inherently contain
                   powerful generative capabilities, challenging the fundamental
                   distinction between discriminative and generative
                   architectures. Our method, Direct Ascent Synthesis (DAS),
                   reveals these latent capabilities through multi-resolution
                   optimization of CLIP model representations. While traditional
                   inversion attempts produce adversarial patterns, DAS achieves
                   high-quality image synthesis by decomposing optimization
                   across multiple spatial scales (1x1 to 224x224), requiring no
                   additional training. This approach not only enables diverse
                   applications -- from text-to-image generation to style
                   transfer -- but maintains natural image statistics ($1/f^2$
                   spectrum) and guides the generation away from non-robust
                   adversarial patterns. Our results demonstrate that standard
                   discriminative models encode substantially richer generative
                   knowledge than previously recognized, providing new
                   perspectives on model interpretability and the relationship
                   between adversarial examples and natural image synthesis.",
  month         =  "11~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.07753"
}

@ARTICLE{Francesco2025-md,
  title         = "Comparing pass-through quality of mixed reality devices: A
                   user experience study during real-world tasks",
  author        = "Francesco, Vona and Julia, Schorlemmer and Michael, Stern and
                   Navid, Ashrafi and Maurizio, Vergari and Tanja, Kojic and
                   Jan-Niklas, Voigt-Antons",
  journal       = "arXiv [cs.HC]",
  abstract      = "In extended reality, pass-through enables users to view their
                   real-world surroundings via cameras on the headset,
                   displaying live video inside the device. This study compared
                   the pass-through quality of three devices: Apple Vision Pro,
                   Meta Quest 3, and Varjo XR3. Thirtyone participants performed
                   two tasks, reading a text and solving a puzzle, while using
                   each headset with the pass-through feature activated.
                   Participants then rated their experiences, focusing on
                   workload and cybersickness. Results showed that the Apple
                   Vision Pro outperformed the Meta Quest 3 and Varjo XR3,
                   receiving the highest ratings for pass-through quality.",
  month         =  "10~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2502.06382"
}

@ARTICLE{Xiao2025-bj,
  title         = "Understanding representation dynamics of diffusion models via
                   low-dimensional modeling",
  author        = "Xiao, Li and Zekai, Zhang and Xiang, Li and Siyi, Chen and
                   Zhihui, Zhu and Peng, Wang and Qing, Qu",
  journal       = "arXiv [cs.LG]",
  abstract      = "This work addresses the critical question of why and when
                   diffusion models, despite being designed for generative
                   tasks, can excel at learning high-quality representations in
                   a self-supervised manner. To address this, we develop a
                   mathematical framework based on a low-dimensional data model
                   and posterior estimation, revealing a fundamental trade-off
                   between generation and representation quality near the final
                   stage of image generation. Our analysis explains the unimodal
                   representation dynamics across noise scales, mainly driven by
                   the interplay between data denoising and class specification.
                   Building on these insights, we propose an ensemble method
                   that aggregates features across noise levels, significantly
                   improving both clean performance and robustness under label
                   noise. Extensive experiments on both synthetic and real-world
                   datasets validate our findings.",
  month         =  "9~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2502.05743"
}

@ARTICLE{Shoufa2025-iw,
  title         = "Goku: Flow based video generative foundation models",
  author        = "Shoufa, Chen and Chongjian, Ge and Yuqi, Zhang and Yida,
                   Zhang and Fengda, Zhu and Hao, Yang and Hongxiang, Hao and
                   Hui, Wu and Zhichao, Lai and Yifei, Hu and Ting-Che, Lin and
                   Shilong, Zhang and Fu, Li and Chuan, Li and Xing, Wang and
                   Yanghua, Peng and Peize, Sun and Ping, Luo and Yi, Jiang and
                   Zehuan, Yuan and Bingyue, Peng and Xiaobing, Liu",
  journal       = "arXiv [cs.CV]",
  abstract      = "This paper introduces Goku, a state-of-the-art family of
                   joint image-and-video generation models leveraging rectified
                   flow Transformers to achieve industry-leading performance. We
                   detail the foundational elements enabling high-quality visual
                   generation, including the data curation pipeline, model
                   architecture design, flow formulation, and advanced
                   infrastructure for efficient and robust large-scale training.
                   The Goku models demonstrate superior performance in both
                   qualitative and quantitative evaluations, setting new
                   benchmarks across major tasks. Specifically, Goku achieves
                   0.76 on GenEval and 83.65 on DPG-Bench for text-to-image
                   generation, and 84.85 on VBench for text-to-video tasks. We
                   believe that this work provides valuable insights and
                   practical advancements for the research community in
                   developing joint image-and-video generation models.",
  month         =  "7~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.04896"
}

@ARTICLE{Yujin2025-di,
  title         = "Can diffusion models learn hidden inter-feature rules behind
                   images?",
  author        = "Yujin, Han and Andi, Han and Wei, Huang and Chaochao, Lu and
                   Difan, Zou",
  journal       = "arXiv [cs.CV]",
  abstract      = "Despite the remarkable success of diffusion models (DMs) in
                   data generation, they exhibit specific failure cases with
                   unsatisfactory outputs. We focus on one such limitation: the
                   ability of DMs to learn hidden rules between image features.
                   Specifically, for image data with dependent features
                   ($\mathbf{x}$) and ($\mathbf{y}$) (e.g., the height of the
                   sun ($\mathbf{x}$) and the length of the shadow
                   ($\mathbf{y}$)), we investigate whether DMs can accurately
                   capture the inter-feature rule ($p(\mathbf{y}|\mathbf{x})$).
                   Empirical evaluations on mainstream DMs (e.g., Stable
                   Diffusion 3.5) reveal consistent failures, such as
                   inconsistent lighting-shadow relationships and mismatched
                   object-mirror reflections. Inspired by these findings, we
                   design four synthetic tasks with strongly correlated features
                   to assess DMs' rule-learning abilities. Extensive experiments
                   show that while DMs can identify coarse-grained rules, they
                   struggle with fine-grained ones. Our theoretical analysis
                   demonstrates that DMs trained via denoising score matching
                   (DSM) exhibit constant errors in learning hidden rules, as
                   the DSM objective is not compatible with rule conformity. To
                   mitigate this, we introduce a common technique -
                   incorporating additional classifier guidance during sampling,
                   which achieves (limited) improvements. Our analysis reveals
                   that the subtle signals of fine-grained rules are challenging
                   for the classifier to capture, providing insights for future
                   exploration.",
  month         =  "7~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.04725",
  keywords      = "\_To\_read;Project/Density\_IQA"
}

@INPROCEEDINGS{Tran2024-tp,
  title     = "A survey on measuring presence in mixed reality",
  author    = "Tran, Tanh Quang and Langlotz, Tobias and Regenbrecht, Holger",
  booktitle = "Proceedings of the CHI Conference on Human Factors in Computing
               Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  volume    =  77,
  pages     = "1--38",
  month     =  "11~" # may,
  year      =  2024,
  keywords  = "\_To\_read",
  doi       = "10.1145/3613904.3642383",
  language  = "en"
}

@ARTICLE{Goel2025-yn,
  title    = "Great Models Think Alike and this Undermines {AI} Oversight",
  author   = "Goel, Shashwat and Struber, Joschka and Auzina, Ilze Amanda and
              Chandra, Karuna K and Kumaraguru, Ponnurangam and Kiela, Douwe and
              Prabhu, Ameya and Bethge, Matthias and Geiping, Jonas",
  abstract = "As Language Model (LM) capabilities advance, evaluating and
              supervising them at scale is getting harder for humans. There is
              hope that other language models can automate both these tasks,
              which we refer to as ``AI Oversight''. We study how model
              similarity affects both aspects of AI oversight by proposing a
              probabilistic metric for LM similarity based on overlap in model
              mistakes. Using this metric, we first show that LLM-as-a-judge
              scores favor models similar to the judge, generalizing recent
              self-preference results. Then, we study training on LM
              annotations, and find complementary knowledge between the weak
              supervisor and strong student model plays a crucial role in gains
              from ``weak-to-strong generalization''. As model capabilities
              increase, it becomes harder to find their mistakes, and we might
              defer more to AI oversight. However, we observe a concerning trend
              -- model mistakes are becoming more similar with increasing
              capabilities, pointing to risks from correlated failures. Our work
              underscores the importance of reporting and correcting for model
              similarity, especially in the emerging paradigm of AI oversight.",
  month    =  "6~" # feb,
  year     =  2025,
  eprint   = "2502.04313"
}

@ARTICLE{Alec2025-yp,
  title         = "{ConceptAttention}: Diffusion transformers learn highly
                   interpretable features",
  author        = "Alec, Helbling and Meral, Tuna Han Salih and Ben, Hoover and
                   Pinar, Yanardag and Chau, Duen Horng",
  journal       = "arXiv [cs.CV]",
  abstract      = "Do the rich representations of multi-modal diffusion
                   transformers (DiTs) exhibit unique properties that enhance
                   their interpretability? We introduce ConceptAttention, a
                   novel method that leverages the expressive power of DiT
                   attention layers to generate high-quality saliency maps that
                   precisely locate textual concepts within images. Without
                   requiring additional training, ConceptAttention repurposes
                   the parameters of DiT attention layers to produce highly
                   contextualized concept embeddings, contributing the major
                   discovery that performing linear projections in the output
                   space of DiT attention layers yields significantly sharper
                   saliency maps compared to commonly used cross-attention
                   mechanisms. Remarkably, ConceptAttention even achieves
                   state-of-the-art performance on zero-shot image segmentation
                   benchmarks, outperforming 11 other zero-shot interpretability
                   methods on the ImageNet-Segmentation dataset and on a
                   single-class subset of PascalVOC. Our work contributes the
                   first evidence that the representations of multi-modal DiT
                   models like Flux are highly transferable to vision tasks like
                   segmentation, even outperforming multi-modal foundation
                   models like CLIP.",
  month         =  "6~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.04320"
}

@ARTICLE{Farahat2025-ty,
  title    = "Neural responses in early, but not late, visual cortex are well
              predicted by random-weight {CNNs} with sufficient model complexity",
  author   = "Farahat, Amr and Vinck, Martin",
  journal  = "bioRxiv",
  pages    = "2025.02.05.636721",
  abstract = "Convolutional neural networks (CNNs) were inspired by the
              organization of the primate visual system, and in turn have become
              effective models of the visual cortex, allowing for accurate
              predictions of neural stimulus responses. While training CNNs on
              brain-relevant object-recognition tasks may be an important
              pre-requisite to predict brain activity, the CNN's brain-like
              architecture alone may already allow for accurate prediction of
              neural activity. Here, we evaluated the performance of both
              task-optimized and brain-optimized convolutional neural networks
              (CNNs) in predicting neural responses across visual cortex, and
              performed systematic architectural manipulations and comparisons
              between trained and untrained feature extractors to reveal key
              structural components influencing model performance. For human and
              monkey area V1, random-weight CNNs employing the ReLU activation
              function, combined with either average or max pooling,
              significantly outperformed other activation functions.
              Random-weight CNNs matched their trained counterparts in
              predicting V1 responses. The extent to which V1 responses can be
              predicted correlated strongly with the neural network's
              complexity, which reflects the non-linearity of neural activation
              functions and pooling operations. However, this correlation
              between encoding performance and complexity was significantly
              weaker for higher visual areas that are classically associated
              with object recognition, such as monkey IT. To test whether this
              difference between visual areas reflects functional differences,
              we trained neural network models on both texture discrimination
              and object recognition tasks. \%, and analyzed the relationship
              between model complexity and task performance. Consistent with our
              hypothesis, model complexity correlated more strongly with
              performance on texture discrimination than object recognition. Our
              findings indicate that random-weight CNNs with sufficient model
              complexity allow for comparable prediction of V1 activity as
              trained CNNs, while higher visual areas require precise weight
              configurations acquired through training via gradient descent.
              \#\#\# Competing Interest Statement The authors have declared no
              competing interest.",
  month    =  "6~" # feb,
  year     =  2025,
  doi      = "10.1101/2025.02.05.636721",
  language = "en"
}

@ARTICLE{Rajabi2025-vc,
  title         = "Human-aligned image models improve visual decoding from the
                   brain",
  author        = "Rajabi, Nona and H Ribeiro, Antônio and Vasco, Miguel and
                   Taleb, Farzaneh and Björkman, Mårten and Kragic, Danica",
  journal       = "arXiv [cs.CV]",
  abstract      = "Decoding visual images from brain activity has significant
                   potential for advancing brain-computer interaction and
                   enhancing the understanding of human perception. Recent
                   approaches align the representation spaces of images and
                   brain activity to enable visual decoding. In this paper, we
                   introduce the use of human-aligned image encoders to map
                   brain signals to images. We hypothesize that these models
                   more effectively capture perceptual attributes associated
                   with the rapid visual stimuli presentations commonly used in
                   visual brain data recording experiments. Our empirical
                   results support this hypothesis, demonstrating that this
                   simple modification improves image retrieval accuracy by up
                   to 21\% compared to state-of-the-art methods. Comprehensive
                   experiments confirm consistent performance improvements
                   across diverse EEG architectures, image encoders, alignment
                   methods, participants, and brain imaging modalities.",
  month         =  "5~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.03081"
}

@ARTICLE{Tragenap2025-sj,
  title     = "The developmental emergence of reliable cortical representations",
  author    = "Trägenap, Sigrid and Whitney, David E and Fitzpatrick, David and
               Kaschube, Matthias",
  journal   = "Nature neuroscience",
  publisher = "Springer Science and Business Media LLC",
  pages     = "1--12",
  abstract  = "The fundamental structure of cortical networks arises early in
               development before the onset of sensory experience. However, how
               endogenously generated networks respond to the onset of sensory
               experience and how they form mature sensory representations with
               experience remain unclear. In this study, we examined this
               ‘nature–nurture transform’ at the single-trial level using
               chronic in vivo calcium imaging in ferret visual cortex. At eye
               opening, visual stimulation evokes robust patterns of modular
               cortical network activity that are highly variable within and
               across trials, severely limiting stimulus discriminability. These
               initial stimulus-evoked modular patterns are distinct from
               spontaneous network activity patterns present before and at the
               time of eye opening. Within a week of normal visual experience,
               cortical networks develop low-dimensional, highly reliable
               stimulus representations that correspond with reorganized
               patterns of spontaneous activity. Using a computational model, we
               propose that reliable visual representations derive from the
               alignment of feedforward and recurrent cortical networks shaped
               by novel patterns of visually driven activity. Sensory experience
               transforms endogenously structured cortical networks with diverse
               and unreliable visual responses into reliable representations.
               This process is proposed to involve the alignment of feedforward
               and recurrent networks.",
  month     =  "4~" # feb,
  year      =  2025,
  doi       = "10.1038/s41593-024-01857-3",
  issn      = "1097-6256,1546-1726",
  language  = "en"
}

@ARTICLE{Chefer2025-ob,
  title         = "{VideoJAM}: Joint appearance-motion representations for
                   enhanced motion generation in video models",
  author        = "Chefer, Hila and Singer, Uriel and Zohar, Amit and Kirstain,
                   Yuval and Polyak, Adam and Taigman, Yaniv and Wolf, Lior and
                   Sheynin, Shelly",
  journal       = "arXiv [cs.CV]",
  abstract      = "Despite tremendous recent progress, generative video models
                   still struggle to capture real-world motion, dynamics, and
                   physics. We show that this limitation arises from the
                   conventional pixel reconstruction objective, which biases
                   models toward appearance fidelity at the expense of motion
                   coherence. To address this, we introduce VideoJAM, a novel
                   framework that instills an effective motion prior to video
                   generators, by encouraging the model to learn a joint
                   appearance-motion representation. VideoJAM is composed of two
                   complementary units. During training, we extend the objective
                   to predict both the generated pixels and their corresponding
                   motion from a single learned representation. During
                   inference, we introduce Inner-Guidance, a mechanism that
                   steers the generation toward coherent motion by leveraging
                   the model's own evolving motion prediction as a dynamic
                   guidance signal. Notably, our framework can be applied to any
                   video model with minimal adaptations, requiring no
                   modifications to the training data or scaling of the model.
                   VideoJAM achieves state-of-the-art performance in motion
                   coherence, surpassing highly competitive proprietary models
                   while also enhancing the perceived visual quality of the
                   generations. These findings emphasize that appearance and
                   motion can be complementary and, when effectively integrated,
                   enhance both the visual quality and the coherence of video
                   generation. Project website:
                   https://hila-chefer.github.io/videojam-paper.github.io/",
  month         =  "4~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.02492"
}

@ARTICLE{De_Moreau2025-ss,
  title         = "{DOC}-Depth: A novel approach for dense depth ground truth
                   generation",
  author        = "de Moreau, Simon and Mathias, Corsia and Hassan, Bouchiba and
                   Yasser, Almehio and Andrei, Bursuc and Hafid, El-Idrissi and
                   Fabien, Moutarde",
  journal       = "arXiv [cs.CV]",
  abstract      = "Accurate depth information is essential for many computer
                   vision applications. Yet, no available dataset recording
                   method allows for fully dense accurate depth estimation in a
                   large scale dynamic environment. In this paper, we introduce
                   DOC-Depth, a novel, efficient and easy-to-deploy approach for
                   dense depth generation from any LiDAR sensor. After
                   reconstructing consistent dense 3D environment using LiDAR
                   odometry, we address dynamic objects occlusions automatically
                   thanks to DOC, our state-of-the art dynamic object
                   classification method. Additionally, DOC-Depth is fast and
                   scalable, allowing for the creation of unbounded datasets in
                   terms of size and time. We demonstrate the effectiveness of
                   our approach on the KITTI dataset, improving its density from
                   16.1\% to 71.2\% and release this new fully dense depth
                   annotation, to facilitate future research in the domain. We
                   also showcase results using various LiDAR sensors and in
                   multiple environments. All software components are publicly
                   available for the research community.",
  month         =  "4~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.02144"
}

@ARTICLE{Jingming2025-hq,
  title         = "Leveraging Stable Diffusion for monocular depth estimation
                   via image semantic encoding",
  author        = "Jingming, Xia and Guanqun, Cao and Guang, Ma and Yiben, Luo
                   and Qinzhao, Li and John, Oyekan",
  journal       = "arXiv [cs.CV]",
  abstract      = "Monocular depth estimation involves predicting depth from a
                   single RGB image and plays a crucial role in applications
                   such as autonomous driving, robotic navigation, 3D
                   reconstruction, etc. Recent advancements in learning-based
                   methods have significantly improved depth estimation
                   performance. Generative models, particularly Stable
                   Diffusion, have shown remarkable potential in recovering fine
                   details and reconstructing missing regions through
                   large-scale training on diverse datasets. However, models
                   like CLIP, which rely on textual embeddings, face limitations
                   in complex outdoor environments where rich context
                   information is needed. These limitations reduce their
                   effectiveness in such challenging scenarios. Here, we propose
                   a novel image-based semantic embedding that extracts
                   contextual information directly from visual features,
                   significantly improving depth prediction in complex
                   environments. Evaluated on the KITTI and Waymo datasets, our
                   method achieves performance comparable to state-of-the-art
                   models while addressing the shortcomings of CLIP embeddings
                   in handling outdoor scenes. By leveraging visual semantics
                   directly, our method demonstrates enhanced robustness and
                   adaptability in depth estimation tasks, showcasing its
                   potential for application to other visual perception tasks.",
  month         =  "1~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.01666"
}

@ARTICLE{Babaiee2024-rf,
  title         = "The master key filters hypothesis: Deep filters are general",
  author        = "Babaiee, Zahra and M Kiasari, Peyman and Rus, Daniela and
                   Grosu, Radu",
  journal       = "arXiv [cs.CV]",
  abstract      = "This paper challenges the prevailing view that convolutional
                   neural network (CNN) filters become increasingly specialized
                   in deeper layers. Motivated by recent observations of
                   clusterable repeating patterns in depthwise separable CNNs
                   (DS-CNNs) trained on ImageNet, we extend this investigation
                   across various domains and datasets. Our analysis of DS-CNNs
                   reveals that deep filters maintain generality, contradicting
                   the expected transition to class-specific filters. We
                   demonstrate the generalizability of these filters through
                   transfer learning experiments, showing that frozen filters
                   from models trained on different datasets perform well and
                   can be further improved when sourced from larger datasets.
                   Our findings indicate that spatial features learned by
                   depthwise separable convolutions remain generic across all
                   layers, domains, and architectures. This research provides
                   new insights into the nature of generalization in neural
                   networks, particularly in DS-CNNs, and has significant
                   implications for transfer learning and model design.",
  month         =  "21~" # dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2412.16751"
}

@ARTICLE{Ollikka2024-at,
  title         = "A comparison between humans and {AI} at recognizing objects
                   in unusual poses",
  author        = "Ollikka, Netta and Abbas, Amro and Perin, Andrea and
                   Kilpeläinen, Markku and Deny, Stephane",
  journal       = "arXiv [cs.CV]",
  abstract      = "Deep learning is closing the gap with human vision on several
                   object recognition benchmarks. Here we investigate this gap
                   for challenging images where objects are seen in unusual
                   poses. We find that humans excel at recognizing objects in
                   such poses. In contrast, state-of-the-art deep networks for
                   vision (EfficientNet, SWAG, ViT, SWIN, BEiT, ConvNext) and
                   state-of-the-art large vision-language models (Claude 3.5,
                   Gemini 1.5, GPT-4) are systematically brittle on unusual
                   poses, with the exception of Gemini showing excellent
                   robustness in that condition. As we limit image exposure
                   time, human performance degrades to the level of deep
                   networks, suggesting that additional mental processes
                   (requiring additional time) are necessary to identify objects
                   in unusual poses. An analysis of error patterns of humans vs.
                   networks reveals that even time-limited humans are dissimilar
                   to feed-forward deep networks. In conclusion, our comparison
                   reveals that humans and deep networks rely on different
                   mechanisms for recognizing objects in unusual poses.
                   Understanding the nature of the mental processes taking place
                   during extra viewing time may be key to reproduce the
                   robustness of human vision in silico.",
  month         =  "6~" # feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2402.03973",
  doi           = "10.31234/osf.io/z95t7"
}

@ARTICLE{Gandikota2025-oz,
  title         = "{SliderSpace}: Decomposing the visual capabilities of
                   diffusion models",
  author        = "Gandikota, Rohit and Wu, Zongze and Zhang, Richard and Bau,
                   David and Shechtman, Eli and Kolkin, Nick",
  journal       = "arXiv [cs.CV]",
  abstract      = "We present SliderSpace, a framework for automatically
                   decomposing the visual capabilities of diffusion models into
                   controllable and human-understandable directions. Unlike
                   existing control methods that require a user to specify
                   attributes for each edit direction individually, SliderSpace
                   discovers multiple interpretable and diverse directions
                   simultaneously from a single text prompt. Each direction is
                   trained as a low-rank adaptor, enabling compositional control
                   and the discovery of surprising possibilities in the model's
                   latent space. Through extensive experiments on
                   state-of-the-art diffusion models, we demonstrate
                   SliderSpace's effectiveness across three applications:
                   concept decomposition, artistic style exploration, and
                   diversity enhancement. Our quantitative evaluation shows that
                   SliderSpace-discovered directions decompose the visual
                   structure of model's knowledge effectively, offering insights
                   into the latent capabilities encoded within diffusion models.
                   User studies further validate that our method produces more
                   diverse and useful variations compared to baselines. Our
                   code, data and trained weights are available at
                   https://sliderspace.baulab.info",
  month         =  "3~" # feb,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2502.01639"
}

@ARTICLE{Raja2024-cc,
  title         = "Learning human-aligned representations with contrastive
                   learning and generative similarity",
  author        = "Raja, Marjieh and Sreejan, Kumar and Declan, Campbell and
                   Liyi, Zhang and Gianluca, Bencomo and Jake, Snell and Thomas,
                   L Griffiths",
  journal       = "arXiv [cs.LG]",
  abstract      = "Humans rely on effective representations to learn from few
                   examples and abstract useful information from sensory data.
                   Inducing such representations in machine learning models has
                   been shown to improve their performance on various benchmarks
                   such as few-shot learning and robustness. However, finding
                   effective training procedures to achieve that goal can be
                   challenging as psychologically rich training data such as
                   human similarity judgments are expensive to scale, and
                   Bayesian models of human inductive biases are often
                   intractable for complex, realistic domains. Here, we address
                   this challenge by leveraging a Bayesian notion of generative
                   similarity whereby two data points are considered similar if
                   they are likely to have been sampled from the same
                   distribution. This measure can be applied to complex
                   generative processes, including probabilistic programs. We
                   incorporate generative similarity into a contrastive learning
                   objective to enable learning of embeddings that express human
                   cognitive representations. We demonstrate the utility of our
                   approach by showing that it can be used to capture human-like
                   representations of shape regularity, abstract Euclidean
                   geometric concepts, and semantic hierarchies for natural
                   images.",
  month         =  "29~" # may,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2405.19420"
}

@ARTICLE{Zhengqi2025-qo,
  title         = "{REG}: Rectified gradient guidance for conditional diffusion
                   models",
  author        = "Zhengqi, Gao and Kaiwen, Zha and Tianyuan, Zhang and Zihui,
                   Xue and Duane, S Boning",
  journal       = "arXiv [cs.CV]",
  abstract      = "Guidance techniques are simple yet effective for improving
                   conditional generation in diffusion models. Albeit their
                   empirical success, the practical implementation of guidance
                   diverges significantly from its theoretical motivation. In
                   this paper, we reconcile this discrepancy by replacing the
                   scaled marginal distribution target, which we prove
                   theoretically invalid, with a valid scaled joint distribution
                   objective. Additionally, we show that the established
                   guidance implementations are approximations to the
                   intractable optimal solution under no future foresight
                   constraint. Building on these theoretical insights, we
                   propose rectified gradient guidance (REG), a versatile
                   enhancement designed to boost the performance of existing
                   guidance methods. Experiments on 1D and 2D demonstrate that
                   REG provides a better approximation to the optimal solution
                   than prior guidance techniques, validating the proposed
                   theoretical framework. Extensive experiments on
                   class-conditional ImageNet and text-to-image generation tasks
                   show that incorporating REG consistently improves FID and
                   Inception/CLIP scores across various settings compared to its
                   absence.",
  month         =  "31~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.18865"
}

@ARTICLE{Ping-Rui2025-ti,
  title         = "Three Laws of Statistical Linguistics Emerging in images",
  author        = "Ping-Rui, Tsai and Chi-hsiang, Wang and Yu-Cheng, Liao and
                   Tzay-Ming, Hong",
  journal       = "arXiv [cs.CV]",
  abstract      = "Images, as a product evolving alongside civilization, develop
                   similarly to natural languages with the advancement of
                   civilization. Not only are images abundant in daily life, but
                   are also influenced by technology in shaping their forms,
                   embodying various characteristics as they evolve in time.
                   Language is a sequence of symbols that represents thoughts.
                   While a written language is typically associated with the
                   close integration of text and sound, as a combination of
                   visual symbols and perception, the communicative power of
                   image is no less significant. This is especially notable
                   since 60\% of the sensory input received by our central
                   nervous system comes from vision. Given the symbolic system
                   inherent in images, we are curious whether images can also
                   exhibit the laws of statistical linguistics. To explore this,
                   we begin with the relationship between human thought and
                   visual perception to decode how images are formed by the
                   latter mechanism. Building upon previous studies that
                   established the high correlation between pre-trained deep
                   convolutional neural networks and the human visual system, we
                   use the VGG-19 to define words via each kernel and calculate
                   the number of pixels with grayscale values greater than 90\%.
                   By (a) ranking words frequency, (b) randomizing the order of
                   kernel appearances and performing the same word count
                   accumulation, and (c) summing the word counts layer by layer,
                   we are surprised to find that Zipf's, Heaps', and Benford's
                   laws of statistical linguistics also exist in the words that
                   comprises the text representing different images.",
  month         =  "26~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.18620"
}

@ARTICLE{Cherian2025-gi,
  title    = "Do monkeys see the way we do? Qualitative similarities and
              differences between monkey and human perception",
  author   = "Cherian, Thomas and Jacob, Georgin and Arun, S P",
  journal  = "bioRxiv",
  pages    = "2025.01.30.635614",
  abstract = "Monkeys are widely used as model organisms for studying vision and
              cognition. While their anatomy and physiology have strong
              correspondences with humans, it is unclear to what extent their
              perception matches with ours. Most previous studies evaluated only
              specific aspects of perception, often after extensive training on
              customised tasks that could have altered their perception. To
              resolve these issues, we investigated a broad array of perceptual
              phenomena in monkeys using the same oddball visual search task,
              negating task related confounds, and compared them with human
              performance on the same tasks. This revealed a number of
              interesting qualitative similarities and differences between
              monkeys and humans. Like humans, monkeys showed similar object
              relations, Weber's law, and amodal completion. However, unlike
              humans, monkeys did not show mirror confusion or global advantage.
              These findings represent a first comprehensive evaluation of
              visual perception in monkeys and humans, revealing the limitations
              of monkeys as a model organism for human vision.",
  month    =  "31~" # jan,
  year     =  2025,
  doi      = "10.1101/2025.01.30.635614",
  language = "en"
}

@ARTICLE{Sophie2025-ou,
  title         = "Towards understanding depth perception in foveated rendering",
  author        = "Sophie, Kergaßner and Taimoor, Tariq and Piotr, Didyk",
  journal       = "arXiv [cs.CV]",
  abstract      = "The true vision for real-time virtual and augmented reality
                   is reproducing our visual reality in its entirety on
                   immersive displays. To this end, foveated rendering leverages
                   the limitations of spatial acuity in human peripheral vision
                   to allocate computational resources to the fovea while
                   reducing quality in the periphery. Such methods are often
                   derived from studies on the spatial resolution of the human
                   visual system and its ability to perceive blur in the
                   periphery, enabling the potential for high spatial quality in
                   real-time. However, the effects of blur on other visual cues
                   that depend on luminance contrast, such as depth, remain
                   largely unexplored. It is critical to understand this
                   interplay, as accurate depth representation is a fundamental
                   aspect of visual realism. In this paper, we present the first
                   evaluation exploring the effects of foveated rendering on
                   stereoscopic depth perception. We design a psychovisual
                   experiment to quantitatively study the effects of peripheral
                   blur on depth perception. Our analysis demonstrates that
                   stereoscopic acuity remains unaffected (or even improves) by
                   high levels of peripheral blur. Based on our studies, we
                   derive a simple perceptual model that determines the amount
                   of foveation that does not affect stereoacuity. Furthermore,
                   we analyze the model in the context of common foveation
                   practices reported in literature. The findings indicate that
                   foveated rendering does not impact stereoscopic depth
                   perception, and stereoacuity remains unaffected up to 2x
                   stronger foveation than commonly used. Finally, we conduct a
                   validation experiment and show that our findings hold for
                   complex natural stimuli.",
  month         =  "28~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.18635",
  keywords      = "\_To\_read"
}

@ARTICLE{Kailas2024-yg,
  title         = "Dual thinking and logical processing -- are multi-modal large
                   language models closing the gap with human vision ?",
  author        = "Kailas, Dayanandan and Nikhil, Kumar and Anand, Sinha and
                   Brejesh, Lall",
  journal       = "arXiv [cs.CV]",
  abstract      = "The dual thinking framework considers fast, intuitive
                   processing and slower, logical processing. The perception of
                   dual thinking in vision requires images where inferences from
                   intuitive and logical processing differ. We introduce an
                   adversarial dataset to provide evidence for the dual thinking
                   framework in human vision, which also aids in studying the
                   qualitative behavior of deep learning models. The evidence
                   underscores the importance of shape in identifying instances
                   in human vision. Our psychophysical studies show the presence
                   of multiple inferences in rapid succession, and analysis of
                   errors shows the early stopping of visual processing can
                   result in missing relevant information. Our study shows that
                   segmentation models lack an understanding of sub-structures,
                   as indicated by errors related to the position and number of
                   sub-components. Additionally, the similarity in errors made
                   by models and intuitive human processing indicates that
                   models only address intuitive thinking in human vision. In
                   contrast, multi-modal LLMs, including open-source models,
                   demonstrate tremendous progress on errors made in intuitive
                   processing. The models have improved performance on images
                   that require logical reasoning and show recognition of
                   sub-components. However, they have not matched the
                   performance improvements made on errors in intuitive
                   processing.",
  month         =  "11~" # jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2406.06967"
}

@ARTICLE{Yinbo2025-wy,
  title         = "Diffusion Autoencoders are Scalable Image Tokenizers",
  author        = "Yinbo, Chen and Rohit, Girdhar and Xiaolong, Wang and
                   Rambhatla, Sai Saketh and Ishan, Misra",
  journal       = "arXiv [cs.CV]",
  abstract      = "Tokenizing images into compact visual representations is a
                   key step in learning efficient and high-quality image
                   generative models. We present a simple diffusion tokenizer
                   (DiTo) that learns compact visual representations for image
                   generation models. Our key insight is that a single learning
                   objective, diffusion L2 loss, can be used for training
                   scalable image tokenizers. Since diffusion is already widely
                   used for image generation, our insight greatly simplifies
                   training such tokenizers. In contrast, current
                   state-of-the-art tokenizers rely on an empirically found
                   combination of heuristics and losses, thus requiring a
                   complex training recipe that relies on non-trivially
                   balancing different losses and pretrained supervised models.
                   We show design decisions, along with theoretical grounding,
                   that enable us to scale DiTo for learning competitive image
                   representations. Our results show that DiTo is a simpler,
                   scalable, and self-supervised alternative to the current
                   state-of-the-art image tokenizer which is supervised. DiTo
                   achieves competitive or better quality than state-of-the-art
                   in image reconstruction and downstream image generation
                   tasks.",
  month         =  "30~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.18593"
}

@ARTICLE{Ruofan2025-fv,
  title         = "{DiffusionRenderer}: Neural inverse and forward rendering
                   with video diffusion models",
  author        = "Ruofan, Liang and Zan, Gojcic and Huan, Ling and Jacob,
                   Munkberg and Jon, Hasselgren and Zhi-Hao, Lin and Jun, Gao
                   and Alexander, Keller and Nandita, Vijaykumar and Sanja,
                   Fidler and Zian, Wang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Understanding and modeling lighting effects are fundamental
                   tasks in computer vision and graphics. Classic
                   physically-based rendering (PBR) accurately simulates the
                   light transport, but relies on precise scene
                   representations--explicit 3D geometry, high-quality material
                   properties, and lighting conditions--that are often
                   impractical to obtain in real-world scenarios. Therefore, we
                   introduce DiffusionRenderer, a neural approach that addresses
                   the dual problem of inverse and forward rendering within a
                   holistic framework. Leveraging powerful video diffusion model
                   priors, the inverse rendering model accurately estimates
                   G-buffers from real-world videos, providing an interface for
                   image editing tasks, and training data for the rendering
                   model. Conversely, our rendering model generates
                   photorealistic images from G-buffers without explicit light
                   transport simulation. Experiments demonstrate that
                   DiffusionRenderer effectively approximates inverse and
                   forwards rendering, consistently outperforming the
                   state-of-the-art. Our model enables practical applications
                   from a single video input--including relighting, material
                   editing, and realistic object insertion.",
  month         =  "30~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.18590"
}

@ARTICLE{Cavanagh2024-ok,
  title     = "Using illusions to track the emergence of visual perception",
  author    = "Cavanagh, Patrick",
  journal   = "Annual review of vision science",
  publisher = "Annual Reviews",
  volume    =  10,
  number    =  1,
  pages     = "1--22",
  abstract  = "Everybody loves illusions. At times, the content on the internet
               seems to be mostly about illusions-shoes, dresses, straight lines
               looking bent. This attraction has a long history. Almost 2,000
               years ago, Ptolemy marveled at how the sail of a distant boat
               could appear convex or concave. This sense of marvel continues to
               drive our fascination with illusions; indeed, few other corners
               of science can boast of such a large reach. However, illusions
               not only draw in the crowds; they also offer insights into visual
               processes. This review starts with a simple definition of
               illusions as conflicts between perception and cognition, where
               what we see does not agree with what we believe we should see.
               This mismatch can be either because cognition has misunderstood
               how perception works or because perception has misjudged the
               visual input. It is the perceptual errors that offer the chance
               to track the development of perception across visual regions.
               Unfortunately, the effects of illusions in different brain
               regions cannot be isolated in any simple way: Top-down
               projections from attention broadcast the expected perceptual
               properties everywhere, obscuring the critical evidence of where
               the illusion and perception emerge. The second part of this
               review then highlights the roadblocks to research raised by
               attention and describes current solutions for accessing what
               illusions can offer.",
  month     =  "15~" # sep,
  year      =  2024,
  keywords  = "attention; illusions; perception",
  doi       = "10.1146/annurev-vision-103023-012730",
  pmid      =  38871345,
  issn      = "2374-4642,2374-4650",
  language  = "en"
}

@ARTICLE{Ntrougkas2025-xt,
  title         = "{P}-{TAME}: Explain any image classifier with trained
                   perturbations",
  author        = "Ntrougkas, V, Mariano and Mezaris, Vasileios and Patras,
                   Ioannis",
  journal       = "arXiv [cs.CV]",
  abstract      = "The adoption of Deep Neural Networks (DNNs) in critical
                   fields where predictions need to be accompanied by
                   justifications is hindered by their inherent black-box
                   nature. In this paper, we introduce P-TAME
                   (Perturbation-based Trainable Attention Mechanism for
                   Explanations), a model-agnostic method for explaining
                   DNN-based image classifiers. P-TAME employs an auxiliary
                   image classifier to extract features from the input image,
                   bypassing the need to tailor the explanation method to the
                   internal architecture of the backbone classifier being
                   explained. Unlike traditional perturbation-based methods,
                   which have high computational requirements, P-TAME offers an
                   efficient alternative by generating high-resolution
                   explanations in a single forward pass during inference. We
                   apply P-TAME to explain the decisions of VGG-16, ResNet-50,
                   and ViT-B-16, three distinct and widely used image
                   classifiers. Quantitative and qualitative results show that
                   our method matches or outperforms previous explainability
                   methods, including model-specific approaches. Code and
                   trained models will be released upon acceptance.",
  month         =  "29~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.17813"
}

@ARTICLE{C_Bendel2025-td,
  title         = "Solving inverse problems using diffusion with fast iterative
                   renoising",
  author        = "C Bendel, Matt and K Shastri, Saurav and Ahmad, Rizwan and
                   Schniter, Philip",
  journal       = "arXiv [cs.CV]",
  abstract      = "Imaging inverse problems can be solved in an unsupervised
                   manner using pre-trained diffusion models. In most cases,
                   that involves approximating the gradient of the
                   measurement-conditional score function in the reverse
                   process. Since the approximations produced by existing
                   methods are quite poor, especially early in the reverse
                   process, we propose a new approach that re-estimates and
                   renoises the image several times per diffusion step.
                   Renoising adds carefully shaped colored noise that ensures
                   the pre-trained diffusion model sees white-Gaussian error, in
                   accordance with how it was trained. We demonstrate the
                   effectiveness of our ``DDfire'' method at 20, 100, and 1000
                   neural function evaluations on linear inverse problems and
                   phase retrieval.",
  month         =  "29~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.17468"
}

@ARTICLE{Akan2025-go,
  title         = "Slot-guided adaptation of pre-trained diffusion models for
                   object-centric learning and compositional generation",
  author        = "Akan, Adil Kaan and Yucel, Yemez",
  journal       = "arXiv [cs.CV]",
  abstract      = "We present SlotAdapt, an object-centric learning method that
                   combines slot attention with pretrained diffusion models by
                   introducing adapters for slot-based conditioning. Our method
                   preserves the generative power of pretrained diffusion
                   models, while avoiding their text-centric conditioning bias.
                   We also incorporate an additional guidance loss into our
                   architecture to align cross-attention from adapter layers
                   with slot attention. This enhances the alignment of our model
                   with the objects in the input image without using external
                   supervision. Experimental results show that our method
                   outperforms state-of-the-art techniques in object discovery
                   and image generation tasks across multiple datasets,
                   including those with real images. Furthermore, we demonstrate
                   through experiments that our method performs remarkably well
                   on complex real-world images for compositional generation, in
                   contrast to other slot-based generative methods in the
                   literature. The project page can be found at
                   https://kaanakan.github.io/SlotAdapt/.",
  month         =  "27~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.15878"
}

@ARTICLE{Tianyi2025-zk,
  title         = "{PhysAnimator}: Physics-Guided Generative Cartoon Animation",
  author        = "Tianyi, Xie and Yiwei, Zhao and Ying, Jiang and Chenfanfu,
                   Jiang",
  journal       = "arXiv [cs.GR]",
  abstract      = "Creating hand-drawn animation sequences is labor-intensive
                   and demands professional expertise. We introduce
                   PhysAnimator, a novel approach for generating physically
                   plausible meanwhile anime-stylized animation from static
                   anime illustrations. Our method seamlessly integrates
                   physics-based simulations with data-driven generative models
                   to produce dynamic and visually compelling animations. To
                   capture the fluidity and exaggeration characteristic of
                   anime, we perform image-space deformable body simulations on
                   extracted mesh geometries. We enhance artistic control by
                   introducing customizable energy strokes and incorporating
                   rigging point support, enabling the creation of tailored
                   animation effects such as wind interactions. Finally, we
                   extract and warp sketches from the simulation sequence,
                   generating a texture-agnostic representation, and employ a
                   sketch-guided video diffusion model to synthesize
                   high-quality animation frames. The resulting animations
                   exhibit temporal consistency and visual plausibility,
                   demonstrating the effectiveness of our method in creating
                   dynamic anime-style animations.",
  month         =  "27~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.GR",
  eprint        = "2501.16550"
}

@ARTICLE{Wei2025-re,
  title         = "{PhysBench}: Benchmarking and enhancing vision-Language
                   Models for physical world understanding",
  author        = "Wei, Chow and Jiageng, Mao and Boyi, Li and Daniel, Seita and
                   Vitor, Guizilini and Yue, Wang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Understanding the physical world is a fundamental challenge
                   in embodied AI, critical for enabling agents to perform
                   complex tasks and operate safely in real-world environments.
                   While Vision-Language Models (VLMs) have shown great promise
                   in reasoning and task planning for embodied agents, their
                   ability to comprehend physical phenomena remains extremely
                   limited. To close this gap, we introduce PhysBench, a
                   comprehensive benchmark designed to evaluate VLMs' physical
                   world understanding capability across a diverse set of tasks.
                   PhysBench contains 100,000 entries of interleaved
                   video-image-text data, categorized into four major domains:
                   physical object properties, physical object relationships,
                   physical scene understanding, and physics-based dynamics,
                   further divided into 19 subclasses and 8 distinct capability
                   dimensions. Our extensive experiments, conducted on 75
                   representative VLMs, reveal that while these models excel in
                   common-sense reasoning, they struggle with understanding the
                   physical world -- likely due to the absence of physical
                   knowledge in their training data and the lack of embedded
                   physical priors. To tackle the shortfall, we introduce
                   PhysAgent, a novel framework that combines the generalization
                   strengths of VLMs with the specialized expertise of vision
                   models, significantly enhancing VLMs' physical understanding
                   across a variety of tasks, including an 18.4\% improvement on
                   GPT-4o. Furthermore, our results demonstrate that enhancing
                   VLMs' physical world understanding capabilities can help
                   embodied agents such as MOKA. We believe that PhysBench and
                   PhysAgent offer valuable insights and contribute to bridging
                   the gap between VLMs and physical world understanding.",
  month         =  "27~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.16411"
}

@ARTICLE{Luca2025-wm,
  title         = "Temporal Brightness Management for Immersive Content",
  author        = "Luca, Surace and Jorge, Condor and Piotr, Didyk",
  journal       = "arXiv [eess.IV]",
  abstract      = "Modern virtual reality headsets demand significant
                   computational resources to render high-resolution content in
                   real-time. Therefore, prioritizing power efficiency becomes
                   crucial, particularly for portable versions reliant on
                   batteries. A significant portion of the energy consumed by
                   these systems is attributed to their displays. Dimming the
                   screen can save a considerable amount of energy; however, it
                   may also result in a loss of visible details and contrast in
                   the displayed content. While contrast may be partially
                   restored by applying post-processing contrast enhancement
                   steps, our work is orthogonal to these approaches, and
                   focuses on optimal temporal modulation of screen brightness.
                   We propose a technique that modulates brightness over time
                   while minimizing the potential loss of visible details and
                   avoiding noticeable temporal instability. Given a
                   predetermined power budget and a video sequence, we achieve
                   this by measuring contrast loss through band decomposition of
                   the luminance image and optimizing the brightness level of
                   each frame offline to ensure uniform temporal contrast loss.
                   We evaluate our method through a series of subjective
                   experiments and an ablation study, on a variety of content.
                   We showcase its power-saving capabilities in practice using a
                   built-in hardware proxy. Finally, we present an online
                   version of our approach which further emphasizes the
                   potential for low level vision models to be leveraged in
                   power saving settings to preserve content quality.",
  month         =  "24~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "eess.IV",
  eprint        = "2501.14853"
}

@ARTICLE{Yifan2024-tt,
  title         = "Do vision-Language Models really understand visual language?",
  author        = "Yifan, Hou and Buse, Giledereli and Yilei, Tu and Mrinmaya,
                   Sachan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Visual language is a system of communication that conveys
                   information through symbols, shapes, and spatial
                   arrangements. Diagrams are a typical example of a visual
                   language depicting complex concepts and their relationships
                   in the form of an image. The symbolic nature of diagrams
                   presents significant challenges for building models capable
                   of understanding them. Recent studies suggest that Large
                   Vision-Language Models (LVLMs) can even tackle complex
                   reasoning tasks involving diagrams. In this paper, we
                   investigate this phenomenon by developing a comprehensive
                   test suite to evaluate the diagram comprehension capability
                   of LVLMs. Our test suite uses a variety of questions focused
                   on concept entities and their relationships over a set of
                   synthetic as well as real diagrams across domains to evaluate
                   the recognition and reasoning abilities of models. Our
                   evaluation of LVLMs shows that while they can accurately
                   identify and reason about entities, their ability to
                   understand relationships is notably limited. Further testing
                   reveals that the decent performance on diagram understanding
                   largely stems from leveraging their background knowledge as
                   shortcuts to identify and reason about the relational
                   information. Thus, we conclude that LVLMs have a limited
                   capability for genuine diagram understanding, and their
                   impressive performance in diagram reasoning is an illusion
                   emanating from other confounding factors, such as the
                   background knowledge in the models.",
  month         =  "30~" # sep,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2410.00193"
}

@ARTICLE{Jieyu2024-ut,
  title         = "Task Me Anything",
  author        = "Jieyu, Zhang and Weikai, Huang and Zixian, Ma and Oscar,
                   Michel and Dong, He and Tanmay, Gupta and Wei-Chiu, Ma and
                   Ali, Farhadi and Aniruddha, Kembhavi and Ranjay, Krishna",
  journal       = "arXiv [cs.CV]",
  abstract      = "Benchmarks for large multimodal language models (MLMs) now
                   serve to simultaneously assess the general capabilities of
                   models instead of evaluating for a specific capability. As a
                   result, when a developer wants to identify which models to
                   use for their application, they are overwhelmed by the number
                   of benchmarks and remain uncertain about which benchmark's
                   results are most reflective of their specific use case. This
                   paper introduces Task-Me-Anything, a benchmark generation
                   engine which produces a benchmark tailored to a user's needs.
                   Task-Me-Anything maintains an extendable taxonomy of visual
                   assets and can programmatically generate a vast number of
                   task instances. Additionally, it algorithmically addresses
                   user queries regarding MLM performance efficiently within a
                   computational budget. It contains 113K images, 10K videos, 2K
                   3D object assets, over 365 object categories, 655 attributes,
                   and 335 relationships. It can generate 750M image/video
                   question-answering pairs, which focus on evaluating MLM
                   perceptual capabilities. Task-Me-Anything reveals critical
                   insights: open-source MLMs excel in object and attribute
                   recognition but lack spatial and temporal understanding; each
                   model exhibits unique strengths and weaknesses; larger models
                   generally perform better, though exceptions exist; and GPT4o
                   demonstrates challenges in recognizing rotating/moving
                   objects and distinguishing colors.",
  month         =  "17~" # jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2406.11775"
}

@ARTICLE{Pingping2025-pb,
  title         = "{MAP}-based Problem-Agnostic diffusion model for Inverse
                   Problems",
  author        = "Pingping, Tao and Haixia, Liu and Jing, Su and Xiaochen, Yang
                   and Hongchen, Tan",
  journal       = "arXiv [eess.IV]",
  abstract      = "Diffusion models have indeed shown great promise in solving
                   inverse problems in image processing. In this paper, we
                   propose a novel, problem-agnostic diffusion model called the
                   maximum a posteriori (MAP)-based guided term estimation
                   method for inverse problems. We divide the conditional score
                   function into two terms according to Bayes' rule: the
                   unconditional score function and the guided term. We design
                   the MAP-based guided term estimation method, while the
                   unconditional score function is approximated by an existing
                   score network. To estimate the guided term, we base on the
                   assumption that the space of clean natural images is
                   inherently smooth, and introduce a MAP estimate of the $t$-th
                   latent variable. We then substitute this estimation into the
                   expression of the inverse problem and obtain the
                   approximation of the guided term. We evaluate our method
                   extensively on super-resolution, inpainting, and denoising
                   tasks, and demonstrate comparable performance to DDRM, DMPS,
                   DPS and $\Pi$GDM.",
  month         =  "25~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "eess.IV",
  eprint        = "2501.15128"
}

@ARTICLE{Karahan2025-kt,
  title         = "A data-centric approach: Dimensions of visual complexity and
                   how to find them",
  author        = "Karahan, Sarıtaş and Tingke, Shen and Surabhi, S Nath and
                   Peter, Dayan",
  journal       = "arXiv [cs.CV]",
  abstract      = "Understanding how humans perceive visual complexity is a key
                   area of study in visual cognition. Previous approaches to
                   modeling visual complexity have often resulted in intricate,
                   difficult-to-interpret solutions that employ numerous
                   features or sophisticated deep learning architectures. While
                   these complex models achieve high performance on specific
                   datasets, they often sacrifice interpretability, making it
                   challenging to understand the factors driving human
                   perception of complexity. A recent model based on image
                   segmentations showed promise in addressing this challenge;
                   however, it presented limitations in capturing structural and
                   semantic aspects of visual complexity. In this paper, we
                   propose viable and effective features to overcome these
                   shortcomings. Specifically, we develop multiscale features
                   for the structural aspect of complexity, including the
                   Multiscale Sobel Gradient (MSG), which captures spatial
                   intensity variations across scales, and Multiscale Unique
                   Colors (MUC), which quantifies image colorfulness by indexing
                   quantized RGB values. We also introduce a new dataset SVG
                   based on Visual Genome to explore the semantic aspect of
                   visual complexity, obtaining surprise scores based on the
                   element of surprise in images, which we demonstrate
                   significantly contributes to perceived complexity. Overall,
                   we suggest that the nature of the data is fundamental to
                   understanding and modeling visual complexity, highlighting
                   the importance of both structural and semantic dimensions in
                   providing a comprehensive, interpretable assessment. The code
                   for our analysis, experimental setup, and dataset will be
                   made publicly available upon acceptance.",
  month         =  "27~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.15890"
}

@ARTICLE{Huayu2025-jr,
  title         = "Visual generation without guidance",
  author        = "Huayu, Chen and Kai, Jiang and Kaiwen, Zheng and Jianfei,
                   Chen and Hang, Su and Jun, Zhu",
  journal       = "arXiv [cs.CV]",
  abstract      = "Classifier-Free Guidance (CFG) has been a default technique
                   in various visual generative models, yet it requires
                   inference from both conditional and unconditional models
                   during sampling. We propose to build visual models that are
                   free from guided sampling. The resulting algorithm,
                   Guidance-Free Training (GFT), matches the performance of CFG
                   while reducing sampling to a single model, halving the
                   computational cost. Unlike previous distillation-based
                   approaches that rely on pretrained CFG networks, GFT enables
                   training directly from scratch. GFT is simple to implement.
                   It retains the same maximum likelihood objective as CFG and
                   differs mainly in the parameterization of conditional models.
                   Implementing GFT requires only minimal modifications to
                   existing codebases, as most design choices and
                   hyperparameters are directly inherited from CFG. Our
                   extensive experiments across five distinct visual models
                   demonstrate the effectiveness and versatility of GFT. Across
                   domains of diffusion, autoregressive, and masked-prediction
                   modeling, GFT consistently achieves comparable or even lower
                   FID scores, with similar diversity-fidelity trade-offs
                   compared with CFG baselines, all while being guidance-free.
                   Code will be available at https://github.com/thu-ml/GFT.",
  month         =  "26~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.15420"
}

@ARTICLE{Hubert2025-xy,
  title         = "Scaling laws for decoding images from brain activity",
  author        = "Hubert, Banville and Yohann, Benchetrit and Stéphane,
                   D'ascoli and King, Jérémy Rapin Amd",
  journal       = "arXiv [eess.IV]",
  abstract      = "Generative AI has recently propelled the decoding of images
                   from brain activity. How do these approaches scale with the
                   amount and type of neural recordings? Here, we systematically
                   compare image decoding from four types of non-invasive
                   devices: electroencephalography (EEG), magnetoencephalography
                   (MEG), high-field functional Magnetic Resonance Imaging (3T
                   fMRI) and ultra-high field (7T) fMRI. For this, we evaluate
                   decoding models on the largest benchmark to date,
                   encompassing 8 public datasets, 84 volunteers, 498 hours of
                   brain recording and 2.3 million brain responses to natural
                   images. Unlike previous work, we focus on single-trial
                   decoding performance to simulate real-time settings. This
                   systematic comparison reveals three main findings. First, the
                   most precise neuroimaging devices tend to yield the best
                   decoding performances, when the size of the training sets are
                   similar. However, the gain enabled by deep learning - in
                   comparison to linear models - is obtained with the noisiest
                   devices. Second, we do not observe any plateau of decoding
                   performance as the amount of training data increases. Rather,
                   decoding performance scales log-linearly with the amount of
                   brain recording. Third, this scaling law primarily depends on
                   the amount of data per subject. However, little decoding gain
                   is observed by increasing the number of subjects. Overall,
                   these findings delineate the path most suitable to scale the
                   decoding of images from non-invasive brain recordings.",
  month         =  "25~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "eess.IV",
  eprint        = "2501.15322"
}

@ARTICLE{Gupta2025-eh,
  title     = "Human-like face pareidolia emerges in deep neural networks
               optimized for face and object recognition",
  author    = "Gupta, Pranjul and Dobs, Katharina",
  journal   = "PLoS computational biology",
  publisher = "Public Library of Science",
  volume    =  21,
  number    =  1,
  pages     = "e1012751",
  abstract  = "The human visual system possesses a remarkable ability to detect
               and process faces across diverse contexts, including the
               phenomenon of face pareidolia—–seeing faces in inanimate objects.
               Despite extensive research, it remains unclear why the visual
               system employs such broadly tuned face detection capabilities. We
               hypothesized that face pareidolia results from the visual
               system’s optimization for recognizing both faces and objects. To
               test this hypothesis, we used task-optimized deep convolutional
               neural networks (CNNs) and evaluated their alignment with human
               behavioral signatures and neural responses, measured via
               magnetoencephalography (MEG), related to pareidolia processing.
               Specifically, we trained CNNs on tasks involving combinations of
               face identification, face detection, object categorization, and
               object detection. Using representational similarity analysis, we
               found that CNNs that included object categorization in their
               training tasks represented pareidolia faces, real faces, and
               matched objects more similarly to neural responses than those
               that did not. Although these CNNs showed similar overall
               alignment with neural data, a closer examination of their
               internal representations revealed that specific training tasks
               had distinct effects on how pareidolia faces were represented
               across layers. Finally, interpretability methods revealed that
               only a CNN trained for both face identification and object
               categorization relied on face-like features—such as ‘eyes’—to
               classify pareidolia stimuli as faces, mirroring findings in human
               perception. Our results suggest that human-like face pareidolia
               may emerge from the visual system’s optimization for face
               identification within the context of generalized object
               categorization.",
  month     =  "27~" # jan,
  year      =  2025,
  doi       = "10.1371/journal.pcbi.1012751",
  issn      = "1553-7358,1553-734X"
}

@ARTICLE{Guoxi2025-ee,
  title         = "Bayesian Neural Networks for one-to-many mapping in image
                   enhancement",
  author        = "Guoxi, Huang and Nantheera, Anantrasirichai and Fei, Ye and
                   Zipeng, Qi and RuiRui, Lin and Qirui, Yang and David, Bull",
  journal       = "arXiv [cs.CV]",
  abstract      = "In image enhancement tasks, such as low-light and underwater
                   image enhancement, a degraded image can correspond to
                   multiple plausible target images due to dynamic photography
                   conditions, such as variations in illumination. This
                   naturally results in a one-to-many mapping challenge. To
                   address this, we propose a Bayesian Enhancement Model (BEM)
                   that incorporates Bayesian Neural Networks (BNNs) to capture
                   data uncertainty and produce diverse outputs. To achieve
                   real-time inference, we introduce a two-stage approach: Stage
                   I employs a BNN to model the one-to-many mappings in the
                   low-dimensional space, while Stage II refines fine-grained
                   image details using a Deterministic Neural Network (DNN). To
                   accelerate BNN training and convergence, we introduce a
                   dynamic \emph{Momentum Prior}. Extensive experiments on
                   multiple low-light and underwater image enhancement
                   benchmarks demonstrate the superiority of our method over
                   deterministic models.",
  month         =  "24~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.14265"
}

@ARTICLE{Hanrui2025-em,
  title         = "{GreedyPixel}: Fine-grained black-box adversarial attack via
                   greedy algorithm",
  author        = "Hanrui, Wang and Ching-Chun, Chang and Chun-Shien, Lu and
                   Christopher, Leckie and Isao, Echizen",
  journal       = "arXiv [cs.CV]",
  abstract      = "A critical requirement for deep learning models is ensuring
                   their robustness against adversarial attacks. These attacks
                   commonly introduce noticeable perturbations, compromising the
                   visual fidelity of adversarial examples. Another key
                   challenge is that while white-box algorithms can generate
                   effective adversarial perturbations, they require access to
                   the model gradients, limiting their practicality in many
                   real-world scenarios. Existing attack mechanisms struggle to
                   achieve similar efficacy without access to these gradients.
                   In this paper, we introduce GreedyPixel, a novel pixel-wise
                   greedy algorithm designed to generate high-quality
                   adversarial examples using only query-based feedback from the
                   target model. GreedyPixel improves computational efficiency
                   in what is typically a brute-force process by perturbing
                   individual pixels in sequence, guided by a pixel-wise
                   priority map. This priority map is constructed by ranking
                   gradients obtained from a surrogate model, providing a
                   structured path for perturbation. Our results demonstrate
                   that GreedyPixel achieves attack success rates comparable to
                   white-box methods without the need for gradient information,
                   and surpasses existing algorithms in black-box settings,
                   offering higher success rates, reduced computational time,
                   and imperceptible perturbations. These findings underscore
                   the advantages of GreedyPixel in terms of attack efficacy,
                   time efficiency, and visual quality.",
  month         =  "24~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.14230"
}

@ARTICLE{Junyeob2025-ex,
  title         = "Dreamweaver: Learning compositional world representations
                   from pixels",
  author        = "Junyeob, Baek and Yi-Fu, Wu and Gautam, Singh and Sungjin,
                   Ahn",
  journal       = "arXiv [cs.CV]",
  abstract      = "Humans have an innate ability to decompose their perceptions
                   of the world into objects and their attributes, such as
                   colors, shapes, and movement patterns. This cognitive process
                   enables us to imagine novel futures by recombining familiar
                   concepts. However, replicating this ability in artificial
                   intelligence systems has proven challenging, particularly
                   when it comes to modeling videos into compositional concepts
                   and generating unseen, recomposed futures without relying on
                   auxiliary data, such as text, masks, or bounding boxes. In
                   this paper, we propose Dreamweaver, a neural architecture
                   designed to discover hierarchical and compositional
                   representations from raw videos and generate compositional
                   future simulations. Our approach leverages a novel Recurrent
                   Block-Slot Unit (RBSU) to decompose videos into their
                   constituent objects and attributes. In addition, Dreamweaver
                   uses a multi-future-frame prediction objective to capture
                   disentangled representations for dynamic concepts more
                   effectively as well as static concepts. In experiments, we
                   demonstrate our model outperforms current state-of-the-art
                   baselines for world modeling when evaluated under the DCI
                   framework across multiple datasets. Furthermore, we show how
                   the modularized concept representations of our model enable
                   compositional imagination, allowing the generation of novel
                   videos by recombining attributes from different objects.",
  month         =  "24~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.14174"
}

@ARTICLE{Shiono2025-rp,
  title     = "Evaluating model alignment with human perception: A study on
               shitsukan in {LLMs} and {LVLMs}",
  author    = "Shiono, Daiki and Brassard, Ana and Ishizuki, Yukiko and Suzuki,
               Jun",
  editor    = "Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and
               Al-Khalifa, Hend and Eugenio, Barbara Di and Schockaert, Steven",
  journal   = "International Conference on Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Abu Dhabi, UAE",
  pages     = "11428--11444",
  abstract  = "We evaluate the alignment of large language models (LLMs) and
               large vision-language models (LVLMs) with human perception,
               focusing on the Japanese concept of *shitsukan*, which reflects
               the sensory experience of perceiving objects. We created a
               dataset of *shitsukan* terms elicited from individuals in
               response to object images. With it, we designed benchmark tasks
               for three dimensions of understanding *shitsukan*: (1) accurate
               perception in object images, (2) commonsense knowledge of typical
               *shitsukan* terms for objects, and (3) distinction of valid
               *shitsukan* terms. Models demonstrated mixed accuracy across
               benchmark tasks, with limited overlap between model- and
               human-generated terms. However, manual evaluations revealed that
               the model-generated terms were still natural to humans. This work
               identifies gaps in culture-specific understanding and contributes
               to aligning models with human sensory perception. We publicly
               release the dataset to encourage further research in this area.",
  month     =  jan,
  year      =  2025
}

@ARTICLE{Jie2025-st,
  title         = "Improving Video Generation with Human Feedback",
  author        = "Jie, Liu and Gongye, Liu and Jiajun, Liang and Ziyang, Yuan
                   and Xiaokun, Liu and Mingwu, Zheng and Xiele, Wu and Qiulin,
                   Wang and Wenyu, Qin and Menghan, Xia and Xintao, Wang and
                   Xiaohong, Liu and Fei, Yang and Pengfei, Wan and Di, Zhang
                   and Kun, Gai and Yujiu, Yang and Wanli, Ouyang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Video generation has achieved significant advances through
                   rectified flow techniques, but issues like unsmooth motion
                   and misalignment between videos and prompts persist. In this
                   work, we develop a systematic pipeline that harnesses human
                   feedback to mitigate these problems and refine the video
                   generation model. Specifically, we begin by constructing a
                   large-scale human preference dataset focused on modern video
                   generation models, incorporating pairwise annotations across
                   multi-dimensions. We then introduce VideoReward, a
                   multi-dimensional video reward model, and examine how
                   annotations and various design choices impact its rewarding
                   efficacy. From a unified reinforcement learning perspective
                   aimed at maximizing reward with KL regularization, we
                   introduce three alignment algorithms for flow-based models by
                   extending those from diffusion models. These include two
                   training-time strategies: direct preference optimization for
                   flow (Flow-DPO) and reward weighted regression for flow
                   (Flow-RWR), and an inference-time technique, Flow-NRG, which
                   applies reward guidance directly to noisy videos.
                   Experimental results indicate that VideoReward significantly
                   outperforms existing reward models, and Flow-DPO demonstrates
                   superior performance compared to both Flow-RWR and standard
                   supervised fine-tuning methods. Additionally, Flow-NRG lets
                   users assign custom weights to multiple objectives during
                   inference, meeting personalized video quality needs. Project
                   page: https://gongyeliu.github.io/videoalign.",
  month         =  "23~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.13918"
}

@ARTICLE{Haoru2024-rs,
  title         = "Aligning human motion generation with human perceptions",
  author        = "Haoru, Wang and Wentao, Zhu and Luyi, Miao and Yishu, Xu and
                   Feng, Gao and Qi, Tian and Yizhou, Wang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Human motion generation is a critical task with a wide range
                   of applications. Achieving high realism in generated motions
                   requires naturalness, smoothness, and plausibility. Despite
                   rapid advancements in the field, current generation methods
                   often fall short of these goals. Furthermore, existing
                   evaluation metrics typically rely on ground-truth-based
                   errors, simple heuristics, or distribution distances, which
                   do not align well with human perceptions of motion quality.
                   In this work, we propose a data-driven approach to bridge
                   this gap by introducing a large-scale human perceptual
                   evaluation dataset, MotionPercept, and a human motion critic
                   model, MotionCritic, that capture human perceptual
                   preferences. Our critic model offers a more accurate metric
                   for assessing motion quality and could be readily integrated
                   into the motion generation pipeline to enhance generation
                   quality. Extensive experiments demonstrate the effectiveness
                   of our approach in both evaluating and improving the quality
                   of generated human motions by aligning with human
                   perceptions. Code and data are publicly available at
                   https://motioncritic.github.io/.",
  month         =  "2~" # jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2407.02272"
}

@ARTICLE{Axel2025-co,
  title         = "Computational modelling of biological systems now and then:
                   revisiting tools and visions from the beginning of the
                   century",
  author        = "Axel, Loewe and Peter, J Hunter and Peter, Kohl",
  journal       = "arXiv [q-bio.QM]",
  abstract      = "Since the turn of the millennium, computational modelling of
                   biological systems has evolved remarkably and sees matured
                   use spanning basic and clinical research. While the topic of
                   the peri-millennial debate about the virtues and limitations
                   of 'reductionism and integrationism' seems less controversial
                   today, a new apparent dichotomy dominates discussions:
                   mechanistic vs. data-driven modelling. In light of this
                   distinction, we provide an overview of recent achievements
                   and new challenges with a focus on the cardiovascular system.
                   Attention has shifted from generating a universal model of
                   the human to either models of individual humans (digital
                   twins) or entire cohorts of models representative of clinical
                   populations to enable in silico clinical trials.
                   Disease-specific parameterisation, inter-individual and
                   intra-individual variability, uncertainty quantification as
                   well as interoperable, standardised, and quality-controlled
                   data are important issues today, which call for open tools,
                   data and metadata standards, as well as strong community
                   interactions. The quantitative, biophysical, and highly
                   controlled approach provided by in silico methods has become
                   an integral part of physiological and medical research. In
                   silico methods have the potential to accelerate future
                   progress also in the fields of integrated multi-physics
                   modelling, multi-scale models, virtual cohort studies, and
                   machine learning beyond what is feasible today. In fact,
                   mechanistic and data-driven modelling can complement each
                   other synergistically and fuel tomorrow's artificial
                   intelligence applications to further our understanding of
                   physiology and disease mechanisms, to generate new hypotheses
                   and assess their plausibility, and thus to contribute to the
                   evolution of preventive, diagnostic, and therapeutic
                   approaches.",
  month         =  "22~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.QM",
  eprint        = "2501.13142"
}

@ARTICLE{Sun2025-xg,
  title         = "Machine Learning Modeling for Multi-order Human Visual Motion
                   Processing",
  author        = "Sun, Zitang and Chen, Yen-Ju and Yang, Yung-Hao and Li, Yuan
                   and Nishida, Shin'ya",
  journal       = "arXiv [cs.CV]",
  abstract      = "Our research aims to develop machines that learn to perceive
                   visual motion as do humans. While recent advances in computer
                   vision (CV) have enabled DNN-based models to accurately
                   estimate optical flow in naturalistic images, a significant
                   disparity remains between CV models and the biological visual
                   system in both architecture and behavior. This disparity
                   includes humans' ability to perceive the motion of
                   higher-order image features (second-order motion), which many
                   CV models fail to capture because of their reliance on the
                   intensity conservation law. Our model architecture mimics the
                   cortical V1-MT motion processing pathway, utilizing a
                   trainable motion energy sensor bank and a recurrent graph
                   network. Supervised learning employing diverse naturalistic
                   videos allows the model to replicate psychophysical and
                   physiological findings about first-order (luminance-based)
                   motion perception. For second-order motion, inspired by
                   neuroscientific findings, the model includes an additional
                   sensing pathway with nonlinear preprocessing before motion
                   energy sensing, implemented using a simple multilayer 3D CNN
                   block. When exploring how the brain acquired the ability to
                   perceive second-order motion in natural environments, in
                   which pure second-order signals are rare, we hypothesized
                   that second-order mechanisms were critical when estimating
                   robust object motion amidst optical fluctuations, such as
                   highlights on glossy surfaces. We trained our dual-pathway
                   model on novel motion datasets with varying material
                   properties of moving objects. We found that training to
                   estimate object motion from non-Lambertian materials
                   naturally endowed the model with the capacity to perceive
                   second-order motion, as can humans. The resulting model
                   effectively aligns with biological systems while generalizing
                   to both first- and second-order motion phenomena in natural
                   scenes.",
  month         =  "22~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.12810"
}

@ARTICLE{Margalit2024-pl,
  title     = "A unifying framework for functional organization in early and
               higher ventral visual cortex",
  author    = "Margalit, Eshed and Lee, Hyodong and Finzi, Dawn and DiCarlo,
               James J and Grill-Spector, Kalanit and Yamins, Daniel L K",
  journal   = "Neuron",
  publisher = "Elsevier BV",
  volume    =  112,
  number    =  14,
  pages     = "2435--2451.e7",
  abstract  = "A key feature of cortical systems is functional organization: the
               arrangement of functionally distinct neurons in characteristic
               spatial patterns. However, the principles underlying the
               emergence of functional organization in the cortex are poorly
               understood. Here, we develop the topographic deep artificial
               neural network (TDANN), the first model to predict several
               aspects of the functional organization of multiple cortical areas
               in the primate visual system. We analyze the factors driving the
               TDANN's success and find that it balances two objectives:
               learning a task-general sensory representation and maximizing the
               spatial smoothness of responses according to a metric that scales
               with cortical surface area. In turn, the representations learned
               by the TDANN are more brain-like than in spatially unconstrained
               models. Finally, we provide evidence that the TDANN's functional
               organization balances performance with between-area connection
               length. Our results offer a unified principle for understanding
               the functional organization of the primate ventral visual system.",
  month     =  "17~" # jul,
  year      =  2024,
  keywords  = "dimensionality; neural network; topography; ventral visual
               cortex; vision; wiring length;Project/Metamerism;\_To\_read",
  doi       = "10.1016/j.neuron.2024.04.018",
  pmc       = "PMC11257790",
  pmid      =  38733985,
  issn      = "0896-6273,1097-4199",
  language  = "en"
}

@ARTICLE{Tariq2024-mr,
  title         = "Boosting latent diffusion with perceptual objectives",
  author        = "Tariq, Berrada and Pietro, Astolfi and Melissa, Hall and
                   Marton, Havasi and Yohann, Benchetrit and Adriana,
                   Romero-Soriano and Karteek, Alahari and Michal, Drozdzal and
                   Jakob, Verbeek",
  journal       = "arXiv [cs.CV]",
  abstract      = "Latent diffusion models (LDMs) power state-of-the-art
                   high-resolution generative image models. LDMs learn the data
                   distribution in the latent space of an autoencoder (AE) and
                   produce images by mapping the generated latents into RGB
                   image space using the AE decoder. While this approach allows
                   for efficient model training and sampling, it induces a
                   disconnect between the training of the diffusion model and
                   the decoder, resulting in a loss of detail in the generated
                   images. To remediate this disconnect, we propose to leverage
                   the internal features of the decoder to define a latent
                   perceptual loss (LPL). This loss encourages the models to
                   create sharper and more realistic images. Our loss can be
                   seamlessly integrated with common autoencoders used in latent
                   diffusion models, and can be applied to different generative
                   modeling paradigms such as DDPM with epsilon and velocity
                   prediction, as well as flow matching. Extensive experiments
                   with models trained on three datasets at 256 and 512
                   resolution show improved quantitative -- with boosts between
                   6\% and 20\% in FID -- and qualitative results when using our
                   perceptual loss.",
  month         =  "6~" # nov,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2411.04873"
}

@ARTICLE{Yiyang2025-wy,
  title         = "{DiffDoctor}: Diagnosing image diffusion models before
                   treating",
  author        = "Yiyang, Wang and Xi, Chen and Xiaogang, Xu and Sihui, Ji and
                   Yu, Liu and Yujun, Shen and Hengshuang, Zhao",
  journal       = "arXiv [cs.CV]",
  abstract      = "In spite of the recent progress, image diffusion models still
                   produce artifacts. A common solution is to refine an
                   established model with a quality assessment system, which
                   generally rates an image in its entirety. In this work, we
                   believe problem-solving starts with identification, yielding
                   the request that the model should be aware of not just the
                   presence of defects in an image, but their specific
                   locations. Motivated by this, we propose DiffDoctor, a
                   two-stage pipeline to assist image diffusion models in
                   generating fewer artifacts. Concretely, the first stage
                   targets developing a robust artifact detector, for which we
                   collect a dataset of over 1M flawed synthesized images and
                   set up an efficient human-in-the-loop annotation process,
                   incorporating a carefully designed class-balance strategy.
                   The learned artifact detector is then involved in the second
                   stage to tune the diffusion model through assigning a
                   per-pixel confidence map for each synthesis. Extensive
                   experiments on text-to-image diffusion models demonstrate the
                   effectiveness of our artifact detector as well as the
                   soundness of our diagnose-then-treat design.",
  month         =  "21~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.12382"
}

@ARTICLE{Imam2025-mh,
  title         = "Can Multimodal {LLMs} do Visual Temporal Understanding and
                   Reasoning? The answer is No!",
  author        = "Imam, Mohamed Fazli and Chenyang, Lyu and Aji, Alham Fikri",
  journal       = "arXiv [cs.CV]",
  abstract      = "Multimodal Large Language Models (MLLMs) have achieved
                   significant advancements in tasks like Visual Question
                   Answering (VQA) by leveraging foundational Large Language
                   Models (LLMs). However, their abilities in specific areas
                   such as temporal understanding, which is crucial for
                   comprehending real-world dynamics, remain underexplored. To
                   address this, we propose a challenging evaluation benchmark
                   named TemporalVQA, consisting of two parts: (1) Temporal
                   Order Understanding and (2) Time-lapse Estimation. The first
                   part requires MLLMs to determine the sequence of events by
                   analyzing temporally consecutive video frames. The second
                   part presents image pairs with varying time differences,
                   framed as multiple-choice questions, asking MLLMs to estimate
                   the time-lapse between images with options ranging from
                   seconds to years. Our evaluations of advanced MLLMs,
                   including models like GPT-4o and Gemini-1.5-Pro, reveal
                   significant challenges: GPT-4o achieved only 43.8\% average
                   consistent accuracy in temporal order tasks and 70\% in
                   time-lapse estimation, with open-source models performing
                   even less effectively. These findings underscore the
                   limitations of current MLLMs in visual temporal understanding
                   and reasoning, highlighting the need for further improvements
                   in their temporal capabilities. Our dataset can be found at
                   https://huggingface.co/datasets/fazliimam/temporal-vqa.",
  month         =  "18~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.10674"
}

@ARTICLE{Gutlin2025-fr,
  title    = "Predictive Coding algorithms induce brain-like responses in
              Artificial Neural Networks",
  author   = "Gutlin, Dirk C and Auksztulewicz, Ryszard",
  journal  = "bioRxiv",
  pages    = "2025.01.16.633317",
  abstract = "This study explores whether predictive coding (PC) inspired Deep
              Neural Networks can serve as biologically plausible neural network
              models of the brain. We compared two PC-inspired training
              objectives, a predictive and a contrastive approach, to a
              supervised baseline in a simple Recurrent Neural Network (RNN)
              architecture. We evaluated the models on key signatures of PC,
              including mismatch responses, formation of priors, and learning of
              semantic information. Our results show that the PC-inspired
              models, especially a locally trained predictive model, exhibited
              these PC-like behaviors better than a Supervised or an Untrained
              RNN. Further, we found that activity regularization evokes
              mismatch response-like effects across all models, suggesting it
              may serve as a proxy for the energy-saving principles of PC.
              Finally, we find that Gain Control (an important mechanism in the
              PC framework) can be implemented using weight regularization.
              Overall, our findings indicate that PC-inspired models are able to
              capture important computational principles of predictive
              processing in the brain, and can serve as a promising foundation
              for building biologically plausible artificial neural networks.
              This work contributes to our understanding of the relationship
              between artificial and biological neural networks, and highlights
              the potential of PC-inspired algorithms for advancing brain
              modelling as well as brain-inspired machine learning.",
  month    =  "20~" # jan,
  year     =  2025,
  doi      = "10.1101/2025.01.16.633317"
}

@ARTICLE{Keita2025-yy,
  title         = "One-{D}-Piece: Image tokenizer meets quality-controllable
                   compression",
  author        = "Keita, Miwa and Kento, Sasaki and Hidehisa, Arai and Tsubasa,
                   Takahashi and Yu, Yamaguchi",
  journal       = "arXiv [cs.CV]",
  abstract      = "Current image tokenization methods require a large number of
                   tokens to capture the information contained within images.
                   Although the amount of information varies across images, most
                   image tokenizers only support fixed-length tokenization,
                   leading to inefficiency in token allocation. In this study,
                   we introduce One-D-Piece, a discrete image tokenizer designed
                   for variable-length tokenization, achieving
                   quality-controllable mechanism. To enable variable
                   compression rate, we introduce a simple but effective
                   regularization mechanism named ``Tail Token Drop'' into
                   discrete one-dimensional image tokenizers. This method
                   encourages critical information to concentrate at the head of
                   the token sequence, enabling support of variadic
                   tokenization, while preserving state-of-the-art
                   reconstruction quality. We evaluate our tokenizer across
                   multiple reconstruction quality metrics and find that it
                   delivers significantly better perceptual quality than
                   existing quality-controllable compression methods, including
                   JPEG and WebP, at smaller byte sizes. Furthermore, we assess
                   our tokenizer on various downstream computer vision tasks,
                   including image classification, object detection, semantic
                   segmentation, and depth estimation, confirming its
                   adaptability to numerous applications compared to other
                   variable-rate methods. Our approach demonstrates the
                   versatility of variable-length discrete image tokenization,
                   establishing a new paradigm in both compression efficiency
                   and reconstruction performance. Finally, we validate the
                   effectiveness of tail token drop via detailed analysis of
                   tokenizers.",
  month         =  "17~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.10064"
}

@ARTICLE{Shangkun2025-vd,
  title         = "{IE}-Bench: Advancing the measurement of text-driven image
                   editing for human perception alignment",
  author        = "Shangkun, Sun and Bowen, Qu and Xiaoyu, Liang and Songlin,
                   Fan and Wei, Gao",
  journal       = "arXiv [cs.CV]",
  abstract      = "Recent advances in text-driven image editing have been
                   significant, yet the task of accurately evaluating these
                   edited images continues to pose a considerable challenge.
                   Different from the assessment of text-driven image
                   generation, text-driven image editing is characterized by
                   simultaneously conditioning on both text and a source image.
                   The edited images often retain an intrinsic connection to the
                   original image, which dynamically change with the semantics
                   of the text. However, previous methods tend to solely focus
                   on text-image alignment or have not aligned with human
                   perception. In this work, we introduce the Text-driven Image
                   Editing Benchmark suite (IE-Bench) to enhance the assessment
                   of text-driven edited images. IE-Bench includes a database
                   contains diverse source images, various editing prompts and
                   the corresponding results different editing methods, and
                   total 3,010 Mean Opinion Scores (MOS) provided by 25 human
                   subjects. Furthermore, we introduce IE-QA, a multi-modality
                   source-aware quality assessment method for text-driven image
                   editing. To the best of our knowledge, IE-Bench offers the
                   first IQA dataset and model tailored for text-driven image
                   editing. Extensive experiments demonstrate IE-QA's superior
                   subjective-alignments on the text-driven image editing task
                   compared with previous metrics. We will make all related data
                   and code available to the public.",
  month         =  "17~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.09927"
}

@ARTICLE{Jeremy2025-rj,
  title         = "Lossy Compression with Pretrained Diffusion Models",
  author        = "Jeremy, Vonderfecht and Feng, Liu",
  journal       = "arXiv [cs.CV]",
  abstract      = "We apply the DiffC algorithm (Theis et al. 2022) to Stable
                   Diffusion 1.5, 2.1, XL, and Flux-dev, and demonstrate that
                   these pretrained models are remarkably capable lossy image
                   compressors. A principled algorithm for lossy compression
                   using pretrained diffusion models has been understood since
                   at least Ho et al. 2020, but challenges in reverse-channel
                   coding have prevented such algorithms from ever being fully
                   implemented. We introduce simple workarounds that lead to the
                   first complete implementation of DiffC, which is capable of
                   compressing and decompressing images using Stable Diffusion
                   in under 10 seconds. Despite requiring no additional
                   training, our method is competitive with other
                   state-of-the-art generative compression methods at low
                   ultra-low bitrates.",
  month         =  "16~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.09815"
}

@ARTICLE{Chen2024-ec,
  title         = "{PixArt}-\Sigma: Weak-to-strong training of Diffusion
                   Transformer for {4K} text-to-image generation",
  author        = "Chen, Junsong and Ge, Chongjian and Xie, Enze and Wu, Yue and
                   Yao, Lewei and Ren, Xiaozhe and Wang, Zhongdao and Luo, Ping
                   and Lu, Huchuan and Li, Zhenguo",
  journal       = "arXiv [cs.CV]",
  abstract      = "In this paper, we introduce PixArt-\Sigma, a Diffusion
                   Transformer model~(DiT) capable of directly generating images
                   at 4K resolution. PixArt-\Sigma represents a significant
                   advancement over its predecessor, PixArt-\alpha, offering
                   images of markedly higher fidelity and improved alignment
                   with text prompts. A key feature of PixArt-\Sigma is its
                   training efficiency. Leveraging the foundational pre-training
                   of PixArt-\alpha, it evolves from the `weaker' baseline to a
                   `stronger' model via incorporating higher quality data, a
                   process we term ``weak-to-strong training''. The advancements
                   in PixArt-\Sigma are twofold: (1) High-Quality Training Data:
                   PixArt-\Sigma incorporates superior-quality image data,
                   paired with more precise and detailed image captions. (2)
                   Efficient Token Compression: we propose a novel attention
                   module within the DiT framework that compresses both keys and
                   values, significantly improving efficiency and facilitating
                   ultra-high-resolution image generation. Thanks to these
                   improvements, PixArt-\Sigma achieves superior image quality
                   and user prompt adherence capabilities with significantly
                   smaller model size (0.6B parameters) than existing
                   text-to-image diffusion models, such as SDXL (2.6B
                   parameters) and SD Cascade (5.1B parameters). Moreover,
                   PixArt-\Sigma's capability to generate 4K images supports the
                   creation of high-resolution posters and wallpapers,
                   efficiently bolstering the production of high-quality visual
                   content in industries such as film and gaming.",
  month         =  "7~" # mar,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2403.04692",
  keywords      = "Project/Density\_IQA"
}

@ARTICLE{Juan2025-li,
  title         = "{FLOL}: Fast baselines for real-world low-light enhancement",
  author        = "Juan, C Benito and Daniel, Feijoo and Alvaro, Garcia and
                   Marcos, V Conde",
  journal       = "arXiv [cs.CV]",
  abstract      = "Low-Light Image Enhancement (LLIE) is a key task in
                   computational photography and imaging. The problem of
                   enhancing images captured during night or in dark
                   environments has been well-studied in the image signal
                   processing literature. However, current deep learning-based
                   solutions struggle with efficiency and robustness in
                   real-world scenarios (e.g. scenes with noise, saturated
                   pixels, bad illumination). We propose a lightweight neural
                   network that combines image processing in the frequency and
                   spatial domains. Our method, FLOL+, is one of the fastest
                   models for this task, achieving state-of-the-art results on
                   popular real scenes datasets such as LOL and LSRW. Moreover,
                   we are able to process 1080p images under 12ms. Code and
                   models at https://github.com/cidautai/FLOL",
  month         =  "16~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.09718"
}

@ARTICLE{Kapil2025-iy,
  title         = "Dynamic neural style transfer for artistic image generation
                   using {VGG19}",
  author        = "Kapil, Kashyap and Mehak, Garg and Sean, Fargose and Sindhu,
                   Nair",
  journal       = "arXiv [cs.CV]",
  abstract      = "Throughout history, humans have created remarkable works of
                   art, but artificial intelligence has only recently started to
                   make strides in generating visually compelling art.
                   Breakthroughs in the past few years have focused on using
                   convolutional neural networks (CNNs) to separate and
                   manipulate the content and style of images, applying texture
                   synthesis techniques. Nevertheless, a number of current
                   techniques continue to encounter obstacles, including lengthy
                   processing times, restricted choices of style images, and the
                   inability to modify the weight ratio of styles. We proposed a
                   neural style transfer system that can add various artistic
                   styles to a desired image to address these constraints
                   allowing flexible adjustments to style weight ratios and
                   reducing processing time. The system uses the VGG19 model for
                   feature extraction, ensuring high-quality, flexible
                   stylization without compromising content integrity.",
  month         =  "16~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.09420"
}

@ARTICLE{Saman2025-rb,
  title         = "Do generative video models learn physical principles from
                   watching videos?",
  author        = "Saman, Motamed and Laura, Culp and Kevin, Swersky and
                   Priyank, Jaini and Robert, Geirhos",
  journal       = "arXiv [cs.CV]",
  abstract      = "AI video generation is undergoing a revolution, with quality
                   and realism advancing rapidly. These advances have led to a
                   passionate scientific debate: Do video models learn ``world
                   models'' that discover laws of physics -- or, alternatively,
                   are they merely sophisticated pixel predictors that achieve
                   visual realism without understanding the physical principles
                   of reality? We address this question by developing
                   Physics-IQ, a comprehensive benchmark dataset that can only
                   be solved by acquiring a deep understanding of various
                   physical principles, like fluid dynamics, optics, solid
                   mechanics, magnetism and thermodynamics. We find that across
                   a range of current models (Sora, Runway, Pika, Lumiere,
                   Stable Video Diffusion, and VideoPoet), physical
                   understanding is severely limited, and unrelated to visual
                   realism. At the same time, some test cases can already be
                   successfully solved. This indicates that acquiring certain
                   physical principles from observation alone may be possible,
                   but significant challenges remain. While we expect rapid
                   advances ahead, our work demonstrates that visual realism
                   does not imply physical understanding. Our project page is at
                   https://physics-iq.github.io; code at
                   https://github.com/google-deepmind/physics-IQ-benchmark.",
  month         =  "14~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.09038",
  keywords      = "Project/Density\_IQA;\_To\_read"
}

@ARTICLE{Parker2025-ke,
  title     = "The saccade target is prioritized for visual stability in
               naturalistic scenes",
  author    = "Parker, Jessica L and Tas, A Caglar",
  journal   = "Vision research",
  publisher = "Elsevier BV",
  volume    =  227,
  number    =  108541,
  pages     =  108541,
  month     =  feb,
  year      =  2025,
  doi       = "10.1016/j.visres.2025.108541",
  issn      = "0042-6989,1878-5646",
  language  = "en"
}

@ARTICLE{Shivang2024-zi,
  title         = "Unconditional stability of a recurrent neural circuit
                   implementing divisive normalization",
  author        = "Shivang, Rawat and David, J Heeger and Stefano, Martiniani",
  journal       = "arXiv [q-bio.NC]",
  abstract      = "Stability in recurrent neural models poses a significant
                   challenge, particularly in developing biologically plausible
                   neurodynamical models that can be seamlessly trained.
                   Traditional cortical circuit models are notoriously difficult
                   to train due to expansive nonlinearities in the dynamical
                   system, leading to an optimization problem with nonlinear
                   stability constraints that are difficult to impose.
                   Conversely, recurrent neural networks (RNNs) excel in tasks
                   involving sequential data but lack biological plausibility
                   and interpretability. In this work, we address these
                   challenges by linking dynamic divisive normalization (DN) to
                   the stability of ORGaNICs, a biologically plausible recurrent
                   cortical circuit model that dynamically achieves DN and that
                   has been shown to simulate a wide range of neurophysiological
                   phenomena. By using the indirect method of Lyapunov, we prove
                   the remarkable property of unconditional local stability for
                   an arbitrary-dimensional ORGaNICs circuit when the recurrent
                   weight matrix is the identity. We thus connect ORGaNICs to a
                   system of coupled damped harmonic oscillators, which enables
                   us to derive the circuit's energy function, providing a
                   normative principle of what the circuit, and individual
                   neurons, aim to accomplish. Further, for a generic recurrent
                   weight matrix, we prove the stability of the 2D model and
                   demonstrate empirically that stability holds in higher
                   dimensions. Finally, we show that ORGaNICs can be trained by
                   backpropagation through time without gradient
                   clipping/scaling, thanks to its intrinsic stability property
                   and adaptive time constants, which address the problems of
                   exploding, vanishing, and oscillating gradients. By
                   evaluating the model's performance on RNN benchmarks, we find
                   that ORGaNICs outperform alternative neurodynamical models on
                   static image classification tasks and perform comparably to
                   LSTMs on sequential tasks.",
  month         =  "27~" # sep,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "2409.18946"
}

@ARTICLE{Nasim2025-sk,
  title         = "A bioplausible model for the Expanding Hole Illusion:
                   Insights into retinal processing and illusory motion",
  author        = "Nasim, Nematzadeh and Powers, David M W",
  journal       = "arXiv [q-bio.NC]",
  abstract      = "The Expanding Hole Illusion is a compelling visual phenomenon
                   in which a static, concentric pattern evokes a strong
                   perception of continuous forward motion. Despite its
                   simplicity, this illusion challenges our understanding of how
                   the brain processes visual information, particularly motion
                   derived from static cues. While the neural basis of this
                   illusion has remained elusive, recent psychophysical studies
                   [1] reveal that this illusion induces not only a perceptual
                   effect but also physiological responses, such as pupil
                   dilation. This paper presents a computational model based on
                   Difference of Gaussians (DoG) filtering and a classical
                   receptive field (CRF) implementation to simulate early
                   retinal processing and to explain the underlying mechanisms
                   of this illusion. Based on our results we hypothesize that
                   the illusion arises from contrast-dependent lateral
                   inhibition in early visual processing. Our results
                   demonstrate that contrast gradients and multi-layered spatial
                   processing contribute to the perception of expansion,
                   aligning closely with psychophysical findings and supporting
                   the role of retinal ganglion cells in generating this
                   illusory motion signal. Our findings provide insights into
                   the perceptual biases driving dynamic illusions and offer a
                   new framework for studying complex visual phenomena.",
  month         =  "15~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "2501.08625"
}

@ARTICLE{Fox2024-ge,
  title         = "A unifying information-theoretic perspective on evaluating
                   generative models",
  author        = "Fox, Alexis and Swarup, Samarth and Adiga, Abhijin",
  journal       = "arXiv [cs.LG]",
  abstract      = "Considering the difficulty of interpreting generative model
                   output, there is significant current research focused on
                   determining meaningful evaluation metrics. Several recent
                   approaches utilize ``precision'' and ``recall,'' borrowed
                   from the classification domain, to individually quantify the
                   output fidelity (realism) and output diversity
                   (representation of the real data variation), respectively.
                   With the increase in metric proposals, there is a need for a
                   unifying perspective, allowing for easier comparison and
                   clearer explanation of their benefits and drawbacks. To this
                   end, we unify a class of kth-nearest-neighbors (kNN)-based
                   metrics under an information-theoretic lens using approaches
                   from kNN density estimation. Additionally, we propose a
                   tri-dimensional metric composed of Precision Cross-Entropy
                   (PCE), Recall Cross-Entropy (RCE), and Recall Entropy (RE),
                   which separately measure fidelity and two distinct aspects of
                   diversity, inter- and intra-class. Our domain-agnostic
                   metric, derived from the information-theoretic concepts of
                   entropy and cross-entropy, can be dissected for both sample-
                   and mode-level analysis. Our detailed experimental results
                   demonstrate the sensitivity of our metric components to their
                   respective qualities and reveal undesirable behaviors of
                   other metrics.",
  month         =  "18~" # dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2412.14340",
  keywords      = "Project/Density\_IQA"
}

@ARTICLE{Tuncok2025-gq,
  title     = "Opposite asymmetry in visual perception of humans and macaques",
  author    = "Tünçok, Ekin and Kiorpes, Lynne and Carrasco, Marisa",
  journal   = "Current biology: CB",
  publisher = "Elsevier BV",
  month     =  jan,
  year      =  2025,
  doi       = "10.1016/j.cub.2024.12.024",
  issn      = "0960-9822,1879-0445",
  language  = "en"
}

@ARTICLE{Ali2024-ba,
  title         = "Titans: Learning to memorize at test time",
  author        = "Ali, Behrouz and Peilin, Zhong and Vahab, Mirrokni",
  journal       = "arXiv [cs.LG]",
  abstract      = "Over more than a decade there has been an extensive research
                   effort on how to effectively utilize recurrent models and
                   attention. While recurrent models aim to compress the data
                   into a fixed-size memory (called hidden state), attention
                   allows attending to the entire context window, capturing the
                   direct dependencies of all tokens. This more accurate
                   modeling of dependencies, however, comes with a quadratic
                   cost, limiting the model to a fixed-length context. We
                   present a new neural long-term memory module that learns to
                   memorize historical context and helps attention to attend to
                   the current context while utilizing long past information. We
                   show that this neural memory has the advantage of fast
                   parallelizable training while maintaining a fast inference.
                   From a memory perspective, we argue that attention due to its
                   limited context but accurate dependency modeling performs as
                   a short-term memory, while neural memory due to its ability
                   to memorize the data, acts as a long-term, more persistent,
                   memory. Based on these two modules, we introduce a new family
                   of architectures, called Titans, and present three variants
                   to address how one can effectively incorporate memory into
                   this architecture. Our experimental results on language
                   modeling, common-sense reasoning, genomics, and time series
                   tasks show that Titans are more effective than Transformers
                   and recent modern linear recurrent models. They further can
                   effectively scale to larger than 2M context window size with
                   higher accuracy in needle-in-haystack tasks compared to
                   baselines.",
  month         =  "31~" # dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2501.00663"
}

@ARTICLE{Xingchen2025-sh,
  title         = "{UnCommon} Objects in {3D}",
  author        = "Xingchen, Liu and Piyush, Tayal and Jianyuan, Wang and Jesus,
                   Zarzar and Tom, Monnier and Konstantinos, Tertikas and Jiali,
                   Duan and Antoine, Toisoul and Jason, Y Zhang and Natalia,
                   Neverova and Andrea, Vedaldi and Roman, Shapovalov and David,
                   Novotny",
  journal       = "arXiv [cs.CV]",
  abstract      = "We introduce Uncommon Objects in 3D (uCO3D), a new
                   object-centric dataset for 3D deep learning and 3D generative
                   AI. uCO3D is the largest publicly-available collection of
                   high-resolution videos of objects with 3D annotations that
                   ensures full-360$^{\circ}$ coverage. uCO3D is significantly
                   more diverse than MVImgNet and CO3Dv2, covering more than
                   1,000 object categories. It is also of higher quality, due to
                   extensive quality checks of both the collected videos and the
                   3D annotations. Similar to analogous datasets, uCO3D contains
                   annotations for 3D camera poses, depth maps and sparse point
                   clouds. In addition, each object is equipped with a caption
                   and a 3D Gaussian Splat reconstruction. We train several
                   large 3D models on MVImgNet, CO3Dv2, and uCO3D and obtain
                   superior results using the latter, showing that uCO3D is
                   better for learning applications.",
  month         =  "13~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.07574"
}

@ARTICLE{Klavinskis-Whiting2025-qa,
  title     = "Prediction of future input explains lateral connectivity in
               primary visual cortex",
  author    = "Klavinskis-Whiting, Sebastian and Fristed, Emil and Singer, Yosef
               and Iacaruso, M Florencia and King, Andrew J and Harper, Nicol S",
  journal   = "Current biology: CB",
  publisher = "Elsevier BV",
  month     =  jan,
  year      =  2025,
  doi       = "10.1016/j.cub.2024.11.073",
  issn      = "0960-9822,1879-0445",
  language  = "en"
}

@ARTICLE{Minami2017-wn,
  title     = "Illusory jitter perceived at the frequency of alpha oscillations",
  author    = "Minami, Sorato and Amano, Kaoru",
  journal   = "Current biology: CB",
  publisher = "Curr Biol",
  volume    =  27,
  number    =  15,
  pages     = "2344--2351.e4",
  abstract  = "Neural oscillations, such as alpha (8-13 Hz), beta (13-30 Hz),
               and gamma (30-100 Hz), are widespread across cortical areas, and
               their possible functional roles include feature binding [1],
               neuronal communication [2, 3], and memory [1, 4]. The most
               prominent signal among these neural oscillations is the alpha
               oscillation. Although accumulating evidence suggests that alpha
               oscillations correlate with various aspects of visual processing
               [5-18], the number of studies proving their causal contribution
               in visual perception is limited [11, 16-18]. Here we report that
               illusory visual vibrations are consciously experienced at the
               frequency of intrinsic alpha oscillations. We employed an
               illusory jitter perception termed the motion-induced spatial
               conflict [19] that originates from the cyclic interaction between
               motion and shape processing. Comparison between the perceived
               frequency of illusory jitter and the peak alpha frequency (PAF)
               measured using magnetoencephalography (MEG) revealed that the
               inter- and intra-participant variations of the PAF are mirrored
               by an illusory jitter perception. More crucially, psychophysical
               and MEG measurements during amplitude-modulated current
               stimulation [20] showed that the PAF can be artificially
               manipulated, which results in a corresponding change in the
               perceived jitter frequency. These results suggest the causal
               contribution of neural oscillations at the alpha frequency in
               creating temporal characteristics of visual perception. Our
               results suggest that cortical areas, dorsal and ventral visual
               areas in this case, are interacting at the frequency of alpha
               oscillations [2, 3, 21-27].",
  month     =  "7~" # aug,
  year      =  2017,
  keywords  = "AM; MEG; alpha oscillations; amplitude modulation; illusory
               jitter; magnetoencephalography; neural oscillations; tACS",
  doi       = "10.1016/j.cub.2017.06.033",
  pmid      =  28756954,
  issn      = "1879-0445,0960-9822",
  language  = "en"
}

@ARTICLE{David2023-rz,
  title         = "Less is more: The influence of pruning on the explainability
                   of {CNNs}",
  author        = "David, Weber and Florian, Merkle and Pascal, Schöttle and
                   Stephan, Schlögl",
  journal       = "arXiv [cs.CV]",
  abstract      = "Modern, state-of-the-art Convolutional Neural Networks (CNNs)
                   in computer vision have millions of parameters. Thus,
                   explaining the complex decisions of such networks to humans
                   is challenging. A technical approach to reduce CNN complexity
                   is network pruning, where less important parameters are
                   deleted. The work presented in this paper investigates
                   whether this technical complexity reduction also helps with
                   perceived explainability. To do so, we conducted a pre-study
                   and two human-grounded experiments, assessing the effects of
                   different pruning ratios on CNN explainability. Overall, we
                   evaluated four different compression rates (i.e., CPR 2, 4,
                   8, and 32) with 37 500 tasks on Mechanical Turk. Results
                   indicate that lower compression rates have a positive
                   influence on explainability, while higher compression rates
                   show negative effects. Furthermore, we were able to identify
                   sweet spots that increase both the perceived explainability
                   and the model's performance.",
  month         =  "17~" # feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2302.08878"
}

@ARTICLE{Huang2025-gv,
  title         = "The {GAN} is dead; long live the {GAN}! A Modern {GAN}
                   Baseline",
  author        = "Huang, Yiwen and Gokaslan, Aaron and Kuleshov, Volodymyr and
                   Tompkin, James",
  journal       = "arXiv [cs.LG]",
  abstract      = "There is a widely-spread claim that GANs are difficult to
                   train, and GAN architectures in the literature are littered
                   with empirical tricks. We provide evidence against this claim
                   and build a modern GAN baseline in a more principled manner.
                   First, we derive a well-behaved regularized relativistic GAN
                   loss that addresses issues of mode dropping and
                   non-convergence that were previously tackled via a bag of
                   ad-hoc tricks. We analyze our loss mathematically and prove
                   that it admits local convergence guarantees, unlike most
                   existing relativistic losses. Second, our new loss allows us
                   to discard all ad-hoc tricks and replace outdated backbones
                   used in common GANs with modern architectures. Using
                   StyleGAN2 as an example, we present a roadmap of
                   simplification and modernization that results in a new
                   minimalist baseline -- R3GAN. Despite being simple, our
                   approach surpasses StyleGAN2 on FFHQ, ImageNet, CIFAR, and
                   Stacked MNIST datasets, and compares favorably against
                   state-of-the-art GANs and diffusion models.",
  month         =  "9~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2501.05441"
}

@ARTICLE{Yu2025-qb,
  title         = "Relative pose estimation through affine corrections of
                   monocular depth priors",
  author        = "Yu, Yifan and Liu, Shaohui and Pautrat, Rémi and Pollefeys,
                   Marc and Larsson, Viktor",
  journal       = "arXiv [cs.CV]",
  abstract      = "Monocular depth estimation (MDE) models have undergone
                   significant advancements over recent years. Many MDE models
                   aim to predict affine-invariant relative depth from monocular
                   images, while recent developments in large-scale training and
                   vision foundation models enable reasonable estimation of
                   metric (absolute) depth. However, effectively leveraging
                   these predictions for geometric vision tasks, in particular
                   relative pose estimation, remains relatively under explored.
                   While depths provide rich constraints for cross-view image
                   alignment, the intrinsic noise and ambiguity from the
                   monocular depth priors present practical challenges to
                   improving upon classic keypoint-based solutions. In this
                   paper, we develop three solvers for relative pose estimation
                   that explicitly account for independent affine (scale and
                   shift) ambiguities, covering both calibrated and uncalibrated
                   conditions. We further propose a hybrid estimation pipeline
                   that combines our proposed solvers with classic point-based
                   solvers and epipolar constraints. We find that the affine
                   correction modeling is beneficial to not only the relative
                   depth priors but also, surprisingly, the ``metric`` ones.
                   Results across multiple datasets demonstrate large
                   improvements of our approach over classic keypoint-based
                   baselines and PnP-based solutions, under both calibrated and
                   uncalibrated setups. We also show that our method improves
                   consistently with different feature matchers and MDE models,
                   and can further benefit from very recent advances on both
                   modules. Code is available at
                   https://github.com/MarkYu98/madpose.",
  month         =  "9~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.05446"
}

@ARTICLE{Haoyu2024-ll,
  title         = "{NeuralDiffuser}: Neuroscience-inspired diffusion guidance
                   for {fMRI} visual reconstruction",
  author        = "Haoyu, Li and Hao, Wu and Badong, Chen",
  journal       = "arXiv [cs.NE]",
  abstract      = "Reconstructing visual stimuli from functional Magnetic
                   Resonance Imaging fMRI enables fine-grained retrieval of
                   brain activity. However, the accurate reconstruction of
                   diverse details, including structure, background, texture,
                   color, and more, remains challenging. The stable diffusion
                   models inevitably result in the variability of reconstructed
                   images, even under identical conditions. To address this
                   challenge, we first uncover the neuroscientific perspective
                   of diffusion methods, which primarily involve top-down
                   creation using pre-trained knowledge from extensive image
                   datasets, but tend to lack detail-driven bottom-up
                   perception, leading to a loss of faithful details. In this
                   paper, we propose NeuralDiffuser, which incorporates primary
                   visual feature guidance to provide detailed cues in the form
                   of gradients. This extension of the bottom-up process for
                   diffusion models achieves both semantic coherence and detail
                   fidelity when reconstructing visual stimuli. Furthermore, we
                   have developed a novel guidance strategy for reconstruction
                   tasks that ensures the consistency of repeated outputs with
                   original images rather than with various outputs. Extensive
                   experimental results on the Natural Senses Dataset (NSD)
                   qualitatively and quantitatively demonstrate the advancement
                   of NeuralDiffuser by comparing it against baseline and
                   state-of-the-art methods horizontally, as well as conducting
                   longitudinal ablation studies.",
  month         =  "21~" # feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE",
  eprint        = "2402.13809"
}

@ARTICLE{Van_Holland2025-tt,
  title         = "{NeRFs} are mirror detectors: Using structural similarity for
                   multi-view mirror scene reconstruction with {3D} surface
                   primitives",
  author        = "Van Holland, Leif and Michael, Weinmann and Jan, U Müller and
                   Patrick, Stotko and Reinhard, Klein",
  journal       = "arXiv [cs.CV]",
  abstract      = "While neural radiance fields (NeRF) led to a breakthrough in
                   photorealistic novel view synthesis, handling mirroring
                   surfaces still denotes a particular challenge as they
                   introduce severe inconsistencies in the scene representation.
                   Previous attempts either focus on reconstructing single
                   reflective objects or rely on strong supervision guidance in
                   terms of additional user-provided annotations of visible
                   image regions of the mirrors, thereby limiting the practical
                   usability. In contrast, in this paper, we present NeRF-MD, a
                   method which shows that NeRFs can be considered as mirror
                   detectors and which is capable of reconstructing neural
                   radiance fields of scenes containing mirroring surfaces
                   without the need for prior annotations. To this end, we first
                   compute an initial estimate of the scene geometry by training
                   a standard NeRF using a depth reprojection loss. Our key
                   insight lies in the fact that parts of the scene
                   corresponding to a mirroring surface will still exhibit a
                   significant photometric inconsistency, whereas the remaining
                   parts are already reconstructed in a plausible manner. This
                   allows us to detect mirror surfaces by fitting geometric
                   primitives to such inconsistent regions in this initial stage
                   of the training. Using this information, we then jointly
                   optimize the radiance field and mirror geometry in a second
                   training stage to refine their quality. We demonstrate the
                   capability of our method to allow the faithful detection of
                   mirrors in the scene as well as the reconstruction of a
                   single consistent scene representation, and demonstrate its
                   potential in comparison to baseline and mirror-aware
                   approaches.",
  month         =  "7~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.04074"
}

@ARTICLE{Samuel2025-xf,
  title         = "Agent Laboratory: Using {LLM} agents as research assistants",
  author        = "Samuel, Schmidgall and Yusheng, Su and Ze, Wang and Ximeng,
                   Sun and Jialian, Wu and Xiaodong, Yu and Jiang, Liu and
                   Zicheng, Liu and Emad, Barsoum",
  journal       = "arXiv [cs.HC]",
  abstract      = "Historically, scientific discovery has been a lengthy and
                   costly process, demanding substantial time and resources from
                   initial conception to final results. To accelerate scientific
                   discovery, reduce research costs, and improve research
                   quality, we introduce Agent Laboratory, an autonomous
                   LLM-based framework capable of completing the entire research
                   process. This framework accepts a human-provided research
                   idea and progresses through three stages--literature review,
                   experimentation, and report writing to produce comprehensive
                   research outputs, including a code repository and a research
                   report, while enabling users to provide feedback and guidance
                   at each stage. We deploy Agent Laboratory with various
                   state-of-the-art LLMs and invite multiple researchers to
                   assess its quality by participating in a survey, providing
                   human feedback to guide the research process, and then
                   evaluate the final paper. We found that: (1) Agent Laboratory
                   driven by o1-preview generates the best research outcomes;
                   (2) The generated machine learning code is able to achieve
                   state-of-the-art performance compared to existing methods;
                   (3) Human involvement, providing feedback at each stage,
                   significantly improves the overall quality of research; (4)
                   Agent Laboratory significantly reduces research expenses,
                   achieving an 84\% decrease compared to previous autonomous
                   research methods. We hope Agent Laboratory enables
                   researchers to allocate more effort toward creative ideation
                   rather than low-level coding and writing, ultimately
                   accelerating scientific discovery.",
  month         =  "8~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2501.04227"
}

@ARTICLE{Nvidia2025-wz,
  title         = "Cosmos World Foundation Model Platform for Physical {AI}",
  author        = "{NVIDIA} and Niket, Agarwal and Arslan, Ali and Maciej, Bala
                   and Yogesh, Balaji and Erik, Barker and Tiffany, Cai and
                   Prithvijit, Chattopadhyay and Yongxin, Chen and Yin, Cui and
                   Yifan, Ding and Daniel, Dworakowski and Jiaojiao, Fan and
                   Michele, Fenzi and Francesco, Ferroni and Sanja, Fidler and
                   Dieter, Fox and Songwei, Ge and Yunhao, Ge and Jinwei, Gu and
                   Siddharth, Gururani and Ethan, He and Jiahui, Huang and
                   Jacob, Huffman and Pooya, Jannaty and Jingyi, Jin and Kim,
                   Seung Wook and Gergely, Klár and Grace, Lam and Shiyi, Lan
                   and Laura, Leal-Taixe and Anqi, Li and Zhaoshuo, Li and
                   Chen-Hsuan, Lin and Tsung-Yi, Lin and Huan, Ling and Ming-Yu,
                   Liu and Xian, Liu and Alice, Luo and Qianli, Ma and Hanzi,
                   Mao and Kaichun, Mo and Arsalan, Mousavian and Seungjun, Nah
                   and Sriharsha, Niverty and David, Page and Despoina,
                   Paschalidou and Zeeshan, Patel and Lindsey, Pavao and
                   Morteza, Ramezanali and Fitsum, Reda and Xiaowei, Ren and
                   Sabavat, Vasanth Rao Naik and Ed, Schmerling and Stella, Shi
                   and Bartosz, Stefaniak and Shitao, Tang and Lyne, Tchapmi and
                   Przemek, Tredak and Wei-Cheng, Tseng and Jibin, Varghese and
                   Hao, Wang and Haoxiang, Wang and Heng, Wang and Ting-Chun,
                   Wang and Fangyin, Wei and Xinyue, Wei and Wu, Jay Zhangjie
                   and Jiashu, Xu and Wei, Yang and Lin, Yen-Chen and Xiaohui,
                   Zeng and Yu, Zeng and Jing, Zhang and Qinsheng, Zhang and
                   Yuxuan, Zhang and Qingqing, Zhao and Artur, Zolkowski",
  journal       = "arXiv [cs.CV]",
  abstract      = "Physical AI needs to be trained digitally first. It needs a
                   digital twin of itself, the policy model, and a digital twin
                   of the world, the world model. In this paper, we present the
                   Cosmos World Foundation Model Platform to help developers
                   build customized world models for their Physical AI setups.
                   We position a world foundation model as a general-purpose
                   world model that can be fine-tuned into customized world
                   models for downstream applications. Our platform covers a
                   video curation pipeline, pre-trained world foundation models,
                   examples of post-training of pre-trained world foundation
                   models, and video tokenizers. To help Physical AI builders
                   solve the most critical problems of our society, we make our
                   platform open-source and our models open-weight with
                   permissive licenses available via
                   https://github.com/NVIDIA/Cosmos.",
  month         =  "7~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.03575"
}

@ARTICLE{J_Gershman2025-sl,
  title         = "Key-value memory in the brain",
  author        = "J Gershman, Samuel and Fiete, Ila and Irie, Kazuki",
  journal       = "arXiv [q-bio.NC]",
  abstract      = "Classical models of memory in psychology and neuroscience
                   rely on similarity-based retrieval of stored patterns, where
                   similarity is a function of retrieval cues and the stored
                   patterns. While parsimonious, these models do not allow
                   distinct representations for storage and retrieval, despite
                   their distinct computational demands. Key-value memory
                   systems, in contrast, distinguish representations used for
                   storage (values) and those used for retrieval (keys). This
                   allows key-value memory systems to optimize simultaneously
                   for fidelity in storage and discriminability in retrieval. We
                   review the computational foundations of key-value memory, its
                   role in modern machine learning systems, related ideas from
                   psychology and neuroscience, applications to a number of
                   empirical puzzles, and possible biological implementations.",
  month         =  "6~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "2501.02950"
}

@ARTICLE{Alexander2024-vx,
  title         = "Brain-inspired {AI} with hyperbolic geometry",
  author        = "Alexander, Joseph and Nathan, Francis and Meijke, Balay",
  journal       = "arXiv [q-bio.NC]",
  abstract      = "Artificial neural networks (ANNs) were inspired by the
                   architecture and functions of the human brain and have
                   revolutionised the field of artificial intelligence (AI).
                   Inspired by studies on the latent geometry of the brain, in
                   this perspective paper we posit that an increase in the
                   research and application of hyperbolic geometry in ANNs and
                   machine learning will lead to increased accuracy, improved
                   feature space representations and more efficient models
                   across a range of tasks. We examine the structure and
                   functions of the human brain, emphasising the correspondence
                   between its scale-free hierarchical organization and
                   hyperbolic geometry, and reflecting on the central role
                   hyperbolic geometry plays in facilitating human intelligence.
                   Empirical evidence indicates that hyperbolic neural networks
                   outperform Euclidean models for tasks including natural
                   language processing, computer vision and complex network
                   analysis, requiring fewer parameters and exhibiting better
                   generalisation. Despite its nascent adoption, hyperbolic
                   geometry holds promise for improving machine learning models
                   through brain-inspired geometric representations.",
  month         =  "4~" # sep,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "2409.12990"
}

@ARTICLE{Suttisak2024-ee,
  title         = "Taming feed-forward reconstruction models as latent encoders
                   for {3D} generative models",
  author        = "Suttisak, Wizadwongsa and Jinfan, Zhou and Edward, Li and
                   Park, Jeong Joon",
  journal       = "arXiv [cs.CV]",
  abstract      = "Recent AI-based 3D content creation has largely evolved along
                   two paths: feed-forward image-to-3D reconstruction approaches
                   and 3D generative models trained with 2D or 3D supervision.
                   In this work, we show that existing feed-forward
                   reconstruction methods can serve as effective latent encoders
                   for training 3D generative models, thereby bridging these two
                   paradigms. By reusing powerful pre-trained reconstruction
                   models, we avoid computationally expensive encoder network
                   training and obtain rich 3D latent features for generative
                   modeling for free. However, the latent spaces of
                   reconstruction models are not well-suited for generative
                   modeling due to their unstructured nature. To enable
                   flow-based model training on these latent features, we
                   develop post-processing pipelines, including protocols to
                   standardize the features and spatial weighting to concentrate
                   on important regions. We further incorporate a 2D image space
                   perceptual rendering loss to handle the high-dimensional
                   latent spaces. Finally, we propose a multi-stream
                   transformer-based rectified flow architecture to achieve
                   linear scaling and high-quality text-conditioned 3D
                   generation. Our framework leverages the advancements of
                   feed-forward reconstruction models to enhance the scalability
                   of 3D generative modeling, achieving both high computational
                   efficiency and state-of-the-art performance in text-to-3D
                   generation.",
  month         =  "31~" # dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.00651"
}

@ARTICLE{Samin2024-sd,
  title         = "{ColorFoil}: Investigating color blindness in large Vision
                   and Language models",
  author        = "Samin, Ahnaf Mozib and Firoz Ahmed, M and Rafee, Md Mushtaq
                   Shahriyar",
  journal       = "arXiv [cs.CV]",
  abstract      = "With the utilization of Transformer architecture, large
                   Vision and Language (V\&L) models have shown promising
                   performance in even zero-shot settings. Several studies,
                   however, indicate a lack of robustness of the models when
                   dealing with complex linguistics and visual attributes. In
                   this work, we introduce a novel V\&L benchmark - ColorFoil,
                   by creating color-related foils to assess the models'
                   perception ability to detect colors like red, white, green,
                   etc. We evaluate seven state-of-the-art V\&L models including
                   CLIP, ViLT, GroupViT, and BridgeTower, etc. in a zero-shot
                   setting and present intriguing findings from the V\&L models.
                   The experimental evaluation indicates that ViLT and
                   BridgeTower demonstrate much better color perception
                   capabilities compared to CLIP and its variants and GroupViT.
                   Moreover, CLIP-based models and GroupViT struggle to
                   distinguish colors that are visually distinct to humans with
                   normal color perception ability.",
  month         =  "19~" # may,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2405.11685"
}

@ARTICLE{Yonggan2022-xf,
  title         = "Patch-Fool: Are vision transformers always robust against
                   adversarial perturbations?",
  author        = "Yonggan, Fu and Shunyao, Zhang and Shang, Wu and Cheng, Wan
                   and Lin, Yingyan Celine",
  journal       = "arXiv [cs.CV]",
  abstract      = "Vision transformers (ViTs) have recently set off a new wave
                   in neural architecture design thanks to their record-breaking
                   performance in various vision tasks. In parallel, to fulfill
                   the goal of deploying ViTs into real-world vision
                   applications, their robustness against potential malicious
                   attacks has gained increasing attention. In particular,
                   recent works show that ViTs are more robust against
                   adversarial attacks as compared with convolutional neural
                   networks (CNNs), and conjecture that this is because ViTs
                   focus more on capturing global interactions among different
                   input/feature patches, leading to their improved robustness
                   to local perturbations imposed by adversarial attacks. In
                   this work, we ask an intriguing question: ``Under what kinds
                   of perturbations do ViTs become more vulnerable learners
                   compared to CNNs?'' Driven by this question, we first conduct
                   a comprehensive experiment regarding the robustness of both
                   ViTs and CNNs under various existing adversarial attacks to
                   understand the underlying reason favoring their robustness.
                   Based on the drawn insights, we then propose a dedicated
                   attack framework, dubbed Patch-Fool, that fools the
                   self-attention mechanism by attacking its basic component
                   (i.e., a single patch) with a series of attention-aware
                   optimization techniques. Interestingly, our Patch-Fool
                   framework shows for the first time that ViTs are not
                   necessarily more robust than CNNs against adversarial
                   perturbations. In particular, we find that ViTs are more
                   vulnerable learners compared with CNNs against our Patch-Fool
                   attack which is consistent across extensive experiments, and
                   the observations from Sparse/Mild Patch-Fool, two variants of
                   Patch-Fool, indicate an intriguing insight that the
                   perturbation density and strength on each patch seem to be
                   the key factors that influence the robustness ranking between
                   ViTs and CNNs.",
  month         =  "16~" # mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2203.08392"
}

@ARTICLE{Arthur2025-ui,
  title         = "Seeing the whole in the parts in self-supervised
                   representation learning",
  author        = "Arthur, Aubret and Céline, Teulière and Jochen, Triesch",
  journal       = "arXiv [cs.LG]",
  abstract      = "Recent successes in self-supervised learning (SSL) model
                   spatial co-occurrences of visual features either by masking
                   portions of an image or by aggressively cropping it. Here, we
                   propose a new way to model spatial co-occurrences by aligning
                   local representations (before pooling) with a global image
                   representation. We present CO-SSL, a family of instance
                   discrimination methods and show that it outperforms previous
                   methods on several datasets, including ImageNet-1K where it
                   achieves 71.5\% of Top-1 accuracy with 100 pre-training
                   epochs. CO-SSL is also more robust to noise corruption,
                   internal corruption, small adversarial attacks, and large
                   training crop sizes. Our analysis further indicates that
                   CO-SSL learns highly redundant local representations, which
                   offers an explanation for its robustness. Overall, our work
                   suggests that aligning local and global representations may
                   be a powerful principle of unsupervised category learning.",
  month         =  "6~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2501.02860"
}

@ARTICLE{Le2025-oz,
  title         = "Inverse receptive field attention for naturalistic image
                   reconstruction from the brain",
  author        = "Le, Lynn and Dado, Thirza and Seeliger, Katja and Papale,
                   Paolo and Lozano, Antonio and Roelfsema, Pieter and
                   Güçlütürk, Yağmur and Marcel, van Gerven and Güçlü, Umut",
  journal       = "arXiv [q-bio.NC]",
  abstract      = "Visual perception in the brain largely depends on the
                   organization of neuronal receptive fields. Although extensive
                   research has delineated the coding principles of receptive
                   fields, most studies have been constrained by their
                   foundational assumptions. Moreover, while machine learning
                   has successfully been used to reconstruct images from brain
                   data, this approach faces significant challenges, including
                   inherent feature biases in the model and the complexities of
                   brain structure and function. In this study, we introduce an
                   inverse receptive field attention (IRFA) model, designed to
                   reconstruct naturalistic images from neurophysiological data
                   in an end-to-end fashion. This approach aims to elucidate the
                   tuning properties and representational transformations within
                   the visual cortex. The IRFA model incorporates an attention
                   mechanism that determines the inverse receptive field for
                   each pixel, weighting neuronal responses across the visual
                   field and feature spaces. This method allows for an
                   examination of the dynamics of neuronal representations
                   across stimuli in both spatial and feature dimensions. Our
                   results show highly accurate reconstructions of naturalistic
                   data, independent of pre-trained models. Notably, IRF models
                   trained on macaque V1, V4, and IT regions yield remarkably
                   consistent spatial receptive fields across different stimuli,
                   while the features to which neuronal representations are
                   selective exhibit significant variation. Additionally, we
                   propose a data-driven method to explore representational
                   clustering within various visual areas, further providing
                   testable hypotheses.",
  month         =  "6~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "2501.03051"
}

@ARTICLE{Shuguang2025-eo,
  title         = "Task-driven fixation network: An efficient architecture with
                   fixation selection",
  author        = "Shuguang, Wang and Yuanjing, Wang",
  journal       = "arXiv [cs.CV]",
  abstract      = "This paper presents a novel neural network architecture
                   featuring automatic fixation point selection, designed to
                   efficiently address complex tasks with reduced network size
                   and computational overhead. The proposed model consists of: a
                   low-resolution channel that captures low-resolution global
                   features from input images; a high-resolution channel that
                   sequentially extracts localized high-resolution features; and
                   a hybrid encoding module that integrates the features from
                   both channels. A defining characteristic of the hybrid
                   encoding module is the inclusion of a fixation point
                   generator, which dynamically produces fixation points,
                   enabling the high-resolution channel to focus on regions of
                   interest. The fixation points are generated in a task-driven
                   manner, enabling the automatic selection of regions of
                   interest. This approach avoids exhaustive high-resolution
                   analysis of the entire image, maintaining task performance
                   and computational efficiency.",
  month         =  "2~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.01548"
}

@ARTICLE{Lee2025-hs,
  title    = "Emergence of a contrast-invariant representation of naturalistic
              texture in macaque visual cortex",
  author   = "Lee, Gerick M and Majaj, Najib J and Rodríguez Deliz, C L and
              Kiorpes, Lynne and Movshon, J Anthony",
  journal  = "bioRxiv",
  pages    = "2025.01.03.631258",
  abstract = "Sensory stimuli vary across a variety of dimensions, like
              contrast, orientation, or texture. The brain must rely on
              population representations to disentangle changes in one dimension
              from changes in another. To understand how the visual system might
              extract separable stimulus representations, we recorded multiunit
              neuronal responses to texture images varying along two dimensions:
              contrast, a property represented as early as the retina, and
              naturalistic statistical structure, a property that modulates
              neuronal responses in V2 and V4, but not in V1. We measured how
              sites in these 3 cortical areas responded to variation in both
              dimensions. Contrast modulated responses in all areas. In V2 and
              V4, the presence of naturalistic structure both modulated
              responses and increased contrast sensitivity. Tuning for
              naturalistic structure was strongest in V4; tuning in both
              dimensions was most heterogeneous in V4. We measured how well
              populations in each area could support the linear readout of both
              dimensions. Populations in V2 and V4 could support the linear
              readout of naturalistic structure, but only in V4 did we find
              evidence for a robust representation that was contrast-invariant.",
  month    =  "4~" # jan,
  year     =  2025,
  doi      = "10.1101/2025.01.03.631258"
}

@ARTICLE{Weihao2023-es,
  title         = "{InceptionNeXt}: When Inception Meets {ConvNeXt}",
  author        = "Weihao, Yu and Pan, Zhou and Shuicheng, Yan and Xinchao, Wang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Inspired by the long-range modeling ability of ViTs,
                   large-kernel convolutions are widely studied and adopted
                   recently to enlarge the receptive field and improve model
                   performance, like the remarkable work ConvNeXt which employs
                   7x7 depthwise convolution. Although such depthwise operator
                   only consumes a few FLOPs, it largely harms the model
                   efficiency on powerful computing devices due to the high
                   memory access costs. For example, ConvNeXt-T has similar
                   FLOPs with ResNet-50 but only achieves ~60\% throughputs when
                   trained on A100 GPUs with full precision. Although reducing
                   the kernel size of ConvNeXt can improve speed, it results in
                   significant performance degradation, which poses a
                   challenging problem: How to speed up large-kernel-based CNN
                   models while preserving their performance. To tackle this
                   issue, inspired by Inceptions, we propose to decompose
                   large-kernel depthwise convolution into four parallel
                   branches along channel dimension, i.e., small square kernel,
                   two orthogonal band kernels, and an identity mapping. With
                   this new Inception depthwise convolution, we build a series
                   of networks, namely IncepitonNeXt, which not only enjoy high
                   throughputs but also maintain competitive performance. For
                   instance, InceptionNeXt-T achieves 1.6x higher training
                   throughputs than ConvNeX-T, as well as attains 0.2\% top-1
                   accuracy improvement on ImageNet-1K. We anticipate
                   InceptionNeXt can serve as an economical baseline for future
                   architecture design to reduce carbon footprint. Code is
                   available at https://github.com/sail-sg/inceptionnext.",
  month         =  "29~" # mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2303.16900"
}

@ARTICLE{Yao2025-xv,
  title         = "Reconstruction vs. Generation: Taming optimization dilemma in
                   latent diffusion models",
  author        = "Yao, Jingfeng and Wang, Xinggang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Latent diffusion models with Transformer architectures excel
                   at generating high-fidelity images. However, recent studies
                   reveal an optimization dilemma in this two-stage design:
                   while increasing the per-token feature dimension in visual
                   tokenizers improves reconstruction quality, it requires
                   substantially larger diffusion models and more training
                   iterations to achieve comparable generation performance.
                   Consequently, existing systems often settle for sub-optimal
                   solutions, either producing visual artifacts due to
                   information loss within tokenizers or failing to converge
                   fully due to expensive computation costs. We argue that this
                   dilemma stems from the inherent difficulty in learning
                   unconstrained high-dimensional latent spaces. To address
                   this, we propose aligning the latent space with pre-trained
                   vision foundation models when training the visual tokenizers.
                   Our proposed VA-VAE (Vision foundation model Aligned
                   Variational AutoEncoder) significantly expands the
                   reconstruction-generation frontier of latent diffusion
                   models, enabling faster convergence of Diffusion Transformers
                   (DiT) in high-dimensional latent spaces. To exploit the full
                   potential of VA-VAE, we build an enhanced DiT baseline with
                   improved training strategies and architecture designs, termed
                   LightningDiT. The integrated system achieves state-of-the-art
                   (SOTA) performance on ImageNet 256x256 generation with an FID
                   score of 1.35 while demonstrating remarkable training
                   efficiency by reaching an FID score of 2.11 in just 64
                   epochs--representing an over 21 times convergence speedup
                   compared to the original DiT. Models and codes are available
                   at: https://github.com/hustvl/LightningDiT.",
  month         =  "2~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.01423",
  keywords      = "\_To\_read;Project/Density\_IQA"
}

@ARTICLE{Xu2025-zf,
  title         = "{HarmonyIQA}: Pioneering benchmark and model for image
                   harmonization quality assessment",
  author        = "Xu, Zitong and Duan, Huiyu and Ma, Guangji and Yang, Liu and
                   Wang, Jiarui and Wu, Qingbo and Min, Xiongkuo and Zhai,
                   Guangtao and Callet, P",
  journal       = "arXiv [cs.CV]",
  abstract      = "Image composition involves extracting a foreground object
                   from one image and pasting it into another image through
                   Image harmonization algorithms (IHAs), which aim to adjust
                   the appearance of the foreground object to better match the
                   background. Existing image quality assessment (IQA) methods
                   may fail to align with human visual preference on image
                   harmonization due to the insensitivity to minor color or
                   light inconsistency. To address the issue and facilitate the
                   advancement of IHAs, we introduce the first Image Quality
                   Assessment Database for image Harmony evaluation
                   (HarmonyIQAD), which consists of 1,350 harmonized images
                   generated by 9 different IHAs, and the corresponding human
                   visual preference scores. Based on this database, we propose
                   a Harmony Image Quality Assessment (HarmonyIQA), to predict
                   human visual preference for harmonized images. Extensive
                   experiments show that HarmonyIQA achieves state-of-the-art
                   performance on human visual preference evaluation for
                   harmonized images, and also achieves competing results on
                   traditional IQA tasks. Furthermore, cross-dataset evaluation
                   also shows that HarmonyIQA exhibits better generalization
                   ability than self-supervised learning-based IQA methods. Both
                   HarmonyIQAD and HarmonyIQA will be made publicly available
                   upon paper publication.",
  month         =  "2~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.01116",
  keywords      = "\_To\_read;Project/Density\_IQA"
}

@ARTICLE{Zhang2025-do,
  title         = "{IllusionBench}: A large-scale and comprehensive benchmark
                   for visual illusion understanding in vision-language models",
  author        = "Zhang, Yiming and Zhang, Zicheng and Wei, Xinyi and Liu,
                   Xiaohong and Zhai, Guangtao and Min, Xiongkuo",
  journal       = "arXiv [cs.CV]",
  abstract      = "Current Visual Language Models (VLMs) show impressive image
                   understanding but struggle with visual illusions, especially
                   in real-world scenarios. Existing benchmarks focus on
                   classical cognitive illusions, which have been learned by
                   state-of-the-art (SOTA) VLMs, revealing issues such as
                   hallucinations and limited perceptual abilities. To address
                   this gap, we introduce IllusionBench, a comprehensive visual
                   illusion dataset that encompasses not only classic cognitive
                   illusions but also real-world scene illusions. This dataset
                   features 1,051 images, 5,548 question-answer pairs, and 1,051
                   golden text descriptions that address the presence, causes,
                   and content of the illusions. We evaluate ten SOTA VLMs on
                   this dataset using true-or-false, multiple-choice, and
                   open-ended tasks. In addition to real-world illusions, we
                   design trap illusions that resemble classical patterns but
                   differ in reality, highlighting hallucination issues in SOTA
                   models. The top-performing model, GPT-4o, achieves 80.59\%
                   accuracy on true-or-false tasks and 76.75\% on
                   multiple-choice questions, but still lags behind human
                   performance. In the semantic description task, GPT-4o's
                   hallucinations on classical illusions result in low scores
                   for trap illusions, even falling behind some open-source
                   models. IllusionBench is, to the best of our knowledge, the
                   largest and most comprehensive benchmark for visual illusions
                   in VLMs to date.",
  month         =  "1~" # jan,
  year          =  2025,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2501.00848"
}

@ARTICLE{Zhu2024-qi,
  title         = "{BundleFit}: Display and see-through models for augmented
                   reality head-mounted displays",
  author        = "Zhu, Yufeng",
  journal       = "arXiv [cs.GR]",
  abstract      = "The head-mounted display is a vital component of augmented
                   reality, incorporating optics with complex display and
                   see-through optical behavior. Computationally modeling these
                   optical behaviors requires meeting three key criteria:
                   accuracy, efficiency, and accessibility. In recent years,
                   various approaches have been proposed to model display and
                   see-through optics, which can broadly be classified into
                   black-box and white-box models. However, both categories face
                   significant limitations that hinder their adoption in
                   commercial applications. To overcome these challenges, we
                   leveraged prior knowledge of ray bundle properties outside
                   the optical hardware and proposed a novel bundle-fit-based
                   model. In this approach, the ray paths within the optics are
                   treated as a black box, while a lightweight optimization
                   problem is solved to fit the ray bundle outside the optics.
                   This method effectively addresses the accuracy issues of
                   black-box models and the accessibility challenges of
                   white-box models. Although our model involves runtime
                   optimization, this is typically not a concern, as it can use
                   the solution from a previous query to initialize the
                   optimization for the current query. We evaluated the
                   performance of our proposed method through both simulations
                   and experiments on real hardware, demonstrating its
                   effectiveness.",
  month         =  "30~" # dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.GR",
  eprint        = "2501.01382"
}

@ARTICLE{Subramanian2025-us,
  title     = "Benchmarking the speed–accuracy tradeoff in object recognition by
               humans and neural networks",
  author    = "Subramanian, Ajay and Price, Sara and Kumbhar, Omkar and
               Sizikova, Elena and Majaj, Najib J and Pelli, Denis G",
  journal   = "Journal of vision",
  publisher = "The Association for Research in Vision and Ophthalmology",
  volume    =  25,
  number    =  1,
  pages     = "4--4",
  month     =  "2~" # jan,
  year      =  2025,
  doi       = "10.1167/jov.25.1.4",
  issn      = "1534-7362"
}

@ARTICLE{Chang2020-ey,
  title     = "Toward the next-generation {VR}/{AR} optics: a review of
               holographic near-eye displays from a human-centric perspective",
  author    = "Chang, Chenliang and Bang, Kiseung and Wetzstein, Gordon and Lee,
               Byoungho and Gao, Liang",
  journal   = "Optica",
  publisher = "Optica Publishing Group",
  volume    =  7,
  number    =  11,
  pages     = "1563--1578",
  abstract  = "Wearable near-eye displays for virtual and augmented reality
               (VR/AR) have seen enormous growth in recent years. While
               researchers are exploiting a plethora of techniques to create
               life-like three-dimensional (3D) objects, there is a lack of
               awareness of the role of human perception in guiding the hardware
               development. An ultimate VR/AR headset must integrate the
               display, sensors, and processors in a compact enclosure that
               people can comfortably wear for a long time while allowing a
               superior immersion experience and user-friendly human-computer
               interaction. Compared with other 3D displays, the holographic
               display has unique advantages in providing natural depth cues and
               correcting eye aberrations. Therefore, it holds great promise to
               be the enabling technology for next-generation VR/AR devices. In
               this review, we survey the recent progress in holographic
               near-eye displays from the human-centric perspective.",
  month     =  "20~" # nov,
  year      =  2020,
  keywords  = "Diffractive optical elements;Holographic displays;Liquid crystal
               displays;Near eye displays;Three dimensional displays;Virtual
               reality;Subject/PerceptionBasedRendering;\_To\_read",
  doi       = "10.1364/OPTICA.406004",
  pmc       = "PMC8208705",
  pmid      =  34141829,
  issn      = "2334-2536",
  language  = "en"
}

@ARTICLE{Chang2025-an,
  title     = "Improving the reliability and accuracy of population receptive
               field measures using a logarithmically warped stimulus",
  author    = "Chang, Kelly and Fine, Ione and Boynton, Geoffrey M",
  journal   = "Journal of Vision",
  publisher = "The Association for Research in Vision and Ophthalmology",
  volume    =  25,
  number    =  1,
  pages     = "5--5",
  month     =  "2~" # jan,
  year      =  2025,
  doi       = "10.1167/jov.25.1.5",
  issn      = "1534-7362"
}

@INPROCEEDINGS{Akiyama2022-tx,
  title     = "Perceptually-Based Optimization for Radiometric Projector
               Compensation",
  author    = "Akiyama, Ryo and Fukiage, Taiki and Nishida, Shin'ya",
  booktitle = "2022 IEEE Conference on Virtual Reality and 3D User Interfaces
               Abstracts and Workshops (VRW)",
  publisher = "IEEE",
  pages     = "750--751",
  abstract  = "Radiometric compensation techniques have been proposed to
               manipulate the appearance of arbitrarily textured surfaces using
               projectors. However, due to the limited dynamic range of the
               projectors, these compensation techniques often fail under bright
               environmental lighting or when the projection surface contains
               high contrast textures, resulting in clipping artifacts. To
               address this issue, we propose to apply a perceptually-based tone
               mapping technique to generate compensated projection images. The
               experimental results demonstrated that our approach minimizes the
               clipping artifacts and contrast degradation under challenging
               conditions.",
  month     =  mar,
  year      =  2022,
  keywords  = "MyPapers",
  doi       = "10.1109/VRW55335.2022.00226",
  isbn      =  9781665484022,
  language  = "en"
}

@ARTICLE{Fukiage2022-xx,
  title    = "{HiddenGazeStereo}: Hiding Gaze-Contingent Disparity Remapping for
              {2D}-Compatible Natural {3D} Viewing",
  author   = "Fukiage, Taiki and Nishida, Shin'ya",
  journal  = "IEEE Access",
  volume   =  10,
  pages    = "94778--94796",
  abstract = "This video presents: (1) Comparison of results with
              gaze-contingent remapping and those without gaze-contingent
              remapping (i.e., global disparity compression). This demonstrates
              the effectiveness of gaze-contingent disparity remapping in
              enhancing depth impressions perceived by Hidden Stereo; (2)
              Comparison of results generated by Hidden Stereo and those
              generated by Unhidden Stereo. It demonstrates that the 3D image
              quality as well as depth impressions of Hidden Stereo is
              comparable to that of Unhidden Stereo when viewed with 3D glasses.
              At the same time, it also demonstrates that the 2D image quality
              of Hidden Stereo is significantly better than that of Unhidden
              Stereo when viewed without 3D glasses.This is a normal 2D video
              that can be viewed on standard 2D displays. It contains a
              comparison of results generated by Hidden Stereo and those
              generated by Unhidden Stereo. In the video, the left and right
              stereo images are fused together to simulate how they are
              perceived on 3DTV for viewers without 3D glasses.",
  year     =  2022,
  keywords = "MyPapers",
  doi      = "10.1109/ACCESS.2022.320487410.1109/ACCESS.2022.3204874/mm110.1109/ACCESS.2022.3204874/mm2",
  issn     = "2169-3536",
  language = "en"
}

@ARTICLE{Sorensen2024-hw,
  title    = "The effects of object category training on the responses of
              macaque inferior temporal cortex are consistent with
              performance-optimizing updates within a visual hierarchy",
  author   = "Sörensen, Lynn K A and DiCarlo, James J and Kar, Kohitij",
  journal  = "bioRxiv",
  pages    = "2024.12.27.630539",
  abstract = "AbstractHow does the primate brain coordinate plasticity to
              support its remarkable ability to learn object categories? To
              address this question, we measured the consequences of category
              learning on the macaque inferior temporal (IT) cortex, a key
              waypoint along the ventral visual stream that is known to support
              object recognition. First, we observed that neural activity across
              task-trained monkeys’ IT showed increased object category
              selectivity, enhanced linear separability (of categories), and
              overall more categorical representations compared to those from
              task-naïve monkeys. To model how these differences could arise, we
              next developed a computational hypothesis-generation framework of
              the monkeys’ learning process using anatomically-mapped artificial
              neural network (ANN) models of the primate ventral stream that we
              combined with various choices of learning algorithms. Our
              simulations revealed that specific gradient-based,
              performance-optimizing updates of the ANN’s internal
              representations substantially approximated the observed changes in
              the IT cortex. Notably, we found that such models predict novel
              training-induced phenomena in the IT cortex, including changes in
              category-orthogonal representations and IT’s alignment with
              behavior. This convergence between experimental and modeling
              results suggests that plasticity in the visual ventral stream
              follows principles of task optimization that are well approximated
              by gradient descent. We propose that models like the ones
              developed here could be used to make accurate predictions about
              visual plasticity in the ventral stream and its transference – or
              lack thereof – to any future test image.",
  month    =  "28~" # dec,
  year     =  2024,
  keywords = "Project/Metamerism;\_To\_read",
  doi      = "10.1101/2024.12.27.630539"
}

@ARTICLE{Robinson2025-hj,
  title     = "Dynamics of visual object coding within and across the
               hemispheres: Objects in the periphery",
  author    = "Robinson, Amanda K and Grootswagers, Tijl and Shatek, Sophia M
               and Behrmann, Marlene and Carlson, Thomas A",
  journal   = "Science advances",
  publisher = "American Association for the Advancement of Science (AAAS)",
  volume    =  11,
  number    =  1,
  abstract  = "The human brain continuously integrates information across its
               two hemispheres to construct a coherent representation of the
               perceptual world. Characterizing how visual information is
               represented in each hemisphere over time is crucial for
               understanding how hemispheric transfer contributes to perception.
               Here, we investigated information processing within each
               hemisphere over time and the degree to which it is distinct or
               duplicated across hemispheres. We presented participants with
               object images lateralized to the left or right visual fields
               while measuring their brain activity with electroencephalography.
               Stimulus coding was more robust and emerged earlier in the
               contralateral than the ipsilateral hemisphere. Presentation of
               two stimuli, one to each hemifield, reduced the fidelity of
               representations in both hemispheres relative to one stimulus
               alone, signifying hemispheric interference. Last, we found that
               processing within the contralateral, but not ipsilateral,
               hemisphere was biased to image-related over concept-related
               information. Together, these results suggest that hemispheric
               transfer operates to filter irrelevant information and
               efficiently prioritize processing of meaning.",
  month     =  "3~" # jan,
  year      =  2025,
  keywords  = "Project/Metamerism",
  doi       = "10.1126/sciadv.adq0889",
  issn      = "2375-2548",
  language  = "en"
}

@ARTICLE{Bossio_Botero2024-lh,
  title    = "When do measured representational distances reflect the neural
              representational geometry?",
  author   = "Bossio Botero, Veronica and Kriegeskorte, Nikolaus",
  journal  = "bioRxiv",
  pages    = "2024.12.30.630743",
  abstract = "The representational geometry of a brain region can be
              characterized by the distances among neural activity patterns for
              a set of experimental conditions. Researchers routinely estimate
              representational distances from brain-activity measurements that
              either sparsely sample the underlying neural population (e.g.
              neural recordings) or pool across the activity of many neurons
              (e.g. fMRI voxels). Here we use theory and simulations to clarify
              under what circumstances representational distances estimated from
              brain-activity measurements reflect the representational geometry
              of the underlying neural population, and what distortions must be
              expected under other circumstances. We demonstrate that the
              estimated representational distances are undistorted if single
              neurons are sampled at random. For voxels that take non-negatively
              weighted linear combinations, the resulting geometry is linearly
              distorted, correctly reflecting the population-mean dimension,
              while downscaling all orthogonal dimensions, for which the
              averaging cancels a large portion of the signal. Surprisingly,
              removing the mean from voxel patterns recovers the underlying
              representational geometry exactly in expectation under idealized
              conditions. This explains why the correlation distance, the most
              popular measure of representational dissimilarity in neuroimaging
              studies, ``works`` so well, yielding geometries that can appear
              similar between fMRI and neural recordings. The Euclidean (or
              Mahalanobis) distance computed after removing the mean of each
              pattern (without normalizing its variance) is an attractive
              alternative to the correlation distance in that it corrects for
              the inflated relative contribution of the population-mean
              dimension, while avoiding the drawback of the correlation
              distance: it can be large for confusable low-norm patterns,
              failing to reflect decodability. Our results demonstrate that
              measured representational distances reflect the neural
              representational geometry when (1) single neurons are sampled at
              random or (2) the weights with which the measured responses sample
              the neurons are drawn i.i.d. and (2a) the weights are drawn from a
              zero-mean distribution or (2b) the population mean is the same for
              all conditions or (2c) the mean is removed from each estimated
              pattern. We discuss practical implications for analyses of neural
              representational geometries.",
  month    =  "30~" # dec,
  year     =  2024,
  keywords = "\_To\_read",
  doi      = "10.1101/2024.12.30.630743"
}

@ARTICLE{Charles2024-qy,
  title         = "The {FlEye} camera: Sampling the joint distribution of
                   natural scenes and motion",
  author        = "Charles, J Edelson and Paul, Smith and Sima, Setayeshgar and
                   William, Bialek and van Steveninck, Rob R de Ruyter",
  journal       = "arXiv [q-bio.NC]",
  abstract      = "To make efficient use of limited physical resources, the
                   brain must match its coding and computational strategies to
                   the statistical structure of input signals. An attractive
                   testing ground for these principles is the problem of motion
                   estimation in the fly visual system: we understand the optics
                   of the compound eye, have a quantitative description of input
                   signals and noise from the retina, and can record from output
                   neurons that encode estimates of different velocity
                   components. Furthermore, recent work provides a nearly
                   complete wiring diagram of the intervening circuitry. What is
                   missing is a characterization of the visual signals and
                   motions that flies encounter in a natural context. We attack
                   this directly with the development of a specialized camera
                   that matches the high temporal resolution, optical
                   properties, and spectral sensitivity of the fly's eye;
                   inertial motion sensors provide ground truth about rotations
                   and translations through the world. We describe the design,
                   construction, and performance characteristics of this FlEye
                   camera. To illustrate the opportunities created by this
                   instrument we use data on movies and motion to construct
                   optimal local motion estimators that can be compared with the
                   responses of the fly's motion sensitive neurons.",
  month         =  "30~" # dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "2412.21081"
}

@ARTICLE{Netanel2024-uc,
  title         = "What makes for a good stereoscopic image?",
  author        = "Netanel, Y Tamir and Shir, Amir and Ranel, Itzhaky and Noam,
                   Atia and Shobhita, Sundaram and Stephanie, Fu and Ron,
                   Sokolovsky and Phillip, Isola and Tali, Dekel and Richard,
                   Zhang and Miriam, Farber",
  journal       = "arXiv [cs.CV]",
  abstract      = "With rapid advancements in virtual reality (VR) headsets,
                   effectively measuring stereoscopic quality of experience
                   (SQoE) has become essential for delivering immersive and
                   comfortable 3D experiences. However, most existing stereo
                   metrics focus on isolated aspects of the viewing experience
                   such as visual discomfort or image quality, and have
                   traditionally faced data limitations. To address these gaps,
                   we present SCOPE (Stereoscopic COntent Preference
                   Evaluation), a new dataset comprised of real and synthetic
                   stereoscopic images featuring a wide range of common
                   perceptual distortions and artifacts. The dataset is labeled
                   with preference annotations collected on a VR headset, with
                   our findings indicating a notable degree of consistency in
                   user preferences across different headsets. Additionally, we
                   present iSQoE, a new model for stereo quality of experience
                   assessment trained on our dataset. We show that iSQoE aligns
                   better with human preferences than existing methods when
                   comparing mono-to-stereo conversion methods.",
  month         =  "30~" # dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2412.21127"
}

@ARTICLE{Keke2024-bi,
  title         = "Structural similarity in deep features: Image Quality
                   Assessment robust to Geometrically Disparate reference",
  author        = "Keke, Zhang and Weiling, Chen and Tiesong, Zhao and Zhou,
                   Wang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Image Quality Assessment (IQA) with references plays an
                   important role in optimizing and evaluating computer vision
                   tasks. Traditional methods assume that all pixels of the
                   reference and test images are fully aligned. Such
                   Aligned-Reference IQA (AR-IQA) approaches fail to address
                   many real-world problems with various geometric deformations
                   between the two images. Although significant effort has been
                   made to attack Geometrically-Disparate-Reference IQA
                   (GDR-IQA) problem, it has been addressed in a task-dependent
                   fashion, for example, by dedicated designs for image
                   super-resolution and retargeting, or by assuming the
                   geometric distortions to be small that can be countered by
                   translation-robust filters or by explicit image
                   registrations. Here we rethink this problem and propose a
                   unified, non-training-based Deep Structural Similarity
                   (DeepSSIM) approach to address the above problems in a single
                   framework, which assesses structural similarity of deep
                   features in a simple but efficient way and uses an attention
                   calibration strategy to alleviate attention deviation. The
                   proposed method, without application-specific design,
                   achieves state-of-the-art performance on AR-IQA datasets and
                   meanwhile shows strong robustness to various GDR-IQA test
                   cases. Interestingly, our test also shows the effectiveness
                   of DeepSSIM as an optimization tool for training image
                   super-resolution, enhancement and restoration, implying an
                   even wider generalizability. \footnote\{Source code will be
                   made public after the review is completed.}",
  month         =  "27~" # dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2412.19553"
}

@ARTICLE{Tomer2024-og,
  title         = "The illusion-illusion: Vision language models see illusions
                   where there are none",
  author        = "Tomer, Ullman",
  journal       = "arXiv [q-bio.NC]",
  abstract      = "Illusions are entertaining, but they are also a useful
                   diagnostic tool in cognitive science, philosophy, and
                   neuroscience. A typical illusion shows a gap between how
                   something ``really is'' and how something ``appears to be'',
                   and this gap helps us understand the mental processing that
                   lead to how something appears to be. Illusions are also
                   useful for investigating artificial systems, and much
                   research has examined whether computational models of
                   perceptions fall prey to the same illusions as people. Here,
                   I invert the standard use of perceptual illusions to examine
                   basic processing errors in current vision language models. I
                   present these models with illusory-illusions, neighbors of
                   common illusions that should not elicit processing errors.
                   These include such things as perfectly reasonable ducks,
                   crooked lines that truly are crooked, circles that seem to
                   have different sizes because they are, in fact, of different
                   sizes, and so on. I show that many current vision language
                   systems mistakenly see these illusion-illusions as illusions.
                   I suggest that such failures are part of broader failures
                   already discussed in the literature.",
  month         =  "7~" # dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "q-bio.NC",
  eprint        = "2412.18613"
}

@ARTICLE{Chenglin2024-zv,
  title         = "1.58-bit {FLUX}",
  author        = "Chenglin, Yang and Celong, Liu and Xueqing, Deng and Dongwon,
                   Kim and Xing, Mei and Xiaohui, Shen and Liang-Chieh, Chen",
  journal       = "arXiv [cs.CV]",
  abstract      = "We present 1.58-bit FLUX, the first successful approach to
                   quantizing the state-of-the-art text-to-image generation
                   model, FLUX.1-dev, using 1.58-bit weights (i.e., values in
                   {-1, 0, +1}) while maintaining comparable performance for
                   generating 1024 x 1024 images. Notably, our quantization
                   method operates without access to image data, relying solely
                   on self-supervision from the FLUX.1-dev model. Additionally,
                   we develop a custom kernel optimized for 1.58-bit operations,
                   achieving a 7.7x reduction in model storage, a 5.1x reduction
                   in inference memory, and improved inference latency.
                   Extensive evaluations on the GenEval and T2I Compbench
                   benchmarks demonstrate the effectiveness of 1.58-bit FLUX in
                   maintaining generation quality while significantly enhancing
                   computational efficiency.",
  month         =  "24~" # dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2412.18653"
}

@ARTICLE{Hosseini2024-ks,
  title    = "Universality of representation in biological and artificial neural
              networks",
  author   = "Hosseini, Eghbal A and Casto, Colton C and Zaslavsky, Noga and
              Conwell, Colin and Richardson, Mark and Fedorenko, Evelina",
  journal  = "bioRxiv",
  pages    = "2024.12.26.629294",
  abstract = "Many artificial neural networks (ANNs) trained with ecologically
              plausible objectives on naturalistic data align with behavior and
              neural representations in biological systems. Here, we show that
              this alignment is a consequence of convergence onto the same
              representations by high-performing ANNs and by brains. We
              developed a method to identify stimuli that systematically vary
              the degree of inter-model representation agreement. Across
              language and vision, we then showed that stimuli from high- and
              low-agreement sets predictably modulated model-to-brain alignment.
              We also examined which stimulus features distinguish high- from
              low-agreement sentences and images. Our results establish
              representation universality as a core component in the
              model-to-brain alignment and provide a new approach for using ANNs
              to uncover the structure of biological representations and
              computations.",
  month    =  "26~" # dec,
  year     =  2024,
  doi      = "10.1101/2024.12.26.629294"
}

@ARTICLE{Xingyou2024-ak,
  title         = "{OmniPred}: Language Models as Universal Regressors",
  author        = "Xingyou, Song and Oscar, Li and Chansoo, Lee and Bangding,
                   Yang and Daiyi, Peng and Sagi, Perel and Yutian, Chen",
  journal       = "arXiv [cs.LG]",
  abstract      = "Regression is a powerful tool to accurately predict the
                   outcome metric of a system given a set of parameters, but has
                   traditionally been restricted to methods which are only
                   applicable to a specific task. In this paper, we propose
                   OmniPred, a framework for training language models as
                   universal end-to-end regressors over $(x,y)$ data from
                   arbitrary formats. Using data sourced from Google Vizier, one
                   of the largest proprietary blackbox optimization databases in
                   the world, our extensive experiments demonstrate that
                   language models are capable of very precise numerical
                   regression using only textual representations of mathematical
                   parameters and values, and if given the opportunity to train
                   at scale over multiple tasks, can significantly outperform
                   traditional regression models.",
  month         =  "22~" # feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2402.14547"
}

@ARTICLE{Kecheng2024-zv,
  title         = "Color-name aware optimization to enhance the perception of
                   transparent overlapped charts",
  author        = "Kecheng, Lu and Lihang, Zhu and Yunhai, Wang and Qiong, Zeng
                   and Weitao, Song and Khairi, Reda",
  journal       = "arXiv [cs.GR]",
  abstract      = "Transparency is commonly utilized in visualizations to
                   overlay color-coded histograms or sets, thereby facilitating
                   the visual comparison of categorical data. However, these
                   charts often suffer from significant overlap between objects,
                   resulting in substantial color interactions. Existing color
                   blending models struggle in these scenarios, frequently
                   leading to ambiguous color mappings and the introduction of
                   false colors. To address these challenges, we propose an
                   automated approach for generating optimal color encodings to
                   enhance the perception of translucent charts. Our method
                   harnesses color nameability to maximize the association
                   between composite colors and their respective class labels.
                   We introduce a color-name aware (CNA) optimization framework
                   that generates maximally coherent color assignments and
                   transparency settings while ensuring perceptual
                   discriminability for all segments in the visualization. We
                   demonstrate the effectiveness of our technique through
                   crowdsourced experiments with composite histograms, showing
                   how our technique can significantly outperform both standard
                   and visualization-specific color blending models.
                   Furthermore, we illustrate how our approach can be
                   generalized to other visualizations, including parallel
                   coordinates and Venn diagrams. We provide an open-source
                   implementation of our technique as a web-based tool.",
  month         =  "19~" # dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.GR",
  eprint        = "2412.16242"
}

@ARTICLE{Wenxuan2024-ye,
  title         = "Guided real image dehazing using {YCbCr} color space",
  author        = "Wenxuan, Fang and Jankai, Fan and Yu, Zheng and Jiangwei,
                   Weng and Ying, Tai and Jun, Li",
  journal       = "arXiv [cs.CV]",
  abstract      = "Image dehazing, particularly with learning-based methods, has
                   gained significant attention due to its importance in
                   real-world applications. However, relying solely on the RGB
                   color space often fall short, frequently leaving residual
                   haze. This arises from two main issues: the difficulty in
                   obtaining clear textural features from hazy RGB images and
                   the complexity of acquiring real haze/clean image pairs
                   outside controlled environments like smoke-filled scenes. To
                   address these issues, we first propose a novel Structure
                   Guided Dehazing Network (SGDN) that leverages the superior
                   structural properties of YCbCr features over RGB. It
                   comprises two key modules: Bi-Color Guidance Bridge (BGB) and
                   Color Enhancement Module (CEM). BGB integrates a phase
                   integration module and an interactive attention module,
                   utilizing the rich texture features of the YCbCr space to
                   guide the RGB space, thereby recovering clearer features in
                   both frequency and spatial domains. To maintain tonal
                   consistency, CEM further enhances the color perception of RGB
                   features by aggregating YCbCr channel information.
                   Furthermore, for effective supervised learning, we introduce
                   a Real-World Well-Aligned Haze (RW$^2$AH) dataset, which
                   includes a diverse range of scenes from various geographical
                   regions and climate conditions. Experimental results
                   demonstrate that our method surpasses existing
                   state-of-the-art methods across multiple real-world
                   smoke/haze datasets. Code and Dataset:
                   \textcolor{blue}{\url{https://github.com/fiwy0527/AAAI25\_SGDN.}}",
  month         =  "23~" # dec,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2412.17496"
}

@ARTICLE{Gallant2024-up,
  title    = "A biologically-inspired hierarchical convolutional energy model
              predicts {V4} responses to natural videos",
  author   = "Gallant, Jack L and Oliver, Michael and Winter, Michele and
              Eickenberg, Michael and la Tour, Tom Dupre",
  journal  = "bioRxiv.org: the preprint server for biology",
  pages    = "2024.12.16.628781",
  abstract = "V4 is a key area within the visual processing hierarchy, and it
              represents features of intermediate complexity. However, no
              current computational model explains V4 responses under natural
              conditions. To address this, we developed a new hierarchical
              convolutional energy (HCE) model reflecting computations thought
              to occur in areas V1, V2, and V4, but which consists entirely of
              simple- and complex-like units like those found in V1. In contrast
              to prior models, the HCE model is trained end-to-end on
              neurophysiology data, without relying on pre-trained network
              features. We recorded 313 V4 neurons during full-color nature
              video stimulation and fit the HCE model to each neuron. The
              model's predicted optimal patterns (POPs) revealed complex
              spatiotemporal pattern selectivity in V4, supporting its role in
              representing space, time, and color. These findings indicate that
              area V4 is crucial for image segmentation and grouping operations
              that are essential for complex vision. Thus, responses of V4
              neurons under naturalistic conditions can be explained by a
              hierarchical three-stage model where each stage consists entirely
              of units like those found in area V1. \#\#\# Competing Interest
              Statement The authors have declared no competing interest.",
  month    =  "22~" # dec,
  year     =  2024,
  keywords = "Project/Metamerism;\_To\_read",
  doi      = "10.1101/2024.12.16.628781",
  language = "en"
}